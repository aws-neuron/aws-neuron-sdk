.. meta::
   :description: Learn how to migrate AWS Neuron NKI Beta 1 kernels to the new NKI compiler in Beta 2.
   :keywords: NKI Compiler, Neuron Kernel Interface, NKI, AWS Neuron SDK, kernel compilation, migrate kernels

.. _migrate-nki-kernels-beta2:

Migrate your NKI Beta 1 Kernels to the New NKI Compiler Syntax
================================================================

This topic covers best practices for migrating your NKI kernels from Beta 1 to Beta 2, which means using the new NKI compiler introduced with Beta 2. Learn the recommended methods to migrate your kernels to leverage the improved NKI developer experience.

For details on the new NKI Compiler, see :doc:`index` and :doc`now-nki-works-with-compiler`.

Overview
--------

The best practices in this topic will help you migrate your kernels written with the Beta 1 to work correctly with the NKI compiler in Beta 2.

Background: NKI has a compiler!
-------------------------------

Starting with Beta 2, NKI now has a compiler. Unlike Beta 1, in which NKI was implemented as an embedded Domain Specific Language (eDSL), NKI is now a true programming language with its own compiler. In Beta 2, we have kept the syntax of NKI as a subset of Python. This means you can place your NKI functions in your Python development, and Python will not complain about syntax errors. **However, it is important to remember that your NKI functions are compiled by the NKI compiler, not evaluated by the Python interpreter**. This change allows us to offer a better programming experience with more precise errors messages, more detailed analysis of your NKI kernels to check for performance and semantic issues, and better optimization tailored to the NKI language.

With the new compiler as part of Beta 2, Neuron defines the NKI language as a subset of Python. This means that all NKI programs are valid Python programs, but not all Python programs are valid NKI programs. You mark code you want the NKI Compiler to process and optimize with the ``nki.jit`` decorator. Unlike in previous versions of NKI, however, the functions under this decorator will be passed to the NKI compiler and not evaluated by the Python interpreter.

.. code-block:: python

   def a_function(x,y,z):
     # this is Python code

   @nki.jit
   def kernel(x,y,z):
     # this is NKI code

If you use Python features within a NKI kernel that are not supported at this point in time, you will receive an error from the NKI compiler. If you find some curious errors or confusing behavior, reach out and let us know on the `NKI GitHub samples repository <https://github.com/aws-neuron/nki-samples>`__.

.. note::
    This document is intended for experienced NKI developers who are looking to quickly migrate their existing kernels to the new NKI compiler. The code examples in this document must be executed within a valid NKI kernel.

Migration Considerations
-------------------------

Here are the key considerations when migrating your existing kernel to the new NKI compiler.

New NKI Compiler Beta 2 Features
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

NKI Beta 2 introduces new features and changes you can use when migrating your kernel:

* There's a new namespace for NKI: ``nki`` (formerly ``neuronxcc.nki``). Much simpler!
* The behavior of loops and branching are now consistent with regular Python.
* ``AccessPattern`` is a new API available for tensor indexing.
* Lists and dictionaries are now supported and their behavior in the loops is consistent with regular Python syntax and behaviors.
* Direct allocation APIs have been reworked to be a tuple element of ``nl.ndarray`` of the form ``(partition_offset, free_offset)``.

Details on these new features are provided further in this document. Follow this link to jump to them:

What has not changed from NKI Beta 1?
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

Some features have not (yet) changed from the Beta 1 version of the NKI Compiler and may still be used:

* Integer-based tensor slicing is still available, and must be the default indexer
* Other than requiring ``dst``, the signature of most ``isa`` instructions stays the same
* Kernels must still have memory and hardware resources manually allocated, or the allocation will be taken care of by the compiler allocator
* ``print`` is still available, now with improved behavior
* Classes are still available, though some special syntax is required.

Details on these new features are provided further in this document. Follow this link to jump to them:

What features in NKI Beta 1 are coming soon?
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

Neuron has additional changes prepared which may not be available in the currrent Beta release. Review this list so you can prepare your code as these changes are rolled out in future releases:

* ``arange`` will be removed. Rewrite your kernels that use it with slicing or Access Patterns instead: 
* The ``mask`` parameter will not be supported anymore: :ref:`Deprecation of masking <deprecation-of-masking>`.
* Block dimensions of tensors have been removed: :ref:`Translation of Block Dimensions <translation-of-block-dimensions>`.
* An explicit ``dst`` parameter is now required for ``nisa`` instructions, which is always the first argument of a NISA instruction: :ref:`New namespace, new APIs <new-namespace-new-apis>`.
* ``nl.load`` and ``nl.store`` has been removed. Use ``nisa.dma_copy`` instead.
* Dynamic Access syntax will change :ref:`Dynamic Access Pattern <dynamic-access-pattern>`
* Decorators on the sub-kernels will be removed: :ref:`Remove nki.jit decorator on sub-kernels <remove-nki-jit-decorator>`
* Dictionaries will support only string keys.
* Python interactive debugging will be available in a future release.
* Nested slicing will be available in a future release.

New namespace, new APIs
------------------------

This section applies to all NKI kernels.

With the introduction of NKI Beta, Neuron is making changes to the NKI APIs. Neuron has introduced a new namespace for the Beta 2 APIs, ``nki`` (formerly ``neuronxcc.nki``). When upgrading from Beta 1 to Beta 2, you must  modify your Python imports in your kernel code to get the new Beta 2 compatible APIs.

.. code-block:: python

   # Legacy Beta 1 APIs
   import neuronxcc.nki as nki
   import neuronxcc.nki.isa as nisa

   # New Beta 2 APIs
   import nki
   import nki.isa as nisa

Neuron has made many small improvements to the APIs, including consistent naming and order of arguments, along with a closer matching with the hardware ISA so that what you write in your NKI kernel code and what you see in :doc:`the profiler </tools/profiler/about/index>` are the same. There is one big change that programmers should be aware of: **all ISA functions now require a ``destination`` parameter**.

All ISA functions require a destination parameter
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

In Beta 2, all of the ISA functions now require a ``dst`` parameter into which the function results are returned (as opposed to providing a function return value). So, instead of writing code like this:

.. code-block:: python

   result[...] = nisa.reciprocal(src)

You must now write:

.. code-block:: python

   nisa.reciprocal(dst=result[...], src)

This change makes the behavior of the APIs more consistent and scales well to cases where APIs may perform accumulation or return multiple results. 

.. _consistent-control-flow-behavior:

Consistent control-flow behavior
---------------------------------

This section applies to all NKI kernels.

In NKI Beta 1, ``range`` iterators were converted into special objects that allowed the eDSL to capture the loop body. Because of this, loops were only executed once by the Python evaluator, which could lead to some surprising results. For example, in the code below, the normal Python variable ``val``, ends up with a value of ``1`` rather than the expected value of ``8``. This has been addressed in the new NKI compiler, as seen in the code example below:

.. code-block:: python

   val = 0
   for i in range(8):
     val += 1
   print(val) # will print 1 in Beta 1, prints 8 in Beta 2

For similar reasons, sometimes Python control flow constructs, such as ``if/then/else`` statements, could not be handled properly when nested within a ``for`` loop. For example, in Beta 1 the code below produces an undefined result. In Beta 2, this code produces the expected result:

.. code-block:: python

   val = 0
   for i in range(8):
     if i == 0:
       val = 1
     else:
       val = 2
   print(val) # undefined behaviour in Beta 1, prints 2 in Beta 2

Many other examples of errant control flow have been fixed, bringing the NKI behaviors for control flow syntax closer to what is expected from the same syntax in Python.

.. _deprecation-of-masking:

Masking is no longer supported
--------------------------------

Follow this section if you are using the mask parameter in your kernel.

In NKI Beta 1 we introduced the concept of masking in order modify the behavior of tensor-indexing expressions. The use of masking was almost always used to avoid out-of-bounds access. For example, suppose you are tiling a tensor of size 129 x 513, and you want to use tiles of size 128 x 512. A typical way to write a tiling loop in Beta 1 is shown below:

.. code-block:: python

   t = nl.ndarray(shape=(129, 513), ...)
   result = nl.ndarray(shape=(129, 513), ...)
   for i in range(2):
     for j in range(2):
       i_p, i_f = nl.mgrid[0:128, 0:512]
       result[i_p+128*i, i_f+512*i] = nisa.tensor_copy(t[i_p+128*i, i_f+512*i],
        mask=(i_p+128*i<129) & (i_f+512*i<513))

This code example shows the masking approach used in NKI Beta 1 for handling tensor boundary conditions. It processes a tensor of size 129x513 using 128x512 tiles. The code creates two nested loops to iterate over the tensor in 2x2 tiles. For each tile, it uses `nl.mgrid` to generate index tensors `i_p` and `i_f` with values from 0 to 127 and 0 to 511 respectively. These indices are then offset by `128*i` and `512*j` to access different regions of the tensor. The critical part is the `mask` parameter, which prevents out-of-bounds access when the indices would exceed the tensor dimensions (129 and 513). The mask expression `(i_p+128*i<129) & (i_f+512*i<513)` evaluates to ``false`` for any indices that would be out of bounds, ensuring safe tensor access at the boundaries.

Note that when ``i`` (or ``j``) is equal to ``1``, then the index expression ``result[i_p+128*i, i_f+512*i]`` would overflow the tensor dimension. The mask expression ``mask=(i_p+128*i<129) & (i_f+512*i<513)`` modifies the indexing so that the equations are true, and thus within the bounds of the tensor. 

**This mechanism had many drawbacks**:

* It is not modular. If we try to introduce variable bindings for the indexing expressions, the code no longer works. 
* The mask is a function parameter although it is modifying the indexes of other arguments, and even the result of the function. 
* The interpretation of the masking expressions and the indexing expressions are not the same. The expression ``i_p+128*i<129`` has a different meaning when it is in an index position versus a mask position. This context dependence leads to confusion and makes the language inconsistent.

In NKI Beta 2, programmers can use standard constructs from Python such as ``min`` and ``slice`` to build indexing expressions that are within a tensor's bounds. For example, the above code can now be written as:

.. code-block:: python

   for i in range(2):
     p_start = i * 128
     p_end = min(129, p_start + 128)
     p = slice(p_start, p_end)  # a.k.a. (p_start:p_end)
     
     for j in range(2):
       f_start = j * 512
       f_end = min(513, f_start + 512)
       f = slice(f_start, f_end)  # a.k.a. (f_start:f_end)
       
       nisa.tensor_copy(result[p, f], t[p, f])

You may also choose to inline the slices, if that is more natural. The below syntax is more common in NKI Beta 1:

.. code-block:: python

       nisa.tensor_copy(result[p_start:p_end, f_start:f_end],
                             t[p_start:p_end, f_start:f_end])

Of course, programmers may also choose to use ``mgrid``. Here is another equivalent way to write the Beta 1 tiling loop using the Beta 2 syntax changes:

.. code-block:: python

   import nki.stdlib as nsl

   for i in range(2):

     for j in range(2):
     
       num_p = min(128, 129-128*i)
       num_f = min(512, 513-512*i)
       
       ip, if_ = nsl.mgrid[0:num_p, 0:num_f]
       
       nisa.tensor_copy(
           nsl.mgrid_wrap(result)[128*i + ip, 512*i + if_],
                                  t[128*i + ip, 512*i + if_])

Syntax changes for dynamic control flow
---------------------------------------

NKI Beta 2 includes support for explicit dynamic (on-chip) control flow. All of the dynamic control flow uses on-chip registers to hold the conditional values. If a control-flow construct uses a register as a conditional, then the loop will be an on-chip, dynamic (runtime) loop. This is very common in scenarios like mixture of experts, where the index space for the expert is known at runtime, but not at compile time. Dynamic control flow with the new NKI APIs enables support for this use case.

To support dynamic control flow, NKI has a new set of ``nki.isa`` APIs for reading and writing to hardware registers.

.. code-block:: python

   def register_alloc(x: Optional[int]) -> register: ...

   def register_move(dst: register, imm: int): ...

   def register_load(dst: register, src: tensor): ...

   def register_store(dst: tensor, src: register): ...

The most basic dynamic loop is a ``for`` loop that uses a register value for the iteration value and another register for the upper bound. Programmers can write this kind of loop using ``dynamic_range``:

.. code-block:: python

   # dynamic loop with dynamically computed upper bounds
   ub = register_alloc()
   register_load(ub, tensor)
   for i in dynamic_range(5, ub, 2):
     ...

Programmers can also write dynamic ``while`` loops. When using a dynamic ``while`` loop the programmer must update the register within the body of the loop.

.. code-block:: python

   # prepare a conditional tensor with a value
   cond = nl.ndarray((1, 1), buffer=nl.shared_hbm, dtype=np.int32)

   # create register with initial value
   reg = register_alloc(1)

   while reg:
     ...
     # load (updated) value from cond tensor into register
     register_load(reg, cond)

.. _update-indexing-syntax:

Update indexing syntax for ``mgrid`` and ``arange``
----------------------------------------------------

If you are using ``nl.mgrid`` or ``nl.arange`` to access continuous elements in your NKI Beta 1 kernels, you must replace these with integer slicing when migrating to NKI Beta 2. Consider the following example:

.. code-block:: python

   # Example 1
   t = nl.ndarray(shape=(128, 16, 64), ...)
   i_p, if0, if1 = nl.mgrid[0:128, 0:8, 0:64]
   t[i_p, if0, if1] # access of the element is continous
   # should just use integers to create the slice
   t[0:128, 0:8, 0:64]

   # Example 2
   t = nl.ndarray(shape=(128, 16*64))
   i_p, if0, if1 = nl.mgrid[0:128, 0:8, 0:64]
   t[i_p, if0*64+if1]
   # should just use integer slicing
   t[0:128, 0:8*64]

If your use case cannot be represented with the slicing syntax above, refer to :ref:`Enhanced Indexing Capabilities <enhanced-indexing-capabilities>` below.

.. _enhanced-indexing-capabilities:

Enhanced indexing capabilities
-------------------------------

Follow this section to use the enhanced indexing capabilities offered in Beta 2. Before using the capability introduced in the this section, refer to :ref:`Update indexing syntax for mgrid and arange <update-indexing-syntax>`. **The indexing capability introduced here must only be used when slicing cannot represent the desired tensor access**.

In NKI Beta 2, Neuron has introduced a new tensor indexing capability called **Access Patterns (AP)**. An Access Pattern consists of a list of tuples and an offset. Given a tensor ``t`` of ``dtype``, the access pattern will always first flatten the tensor to 1d, then perform the tensor access with the following semantics:

.. code-block:: python

   [[w_step, w_num], [z_step, z_num], [y_step, y_num], [x_step, x_num]], offset: int

This enables the following access on the tile:

.. code-block:: python

   for (int w = 0; w < w_num; ++w)
     for (int z = 0; z < z_num; ++z)
       for (int y = 0; y < y_num; ++y)
         for (int x = 0; x < x_num; ++x)
           (dtype*)(t.flatten())[offset + (w * w_step) + (z * z_step)
                                      + (y * y_step) + (x * x_step)]

As an example, given a tensor ``t`` of size ``(16P, 16F)``, to iterate all the elements in ``t[0:16, 8:16]``, you can write the access pattern as:

.. code-block:: python

   t.ap(pattern=[[16, 16], [1, 8]], offset=8) 

   # in loop form
   for w in range(16):
     for z in range(8):
       idx += 8 + (w * 16) + (1 * z)
       access <t[idx]>


.. image:: /nki/img/compiler/memory-access-visualization-1.png

Restriction on SBUF/PSUM tensors
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

For SBUF/PSUM tensors, the first tuple is always the access for the partition dimension. On Trn1/Trn2 hardware, the partition dimension access must be continuous. Therefore, given a tensor of shape ``(p_dim, f_dim0, fdim1)``, the step of the leading dimension must be ``f_dim0 * f_dim1``.

For example, you cannot read every other partition as demonstrated in the following example:

.. code-block:: python

   t = nl.ndarray((16, 32, dtype=nl.float32, buffer=nl.sbuf)

   # The following is illegal, because the first stride is 16*2 and we are reading every other parition
   t.ap(pattern=[[64, 8], [1, 32]], offset=0) 

.. image:: /nki/img/compiler/memory-access-visualization-2.png

.. _restriction-on-nested-indexing:

Restriction on nested indexing
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

The ``ap()`` method is only allowed on a ``nl.ndarray`` instance and cannot be called on a tile produced by it. For example, the following would result in an error:

.. code-block:: python

   t = nl.ndarray((128, 256), dtype=nl.float32, buffer=nl.sbuf)
   t.ap(pattern=[[256, 128],[1, 256], offset=0).ap(pattern=[[256, 64], [1, 64]], offset=0)
        ^-- cannot specify an access pattern on an already indexed tensor

.. _improved-allocation-api:

Improved allocation API
------------------------

The manual allocation API has been simplified to be just a tuple within ``nl.ndarray`` that is ``(partition_offset, free_offset)``. Similar to the behavir in NKI Beta 1, the partition offset corresponds to a physical partition lane on the hardware while the free dimension offset is the byte address virtualized for each kernel. The free dimension virtual address is translated into a physical SBUF address in the compiler during kernel inline compilation.

.. code-block:: python

   # creates your buffer on parition 0, offset by 128 elements of your data type
   a_result = nl.ndarray(dtype=a.dtype, shape=a.shape, name="result", 
     address=(0, 128), buffer=nl.sbuf)

For PSUM, the address is now two dimensions to be consistent with hardware expectations. The PSUM banks are laid out on the free dimension. The allocation for PSUM tensors must start at the beginning of each bank or compiler will throw a memory violation error..

For example, to allocate a PSUM tensor on bank 3, you can do the following:

.. code-block:: python

   bank_id = 3
   PSUM_BANK_SIZE = 2048
   psum_t = nl.ndarray(dtype=nl.bfloat16, shape=(128, 1024), 
     address=(0, bank_id*PSUM_BANK_SIZE))

Translate from the NKI Beta 1 Direct Allocation API
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

To translate the direct allocated kernel from NKI Beta 1 code, first you must ensure all data structures do not use the block dimension. This means reformatting your tensors to place the partition dimension on the left-most position, using either lists or multi-dimensional tensors for the rest of your dimensions. Review :ref:`the block dimension migration guide <translation-of-block-dimensions>` in this document for more details.

After migrating the blocks in your kernel, you must translate the address of each block. For example, given the following tensor in  NKI Beta 1 that uses modular allocation:

.. code-block:: python

   # beta 1 - uses block dimension and mod allocator
   k_loaded = nl.ndarray((num_512_tiles_cur_section, nl.par_dim(p_k), n_k), 
    dtype=nl.bfloat16, 
    buffer=sb_mod(base_addr=sca, num_free_tiles=(num_512_tiles_cur_section, )

Now with Beta 2, you can translate the block dimension into a list and compute the address for each block, like so:

.. code-block:: python

   # Beta 2 - use lists of tensors and get lists of virtual byte addresses
   k_loaded_tensors = []
   for i in range(num_512_tiles_cur_section):
     k_loaded_tensors.append(nl.ndarray(shape=(p_k,n_k), dtype=nl.bfloat16, 
     buffer=nl.sbuf, address=(0, sca + (i%num_512_tiles_cur_section)*n_k*2 ) )

.. _remove-nki-jit-decorator:

Remove ``nki.jit`` decorator on subkernels
-------------------------------------------

Follow this section if your kernel calls other kernels.

In NKI Beta 1, all of the sub-kernels in a top-level kernel could be decorated with a ``nki.jit(mode='trace'``) decorator. This decorator must be removed from your kernel code; otherwise, you will see an error about classes requiring inheritance from ``nl.NKIObject`` thrown from the callsite of the subkernel(s).

.. _translation-of-block-dimensions:

Translation of block dimensions
---------------------------------

Follow this section if any of the tensors in your kernel uses block dimension, defined as a tensor with a partition dimension set to any position other than the left-most position.

**Block dimension has been completely removed in NKI Beta 2.** There are two performance-equivalent ways to translate block dimensions in Beta 2:

Use Python ``list`` syntax
^^^^^^^^^^^^^^^^^^^^^^^^^^^

To avoid performance regressions, Neuron recommends changing ``batch-dim`` to a Python list of tensors and using metaprogramming inside Python loop syntax. This means you must modify these loops to use a standard Python range().

.. code-block:: python

   # Before migration
   t = nl.ndarray((8, nl.par_dim(128), 256), dtype=nl.float32, buffer=nl.sbuf)
   for i in range(8):
     t[i]

   # After migration
   t_lst = []
   # list comprehension is unsupported, use traditional loop
   for i in range(8):
     t_lst.append(nl.ndarray(128, 256), dtype=nl.float32, buffer=nl.sbuf)
   for i in range(8):
     t_list[i]

With this approach, the code generated before and after migration are identical, and should yield the same performance.

Or: Don't use Python ``list`` syntax
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

If blocks need to be alive at the same time, move the block dimension into free dimension, like so:

.. code-block:: python

   a = nl.ndarray((8, par_dim(128), 512), buffer=nl.sbuf, dtype=bfloat16)

   # ----> Migrate to
   a = nl.ndarray((128, 8, 512), buffer=nl.sbuf, dtype=bfloat16)

As an example, all 8 blocks of ``add_buf`` needs to be alive at the same time when the first ``for`` loop finishes. Therefore, the block dimension need to be folded into the free dimension, as seen in this example:

.. code-block:: python

   @nki.jit
   def sb_blocks(inp):
       res = nl.ndarray(shape=(8, 128, 512), dtype=inp.dtype, buffer=nl.shared_hbm)
       add_buf = nl.ndarray(shape=(8, nl.par_dim(128), 512), dtype=inp.dtype, buffer=nl.sbuf)
       for i in range(8):
           add_buf[i] = nl.load(inp[i])
       for i in range(8):
           nl.store(res[i], add_buf[i])
       return res

   # should migrate to
   @nki.jit
   def sb_blocks_migrated(inp):
       res = nl.ndarray(shape=(8, 128, 512), dtype=inp.dtype, buffer=nl.shared_hbm)
       add_buf = nl.ndarray(shape=(128, 8, 512), dtype=inp.dtype, buffer=nl.sbuf)
       for i in range(8):
           add_buf[0:128, i, 0:512] = nl.load(inp[i])
       for i in range(8):
           nl.store(res[i], add_buf[0:128, i, 0:512])
       return res

If 2 or more blocks do not need to be alive at the same time, remove the block dimension and hoist the tensor down:

.. code-block:: python

   a = nl.ndarray((8, par_dim(128), 256))
   for i in nl.affine_range(8):
     <do something with a[i]>

   # should be transformed to ....
   for i in nl.affine_range(8):
     a = nl.ndarray((128, 256))
     <do something with a>

As an example, all 8 blocks of ``add_buf`` do not need to be alive at the same time. We can remove the block dimension and hoist the tensor down inside the loop:

.. code-block:: python

   @nki.jit
   def sb_blocks(inp):
       res = nl.ndarray(shape=(8, 128, 512), dtype=inp.dtype, buffer=nl.shared_hbm)
       add_buf = nl.ndarray(shape=(8, nl.par_dim(128), 512), dtype=inp.dtype, buffer=nl.sbuf)
       for i in range(8):
           add_buf[i] = nl.load(inp[i])
           nl.store(res[i], add_buf[i])
       return res

   # should migrate to
   @nki.jit
   def sb_blocks_migrated(inp):
       res = nl.ndarray(shape=(8, 128, 512), dtype=inp.dtype, buffer=nl.shared_hbm)
       for i in range(8):
           add_buf = nl.ndarray(shape=(128, 512), dtype=inp.dtype, buffer=nl.sbuf)
           add_buf[0:128, 0:512] = nl.load(inp[i])
           nl.store(res[i], add_buf[0:128, 0:512])
       return res

.. important::
    The dependency relationship between loop iterations is different in ``sb_blocks_migrated`` and the following ``sb_blocks_migrated_incorrect``.

    .. code-block:: python

       @nki.jit
       def sb_blocks_migrated_incorrect(inp):
           res = nl.ndarray(shape=(8, 128, 512), dtype=inp.dtype, buffer=nl.shared_hbm)
           add_buf = nl.ndarray(shape=(128, 512), dtype=inp.dtype, buffer=nl.sbuf)
           for i in range(8):
               add_buf[0:128, 0:512] = nl.load(inp[i])
               nl.store(res[i], add_buf[0:128, 0:512])
           return res

    In ``sb_blocks_migrated``, the compiler could unroll the loop and materialize multiple copies of the tensor ``add_buf``. However, in ``sb_blocks_migrated_incorrect``, the execution will be serialized because the loop carries a dependency on ``add_buf``.

.. _dynamic-access-pattern:

Dynamic access patterns
------------------------

Follow this section if your kernel uses dynamic access (such as using a runtime value to index another tensor).

The syntax for representing a dynamic access pattern has changed in NKI Beta 2. In the NKI Beta 1, access with a dynamic scalar offset could be represented as shown in following example, where ``batch_idx`` is the dynamic value in SBUF:

.. code-block:: python

   batch_idx = nl.multiply(nl.bitwise_and(nl.load(dynamic_idx), y=3), 128)
   result = nl.ndarray((128, 256), A.dtype, buffer=nl.shared_hbm)
   batch_idx[...] = 4 # set a constant, but batch_idx is a runtime SBUF value
   i_p, i_f = nl.mgrid[0:128, 0:256]
   nisa.dma_copy(src=A[batch_idx, i_p, i_f], dst=result[...])

Scalar dynamic access
^^^^^^^^^^^^^^^^^^^^^^

Now, in NKI Beta 2, you must use the ``.ap()`` function to represent access with a dynamic scalar offset, as shown in this example:

.. code-block:: python

   def indirect_scalar_dynamic_dma(A):
     batch_idx = nl.ndarray((1, 1), nl.int32, buffer=nl.sbuf)
     # input is of shape (4*128, 512). We want to copy from 3*128
     nisa.memset(batch_idx, value=3*128)

     result = nl.ndarray((128, 256), A.dtype, buffer=nl.shared_hbm)

     nisa.dma_copy(src=A.ap(
       pattern=[[512, 128], [1, 256]], offset=0, 
       scalar_offset=batch_idx, indirect_dim=0
       ),
       dst=result[...])

   return result

In terms of code semantics, dynamic scalar access is performed by starting at this computed location:

``offset + scalar_offset * <accumulated shape to the right of indirect_dim>``

...and follows this pattern across accesses. So, in the example above, the access must start with an element with a computed access location of:

``0 + batch_idx * 512``

Vector dynamic access
^^^^^^^^^^^^^^^^^^^^^^

Vector dynamic access is similar to that of scalar dynamic access, except that you must specify the ``vector_offset`` field, as seen in this example: 

.. note:: Currently, only ``indirect_dimension=0`` is supported.

.. code-block:: python

   def indirect_vector_dynamic_dma(A):
     # shape of A is (128, 512)
     dynamic_idx_legal = nl.ndarray((64, 1), nl.int32, nl.sbuf)
     nisa.iota(dynamic_idx_legal, [[1, 1]], 0, 2)
     
     result_sb = nl.ndarray((64, 512), nl.float32, buffer=nl.sbuf)
     result_hbm = nl.ndarray((64, 512), nl.float32, buffer=nl.shared_hbm)

     nisa.dma_copy(src=A.ap(
       [[512, 64], [1, 512]], 0, vector_offset=dynamic_idx_legal, indirect_dim=0
       ), dst=result_sb, name='inst0')
    
     nisa.dma_copy(result_hbm, result_sb, name="copy1")

     return result_hbm

For this particular case, the semantic for vector dynamic access is:

.. code-block:: python

   indirect_dimension = 0

   for w in range(64):
     for z in range(512):
       dynamic_idx = dynamic_idx_legal[w]
           A[
                  // static offsets
                  offset +
                  // AP with the indirect dimension number replaced
                  1 * z + 512 * dynamic_idx
                 ]

In general, the access semantic is as follows:

.. code-block:: python

    indirect_dimension = 0

    for w in range(W):
    for z in range(Z):
        for y in range(Y):
        for x in range(X):
            // HBM
            dynamic_idx = indirect_tensor[w]
            
            memloc[
                // static offsets
                base_addr + static_start +
                // AP with the indirect dimension number replaced
                s0 * x + s1 * y + s2 * z + s3 * dynamic_idx
                ]
