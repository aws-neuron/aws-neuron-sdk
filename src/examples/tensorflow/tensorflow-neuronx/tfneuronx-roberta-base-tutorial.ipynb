{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e91cf83b",
   "metadata": {},
   "source": [
    "# Running Huggingface Roberta-Base with TensorFlow-NeuronX"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71394e1e",
   "metadata": {},
   "source": [
    "This tutorial demonstrates how to compile the Huggingface roberta-base model and infer on a trn1.2xlarge instance with \n",
    "```tensorflow-neuronx```. To compile larger models like roberta-large, please consider using an inf2 instance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "828ef9bd",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5becc549",
   "metadata": {},
   "source": [
    "To run this tutorial please follow the instructions for [TensorFlow-NeuronX Setup](https://awsdocs-neuron-staging.readthedocs-hosted.com/en/latest/frameworks/tensorflow/tensorflow-neuronx/setup/tensorflow-neuronx-install.html) and the [Jupyter Notebook Quickstart](https://awsdocs-neuron-staging.readthedocs-hosted.com/en/latest/general/setup/notebook/setup-jupyter-notebook-steps-troubleshooting.html) and set your kernel to \"Python (tensorflow-neuronx)\".\n",
    "\n",
    "Next, install some additional dependencies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee1a3b84",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c301cfce",
   "metadata": {},
   "source": [
    "## Download From Huggingface and Compile for AWS-Neuron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92e8050d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_neuronx as tfnx\n",
    "from transformers import RobertaTokenizer, TFRobertaModel\n",
    "from transformers import BertTokenizer, TFBertModel\n",
    "\n",
    "# Create a wrapper for the roberta model that will accept inputs as a list\n",
    "# instead of a dictionary. This will allow the compiled model to be saved\n",
    "# to disk with the model.save() fucntion.\n",
    "class RobertaWrapper(tf.keras.Model):\n",
    "    def __init__(self, model):\n",
    "        super().__init__()\n",
    "        self.model = model\n",
    "    def __call__(self, example_inputs):\n",
    "        return self.model({'input_ids' : example_inputs[0], 'attention_mask' : example_inputs[1]})\n",
    "        \n",
    "\n",
    "tokenizer = RobertaTokenizer.from_pretrained('roberta-base')\n",
    "model = RobertaWrapper(TFRobertaModel.from_pretrained('roberta-base'))\n",
    "\n",
    "batch_size = 16\n",
    "\n",
    "# create example inputs with a batch size of 16\n",
    "text = [\"Paris is the <mask> of France.\"] * batch_size\n",
    "encoded_input = tokenizer(text, return_tensors='tf', padding='max_length', max_length=64)\n",
    "\n",
    "# turn inputs into a list\n",
    "example_input = [encoded_input['input_ids'], encoded_input['attention_mask']]\n",
    "\n",
    "#compile\n",
    "model_neuron = tfnx.trace(model, example_input)\n",
    "\n",
    "print(\"Running on neuron:\", model_neuron(example_input))\n",
    "\n",
    "# save the model to disk to save recompilation time for next usage\n",
    "model_neuron.save('./roberta-neuron-b16')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f2e159a",
   "metadata": {},
   "source": [
    "## Run Basic Inference Benchmarking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccf22e74",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import concurrent.futures\n",
    "import time\n",
    "\n",
    "reloaded_neuron_model = tf.keras.models.load_model('./roberta-neuron-b16')\n",
    "print(\"Reloaded model running on neuron:\", reloaded_neuron_model(example_input))\n",
    "\n",
    "num_threads = 4\n",
    "num_inferences = 1000\n",
    "\n",
    "latency_list = []\n",
    "def inference_with_latency_calculation(example_input):\n",
    "    global latency_list\n",
    "    start = time.time()\n",
    "    result = reloaded_neuron_model(example_input)\n",
    "    end = time.time()\n",
    "    latency_list.append((end-start) * 1000)\n",
    "    return result\n",
    "\n",
    "start = time.time()\n",
    "with concurrent.futures.ThreadPoolExecutor(max_workers=num_threads) as executor:\n",
    "    futures = []\n",
    "    for i in range(num_inferences):\n",
    "        futures.append(executor.submit(inference_with_latency_calculation, example_input))\n",
    "    for future in concurrent.futures.as_completed(futures):\n",
    "        get_result = future.result()\n",
    "end = time.time()\n",
    "\n",
    "total_time = end - start\n",
    "\n",
    "print(f\"Throughput was {(num_inferences * batch_size)/total_time} samples per second.\")\n",
    "print(f\"Latency p50 was {np.percentile(latency_list, 50)} ms\")\n",
    "print(f\"Latency p90 was {np.percentile(latency_list, 90)} ms\")\n",
    "print(f\"Latency p99 was {np.percentile(latency_list, 99)} ms\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (Neuron TensorFlow)",
   "language": "python",
   "name": "aws_neuron_venv_tf"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
