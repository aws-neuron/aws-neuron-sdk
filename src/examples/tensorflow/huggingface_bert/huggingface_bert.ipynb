{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "changing-baking",
   "metadata": {},
   "source": [
    "# Compiling and Deploying Pretrained HuggingFace Pipelines distilBERT with Tensorflow2 Neuron"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cooperative-renewal",
   "metadata": {},
   "source": [
    "### Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "absolute-amateur",
   "metadata": {},
   "source": [
    "In this tutorial you will compile and deploy distilBERT version of HuggingFace ðŸ¤— Transformers BERT for Inferentia. The full list of HuggingFace's pretrained BERT models can be found in the BERT section on this page https://huggingface.co/transformers/pretrained_models.html. you can also read about HuggingFace's pipeline feature here: https://huggingface.co/transformers/main_classes/pipelines.html\n",
    "\n",
    "This Jupyter notebook should be run on an instance which is inf1.6xlarge or larger, but in real life scenario the compilation should be done on a compute instance and the deployment on inf1 instance to save costs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sought-tourist",
   "metadata": {},
   "source": [
    "### Setting up your environment:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fiscal-tyler",
   "metadata": {},
   "source": [
    "To run this tutorial, please make sure you deactivate any existing TensorFlow conda environments you already using. Install TensorFlow 2.x by following the instructions at [TensorFlow Tutorial Setup Guide](https://awsdocs-neuron.readthedocs-hosted.com/en/latest/neuron-guide/neuron-frameworks/tensorflow-neuron/tutorials/tensorflow-tutorial-setup.html#tensorflow-tutorial-setup).\n",
    "\n",
    "After following the Setup Guide, you need to change your kernel to ```Python (Neuron TensorFlow 2)``` by clicking Kerenel->Change Kernel->```Python (Neuron TensorFlow 2)```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "horizontal-finnish",
   "metadata": {},
   "source": [
    "Now you can install TensorFlow Neuron 2.x, HuggingFace transformers, and HuggingFace datasets dependencies here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "electronic-probe",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install --upgrade \"transformers==4.1.0\"\n",
    "!pip install --upgrade \"datasets==1.4.1\"\n",
    "!pip install ipywidgets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adjacent-avatar",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "import tensorflow as tf\n",
    "import tensorflow.neuron as tfn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "british-forwarding",
   "metadata": {},
   "source": [
    "### Compile the model into an AWS Neuron Optimized Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "great-citation",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create the huggingface pipeline for sentiment analysis\n",
    "#this model tries to determine of the input text has a positive\n",
    "#or a negative sentiment.\n",
    "model_name = 'distilbert-base-uncased-finetuned-sst-2-english'\n",
    "\n",
    "pipe = pipeline('sentiment-analysis', model=model_name, framework='tf')\n",
    "\n",
    "#pipelines are extremely easy to use as they do all the tokenization,\n",
    "#inference and output interpretation for you.\n",
    "pipe(['I love pipelines, they are very easy to use!', 'this string makes it batch size two'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "related-component",
   "metadata": {},
   "source": [
    "As yo've seen above, Huggingface's pipline feature is a great wrapper for running inference on their models. It takes care of the tokenization of the string inputs. Then feeds that tokenized input to the model. Finally it interprets the outputs of the model and formats them in a way that is very human readable. Our goal will be to compile the underlying model inside the pipeline as well as make some edits to the tokenizer. The reason you need to edit the tokenizer is to make sure that you have a standard sequence length (in this case 128) as neuron only accepts static input shapes.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ideal-broadway",
   "metadata": {},
   "outputs": [],
   "source": [
    "neuron_pipe = pipeline('sentiment-analysis', model=model_name, framework='tf')\n",
    "\n",
    "#the first step is to modify the underlying tokenizer to create a static \n",
    "#input shape as inferentia does not work with dynamic input shapes\n",
    "original_tokenizer = pipe.tokenizer\n",
    "\n",
    "\n",
    "#you intercept the function call to the original tokenizer\n",
    "#and inject our own code to modify the arguments\n",
    "def wrapper_function(*args, **kwargs):\n",
    "    kwargs['padding'] = 'max_length'\n",
    "    #this is the key line here to set a static input shape\n",
    "    #so that all inputs are set to a len of 128\n",
    "    kwargs['max_length'] = 128 \n",
    "    kwargs['truncation'] = True\n",
    "    kwargs['return_tensors'] = 'tf'\n",
    "    return original_tokenizer(*args, **kwargs)\n",
    "\n",
    "#insert our wrapper function as the new tokenizer as well \n",
    "#as reinserting back some attribute information that was lost\n",
    "#when you replaced the original tokenizer with our wrapper function\n",
    "neuron_pipe.tokenizer = wrapper_function\n",
    "neuron_pipe.tokenizer.decode = original_tokenizer.decode\n",
    "neuron_pipe.tokenizer.mask_token_id = original_tokenizer.mask_token_id\n",
    "neuron_pipe.tokenizer.pad_token_id = original_tokenizer.pad_token_id\n",
    "neuron_pipe.tokenizer.convert_ids_to_tokens = original_tokenizer.convert_ids_to_tokens\n",
    "\n",
    "\n",
    "#Now that our neuron_classifier is ready you can use it to\n",
    "#generate an example input which is needed to compile the model\n",
    "#note that pipe.model is the actual underlying model itself which \n",
    "#is what Tensorflow Neuron actually compiles.\n",
    "from datasets import load_dataset\n",
    "dataset = load_dataset('amazon_polarity')\n",
    "\n",
    "string_inputs = dataset['test'][:128]['content']\n",
    "\n",
    "example_inputs = neuron_pipe.tokenizer(string_inputs)\n",
    "#compile the model by calling tfn.trace by passing in the underlying model\n",
    "#and the example inputs generated by our updated tokenizer\n",
    "neuron_model = tfn.trace(pipe.model, example_inputs)\n",
    "\n",
    "#now you can insert the neuron_model and replace the cpu model\n",
    "#so now you have a huggingface pipeline that uses and underlying neuron model!\n",
    "neuron_pipe.model = neuron_model\n",
    "neuron_pipe.model.config = pipe.model.config"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65663e26",
   "metadata": {},
   "source": [
    "### Why use batch size 128?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2b3c325",
   "metadata": {},
   "source": [
    "You'll notice that in the above example we passed a two tensors of shape 128 (the batch size) x 128 (the sequence length) in this function call ```tfn.trace(pipe.model, example_inputs)```. The example_inputs argument is important to ```tfn.trace``` because it tells the neuron model what to expect (remember that a neuron model needs static input shapes, so example_inputs defines that static input shape). A smaller batch size would also compile, but a large batch size ensures that the neuron hardware will be fed enough data to be as performant as possible."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f15a3e3f",
   "metadata": {},
   "source": [
    "### What if my model isn't a Huggingface pipeline?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dbed4d8",
   "metadata": {},
   "source": [
    "Not to worry! There is no requirement that your model needs to be Huggingface pipeline compatible. The Huggingface pipeline is just a wrapper for an underlying TensorFlow model (in our case ```pipe.model```). As long as you have a TensorFlow 2.x model you can compile it on neuron by calling ```tfn.trace(your_model, example_inputs)```. The processing the input and output to your own model is up to you! Take a look at the example below to see what happens when we call the model without the Huggingface pipeline wrapper as opposed to with it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae38733d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#directly call the model\n",
    "print(neuron_model(example_inputs))\n",
    "#with the model inserted to the wrapper\n",
    "print(neuron_pipe(string_inputs))\n",
    "\n",
    "#Look at the difference between string_inputs\n",
    "#and example_inputs\n",
    "\n",
    "print(example_inputs)\n",
    "print(string_inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "crazy-zoning",
   "metadata": {},
   "source": [
    "### Save your neuron model to disk and avoid recompilation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e867f51",
   "metadata": {},
   "source": [
    "To avoid recompiling the model before every deployment, you can save the neuron model by calling ```model_neuron.save(model_dir)```. This ```save``` method prefers to work on a flat input/output lists and does not work on dictionary input/output - which is what the Huggingface distilBERT expects as input. You can work around this by writing a simple wrapper that takes in an input list instead of a dictionary, compile the wrapped model and save it for later use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5af845f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TFBertForSequenceClassificationFlatIO(tf.keras.Model):\n",
    "    def __init__(self, model):\n",
    "        super().__init__()\n",
    "        self.model = model\n",
    "    def call(self, inputs):\n",
    "        input_ids, attention_mask = inputs\n",
    "        output = self.model({'input_ids': input_ids, 'attention_mask': attention_mask})\n",
    "        return output['logits']\n",
    "\n",
    "#wrap the original model from HuggingFace, now our model accepts a list as input\n",
    "model_wrapped = TFBertForSequenceClassificationFlatIO(pipe.model)\n",
    "#turn the dictionary input into list input\n",
    "example_inputs_list = [example_inputs['input_ids'], example_inputs['attention_mask']]\n",
    "\n",
    "#compile the wrapped model and save it to disk\n",
    "model_wrapped_traced = tfn.trace(model_wrapped, example_inputs_list)\n",
    "model_wrapped_traced.save('./distilbert_b128')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c65c661e",
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls #you should now be able to see the model saved as the folder distilbert_b128"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a113cf0f",
   "metadata": {},
   "source": [
    "### Load the model from disk"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05260d70",
   "metadata": {},
   "source": [
    "Now you can reload the model by calling ```tf.keras.models.load_model(str : model_directory)```. This model is already compiled and could run inference on neuron, but if you want it to work with our Huggingface pipeline, you have to wrap it again to accept dictionary input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b59556a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TFBertForSequenceClassificationDictIO(tf.keras.Model):\n",
    "    def __init__(self, model_wrapped):\n",
    "        super().__init__()\n",
    "        self.model_wrapped = model_wrapped\n",
    "        self.aws_neuron_function = model_wrapped.aws_neuron_function\n",
    "    def call(self, inputs):\n",
    "        input_ids = inputs['input_ids']\n",
    "        attention_mask = inputs['attention_mask']\n",
    "        logits = self.model_wrapped([input_ids, attention_mask])\n",
    "        return [logits]\n",
    "\n",
    "    \n",
    "reloaded_model = tf.keras.models.load_model('./distilbert_b128')\n",
    "rewrapped_model = TFBertForSequenceClassificationDictIO(model_wrapped_traced)\n",
    "\n",
    "#now you can reinsert our reloaded model back into our pipeline\n",
    "neuron_pipe.model = rewrapped_model\n",
    "neuron_pipe.model.config = pipe.model.config"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0712501",
   "metadata": {},
   "source": [
    "### Benchmarking the neuron model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "welcome-supervision",
   "metadata": {},
   "source": [
    "Now you can do some simple benchmarking of the neuron model. If you are running this tutorial on a inf1.6xlarge, as suggested, you must tell neuron to use all 16 Neuron Cores to get maximum throughput. By default, TensorFlow Neuron will use only one Inferentia chip, which has 4 Neuron Cores. An inf1.6xlarge has 4 Inferentia chips. To tell Neuron to run on all available cores, you can set the environment variable ```NEURONCORE_GROUP_SIZES``` or launch multiple processes that query the same model. To read more about this refer to the [Neuron Core groups section](https://awsdocs-neuron.readthedocs-hosted.com/en/latest/neuron-guide/neuron-runtime/nrt-theory-of-operation.html#neuron-core-group) of our documentation. Use a warmup inference on the neuron model before benchmarking, as the first inference call also loads the model onto inferentia."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "amino-listening",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "\n",
    "warnings.warn(\"NEURONCORE_GROUP_SIZES is being deprecated, if your application is using NEURONCORE_GROUP_SIZES please \\\n",
    "see https://awsdocs-neuron.readthedocs-hosted.com/en/latest/release-notes/deprecation.html#announcing-end-of-support-for-neuroncore-group-sizes \\\n",
    "for more details.\", DeprecationWarning)\n",
    "%env NEURONCORE_GROUP_SIZES='16x1'\n",
    "\n",
    "import time\n",
    "\n",
    "#warmup inf\n",
    "neuron_pipe(string_inputs)\n",
    "#benchmark batch 128 neuron model\n",
    "neuron_b128_times = []\n",
    "for i in range(1000):\n",
    "    start = time.time()\n",
    "    outputs = neuron_pipe(string_inputs)\n",
    "    end = time.time()\n",
    "    neuron_b128_times.append(end - start)\n",
    "    \n",
    "\n",
    "neuron_b128_times = sorted(neuron_b128_times)\n",
    "\n",
    "print(f\"Average throughput for batch 128 neuron model is {128/(sum(neuron_b128_times)/len(neuron_b128_times))}.\")\n",
    "print(f\"Peak throughput for batch 128 neuron model is {128/min(neuron_b128_times)}.\")\n",
    "print()\n",
    "\n",
    "\n",
    "print(f\"50th percentile latency for batch 128 neuron model is {neuron_b128_times[int(1000*.5)]}.\")\n",
    "print(f\"90th percentile latency for batch 128 neuron model is {neuron_b128_times[int(1000*.9)]}.\")\n",
    "print(f\"95th percentile latency for bacth 128 neuron model is {neuron_b128_times[int(1000*.95)]}.\")\n",
    "print(f\"99th percentile latency for batch 128 neuron model is {neuron_b128_times[int(1000*.99)]}.\")\n",
    "print()\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (Neuron TensorFlow 2)",
   "language": "python",
   "name": "neuron_tf2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
