{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CodeLlama-13b Inference with 16k sequence length"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this example we compile and deploy the Hugging Face [codellama/CodeLlama-13b-hf](https://huggingface.co/codellama/CodeLlama-13b-hf) model for tensor parallel inference on Neuron using the `Neuronx-Distributed` package. We use 16k sequence length. \n",
    "\n",
    "The example has the following main sections:\n",
    "\n",
    "1. Set up the Jupyter Notebook\n",
    "1. Install dependencies\n",
    "1. Download the model\n",
    "1. Trace the model\n",
    "1. Perform greedy sampling\n",
    "1. Benchmark sampling\n",
    "\n",
    "This Jupyter Notebook can be run on a Trn1 instance (`trn1.32xlarge`). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up the Jupyter Notebook\n",
    "\n",
    "The following steps set up Jupyter Notebook and launch this tutorial:\n",
    "1. Clone the [Neuronx-Distributed](https://github.com/aws-neuron/neuronx-distributed.git) repo to your instance using\n",
    "```\n",
    "git clone https://github.com/aws-neuron/neuronx-distributed.git\n",
    "```\n",
    "\n",
    "2. Navigate to the `examples/inference` samples folder\n",
    "```\n",
    "cd neuronx-distributed/example/inference/\n",
    "```\n",
    "\n",
    "3. Copy the tutorial notebook `codellama_16k_inference.ipynb` to the `example/inference/` directory. \n",
    "```\n",
    "wget https://raw.githubusercontent.com/aws-neuron/aws-neuron-sdk/master/src/examples/pytorch/neuronx_distributed/llama/codellama_16k_inference.ipynb\n",
    "```\n",
    "\n",
    "4. You might need to set `ulimit -n 65536` depending on your OS configuration.\n",
    "\n",
    "5. Follow the instructions in [Jupyter Notebook QuickStart](https://awsdocs-neuron.readthedocs-hosted.com/en/latest/general/setup/notebook/setup-jupyter-notebook-steps-troubleshooting.html) to run Jupyter Notebook on your instance.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Install Dependencies\n",
    "This tutorial requires the following pip packages:\n",
    "\n",
    " - `torch-neuronx`\n",
    " - `neuronx-cc`\n",
    " - `sentencepiece`\n",
    " - `transformers`\n",
    " - `neuronx-distributed`\n",
    "\n",
    "You can install `neuronx-distributed` using the [setup guide](https://awsdocs-neuron.readthedocs-hosted.com/en/latest/libraries/neuronx-distributed/index.html). Most of other packages will be installed when configuring your environment using the [torch-neuronx inference setup guide](https://awsdocs-neuron.readthedocs-hosted.com/en/latest/general/setup/torch-neuronx.html#setup-torch-neuronx). The additional dependencies must be installed here:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install transformers==4.40 sentencepiece "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download the model \n",
    "In order to download the model weights and tokenizer follow the instructions in [codellama/CodeLlama-13b-hf](https://huggingface.co/codellama/CodeLlama-13b-hf). \n",
    "\n",
    "For the purposes of this sample we assume you have saved the CodeLlama-13b model in a directory called `models/CodeLlama-13b-hf`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = \"/home/ubuntu/models/CodeLlama-13b-hf\"\n",
    "traced_model_path = \"/home/ubuntu/models/CodeLlama-13b-hf-trace\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trace and load the model\n",
    "\n",
    "Now we can trace the model using the LlamaRunner script. This saves the model to the `traced_model_path`. After tracing, the model can be loaded.\n",
    "\n",
    "In this sample we use tensor parallelism degree 32 to optimize performance on trn1.32xlarge. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama2.llama2_runner import LlamaRunner\n",
    "\n",
    "# select maximum prompt length and total sequence length\n",
    "max_prompt_length = 12288\n",
    "sequence_length = 16384\n",
    "batch_size = 1\n",
    "tp_degree = 32\n",
    "\n",
    "runner = LlamaRunner(model_path=model_path, \n",
    "                     tokenizer_path=model_path)\n",
    "\n",
    "runner.trace(traced_model_path=traced_model_path,\n",
    "             tp_degree=tp_degree,\n",
    "             batch_size=batch_size,\n",
    "             max_prompt_length=max_prompt_length,\n",
    "             sequence_length=sequence_length,\n",
    "             on_device_sampling=True)\n",
    "\n",
    "neuron_model = runner.load_neuron_model(traced_model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference \n",
    "\n",
    "Now lets use the model to perform autoregressive sampling. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "# construct a tokenizer and encode prompt text (prompt is loaded from a library and appended with instruction to write a function)\n",
    "prompt = requests.get(\"https://raw.githubusercontent.com/kedartatwawadi/stanford_compression_library/e2fca703ac812331a277644ecc4ae5cfef160ab3/scl/compressors/lz77_sliding_window.py\").text\n",
    "prompt += \"\\n\\n# Function to load binary data from user-provided file and compress it with LZ77 and write output to file\\n\" # over 11k tokens\n",
    "\n",
    "prompt = [prompt] # batch size 1\n",
    "generate_ids, outputs = runner.generate_on_neuron(prompt, neuron_model)\n",
    "\n",
    "for idx, output in enumerate(outputs):\n",
    "    print(f\"output {idx}: {output}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aws_neuron_venv_pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
