{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "Python 3.7.6 64-bit ('base': conda)",
   "display_name": "Python 3.7.6 64-bit ('base': conda)",
   "metadata": {
    "interpreter": {
     "hash": "f1042f9c3f844236b3b555ebbe580f8cf54d407899b6992d4d795c5970caf373"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "## Tutorial: Getting started with torch-neuron (resnet-50 tutorial - compile steps in code)\n",
    "**NOTE:** This notebook content represents the compilation parts of the [getting started tutorial](./getting_started.md) - it is not intended to used without reference to the tutorial.  This is why we start at step 3 below :).\n",
    "\n",
    "Steps Overview:\n",
    "\n",
    "We'll cover the following topics as we progress:\n",
    "\n",
    "1. How do I analyze a model for use with AWS Neuron?\n",
    "2. How can I compile for different batch sizes to maximise the throughput of my model on neuron hardware?\n",
    "\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### Step 3: Compile on compilation instance\n",
    "\n",
    "The following cell runs a compilation with a batch size of 1 on the torchvision resnet-50 model (pretrained)\n",
    "\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import os\n",
    "import torch_neuron\n",
    "from torchvision import models\n",
    "import logging\n",
    "\n",
    "## Enable logging so we can see any important warnings\n",
    "logger = logging.getLogger('Neuron')\n",
    "logger.setLevel(logging.INFO)\n",
    "\n",
    "image = torch.zeros([1, 3, 224, 224], dtype=torch.float32)\n",
    "\n",
    "## Load a pretrained ResNet50 model\n",
    "model = models.resnet50(pretrained=True)\n",
    "\n",
    "## Tell the model we are using it for evaluation (not training)\n",
    "model.eval()\n",
    "\n",
    "## Analyze the model - this will show operator support and operator count\n",
    "torch.neuron.analyze_model( model, example_inputs=[image] )\n",
    "\n",
    "## Now compile the model - with logging set to \"info\" we will see\n",
    "## what compiles for Neuron, and if there are any fallbacks\n",
    "model_neuron = torch.neuron.trace(model, example_inputs=[image], compiler_args='-O2')\n",
    "\n",
    "## Export to saved model\n",
    "model_neuron.save(\"resnet50_neuron.pt\")"
   ]
  },
  {
   "source": [
    "You should see something like this, if your instance is correctly configured (your output may vary in some details):\n",
    "\n",
    "```\n",
    "INFO:Neuron:The following operations are currently supported in torch-neuron for this model:\n",
    "INFO:Neuron:aten::relu\n",
    "INFO:Neuron:aten::flatten\n",
    "INFO:Neuron:aten::t\n",
    "INFO:Neuron:aten::max_pool2d\n",
    "INFO:Neuron:aten::add\n",
    "INFO:Neuron:aten::addmm\n",
    "INFO:Neuron:aten::_convolution\n",
    "INFO:Neuron:aten::batch_norm\n",
    "INFO:Neuron:aten::adaptive_avg_pool2d\n",
    "INFO:Neuron:prim::ListConstruct\n",
    "INFO:Neuron:prim::Constant\n",
    "INFO:Neuron:100.00% of all operations (including primitives) (1645 of 1645) are supported\n",
    "INFO:Neuron:100.00% of arithmetic operations (176 of 176) are supported\n",
    "OrderedDict([('percent_supported', 100.0), ('percent_supported_arithmetic', 100.0), ('supported_count', 1645), ('total_count', 1645), ('supported_count_arithmetic', 176), ('total_count_arithmetic', 176), ('supported_operators', {'aten::relu', 'aten::flatten', 'aten::t', 'aten::max_pool2d', 'aten::add', 'aten::addmm', 'aten::_convolution', 'aten::batch_norm', 'aten::adaptive_avg_pool2d', 'prim::ListConstruct', 'prim::Constant'}), ('unsupported_operators', []), ('operators', ['aten::_convolution', 'aten::adaptive_avg_pool2d', 'aten::add', 'aten::addmm', 'aten::batch_norm', 'aten::flatten', 'aten::max_pool2d', 'aten::relu', 'aten::t', 'prim::Constant', 'prim::ListConstruct']), ('operator_count', OrderedDict([('aten::_convolution', 53), ('aten::adaptive_avg_pool2d', 1), ('aten::add', 16), ('aten::addmm', 1), ('aten::batch_norm', 53), ('aten::flatten', 1), ('aten::max_pool2d', 1), ('aten::relu', 49), ('aten::t', 1), ('prim::Constant', 1252), ('prim::ListConstruct', 217)]))])\n",
    "INFO:Neuron:Number of arithmetic operators (pre-compilation) before = 176, fused = 176, percent fused = 100.0%\n",
    "INFO:Neuron:compiling function _NeuronGraph$1108 with neuron-cc\n",
    "INFO:Neuron:Compiling with command line: '/home/ubuntu/test_beta_env/bin/neuron-cc compile /tmp/tmp2fisdcmu/graph_def.pb --framework TENSORFLOW --pipeline compile SaveTemps --output /tmp/tmp2fisdcmu/graph_def.neff --io-config {\"inputs\": {\"0:0\": [[1, 3, 224, 224], \"float32\"]}, \"outputs\": [\"Add_69:0\"]}''\n",
    "INFO:Neuron:Number of arithmetic operators (post-compilation) before = 176, compiled = 176, percent compiled = 100.0%\n",
    "```\n",
    "\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "You can find more on torch.neuron.analyze_model and torch.neuron.trace using the following commands"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "import torch\n",
    "import torch.neuron\n",
    "\n",
    "help(torch.neuron.analyze_model)\n",
    "help(torch.neuron.trace)"
   ],
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "source": [
    "3.3 WARNING: If you run the inference script (in section 4 below) on your CPU instance you will get output, but see the following warning.\n",
    "\n",
    "```\n",
    "[E neuron_op_impl.cpp:53] Warning: Tensor output are *** NOT CALCULATED *** during CPU\n",
    "execution and only indicate tensor shape\n",
    "```\n",
    "\n",
    "The warning is also displayed during trace (where it is expected).  It is not a concern that you see this during compilation\n",
    "\n",
    "This is an artifact of the way we trace a model on your compile instance. \n",
    "\n",
    "NB: Do not perform inference with a neuron traced model on a non neuron supported instance, results will not be calculated, and this warning will be shown\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "3.4. If not compiling and inferring on the same instance, copy the compiled artifacts to the inference server:\n",
    "\n",
    "```\n",
    "scp -i <PEM key file>  ./resnet50_neuron.pt ubuntu@<instance DNS>:~/ # if Ubuntu-based AMI\n",
    "scp -i <PEM key file>  ./resnet50_neuron.pt ec2-user@<instance DNS>:~/  # if using AML2-based AMI\n",
    "```\n",
    "\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## Step 7: Experiment with different batch sizes:\n",
    "\n",
    "Now that we are using all four cores we can experiment with compiling and running large batch sizes on each of our four cores.  Here will will provide the compilation code for batch 5\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import osimport torch_neuron\n",
    "from torchvision \n",
    "import models\n",
    "\n",
    "## Enable logging so we can see any important warnings\n",
    "logger = logging.getLogger('Neuron')\n",
    "logger.setLevel(logging.INFO)\n",
    "\n",
    "batch_size = 5\n",
    "\n",
    "image = torch.zeros([batch_size, 3, 224, 224], dtype=torch.float32)\n",
    "\n",
    "## Load a pretrained ResNet50 model\n",
    "model = models.resnet50(pretrained=True)\n",
    "\n",
    "## Tell the model we are using it for evaluation (not training)\n",
    "model.eval()\n",
    "\n",
    "## Analyze the model - this will show operator support and operator count\n",
    "torch.neuron.analyze_model( model, example_inputs=[image] )\n",
    "\n",
    "## Now compile the model\n",
    "model_neuron = torch.neuron.trace(model, example_inputs=[image], compiler_args='-O2')\n",
    "\n",
    "## Export to saved model\n",
    "model_neuron.save(\"resnet50_neuron_b{}.pt\".format(batch_size))"
   ],
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "source": [
    "7.2. If not compiling and inferring on the same instance, copy the compiled artifacts to the inference server:\n",
    "\n",
    "```\n",
    "scp -i <PEM key file>  ./resnet50_neuron_b5.pt ubuntu@<instance DNS>:~/ # if Ubuntu-based AMI\n",
    "scp -i <PEM key file>  ./resnet50_neuron_b5.pt ec2-user@<instance DNS>:~/  # if using AML2-based AMI\n",
    "```"
   ],
   "cell_type": "markdown",
   "metadata": {}
  }
 ]
}
