{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate YOLO v4 on Inferentia"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "This tutorial walks through compiling and evaluating YOLO v4 model implemented in PyTorch on Inferentia using the AWS Neuron SDK 1.11 release or greater. We recommend running this tutorial on an EC2 inf1.2xlarge instance which contains one Inferentia and 8 vCPU cores, as well as 16 GB of memory."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prerequisites\n",
    "This demo requires the following pip packages:\n",
    "\n",
    "torch-neuron neuron-cc tensorflow~=1.15.0 requests pillow matplotlib pycocotools\n",
    "\n",
    "and debian/rpm package aws-neuron-runtime.\n",
    "\n",
    "On DLAMI, aws-neuron-runtime is already pre-installed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install torch-neuron~=1.7.0 torchvision~=0.8.0 neuron-cc tensorflow~=1.15.0 requests pillow matplotlib pycocotools --force \\\n",
    "    --extra-index-url=https://pip.repos.neuron.amazonaws.com"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before starting the demo, it is recommended to reset the Neuron runtime with `neuron-cli`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!/opt/aws/neuron/bin/neuron-cli reset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Define YOLO v4 model in PyTorch \n",
    "The following PyTorch model definition is from https://github.com/Tianxiaomo/pytorch-YOLOv4/."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "class Mish(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x * (torch.tanh(torch.nn.functional.softplus(x)))\n",
    "        return x\n",
    "\n",
    "\n",
    "class Upsample(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Upsample, self).__init__()\n",
    "\n",
    "    def forward(self, x, target_size, inference=False):\n",
    "        assert (x.data.dim() == 4)\n",
    "        # _, _, tH, tW = target_size\n",
    "\n",
    "        if inference:\n",
    "\n",
    "            #B = x.data.size(0)\n",
    "            #C = x.data.size(1)\n",
    "            #H = x.data.size(2)\n",
    "            #W = x.data.size(3)\n",
    "\n",
    "            return x.view(x.size(0), x.size(1), x.size(2), 1, x.size(3), 1).\\\n",
    "                    expand(x.size(0), x.size(1), x.size(2), target_size[2] // x.size(2), x.size(3), target_size[3] // x.size(3)).\\\n",
    "                    contiguous().view(x.size(0), x.size(1), target_size[2], target_size[3])\n",
    "        else:\n",
    "            return F.interpolate(x, size=(target_size[2], target_size[3]), mode='nearest')\n",
    "\n",
    "\n",
    "class Conv_Bn_Activation(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, kernel_size, stride, activation, bn=True, bias=False):\n",
    "        super().__init__()\n",
    "        pad = (kernel_size - 1) // 2\n",
    "\n",
    "        self.conv = nn.ModuleList()\n",
    "        if bias:\n",
    "            self.conv.append(nn.Conv2d(in_channels, out_channels, kernel_size, stride, pad))\n",
    "        else:\n",
    "            self.conv.append(nn.Conv2d(in_channels, out_channels, kernel_size, stride, pad, bias=False))\n",
    "        if bn:\n",
    "            self.conv.append(nn.BatchNorm2d(out_channels))\n",
    "        if activation == \"mish\":\n",
    "            self.conv.append(Mish())\n",
    "        elif activation == \"relu\":\n",
    "            self.conv.append(nn.ReLU(inplace=True))\n",
    "        elif activation == \"leaky\":\n",
    "            self.conv.append(nn.LeakyReLU(0.1, inplace=True))\n",
    "        elif activation == \"linear\":\n",
    "            pass\n",
    "        else:\n",
    "            print(\"activate error !!! {} {} {}\".format(sys._getframe().f_code.co_filename,\n",
    "                                                       sys._getframe().f_code.co_name, sys._getframe().f_lineno))\n",
    "\n",
    "    def forward(self, x):\n",
    "        for l in self.conv:\n",
    "            x = l(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class ResBlock(nn.Module):\n",
    "    \"\"\"\n",
    "    Sequential residual blocks each of which consists of \\\n",
    "    two convolution layers.\n",
    "    Args:\n",
    "        ch (int): number of input and output channels.\n",
    "        nblocks (int): number of residual blocks.\n",
    "        shortcut (bool): if True, residual tensor addition is enabled.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, ch, nblocks=1, shortcut=True):\n",
    "        super().__init__()\n",
    "        self.shortcut = shortcut\n",
    "        self.module_list = nn.ModuleList()\n",
    "        for i in range(nblocks):\n",
    "            resblock_one = nn.ModuleList()\n",
    "            resblock_one.append(Conv_Bn_Activation(ch, ch, 1, 1, 'mish'))\n",
    "            resblock_one.append(Conv_Bn_Activation(ch, ch, 3, 1, 'mish'))\n",
    "            self.module_list.append(resblock_one)\n",
    "\n",
    "    def forward(self, x):\n",
    "        for module in self.module_list:\n",
    "            h = x\n",
    "            for res in module:\n",
    "                h = res(h)\n",
    "            x = x + h if self.shortcut else h\n",
    "        return x\n",
    "\n",
    "\n",
    "class DownSample1(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = Conv_Bn_Activation(3, 32, 3, 1, 'mish')\n",
    "\n",
    "        self.conv2 = Conv_Bn_Activation(32, 64, 3, 2, 'mish')\n",
    "        self.conv3 = Conv_Bn_Activation(64, 64, 1, 1, 'mish')\n",
    "        # [route]\n",
    "        # layers = -2\n",
    "        self.conv4 = Conv_Bn_Activation(64, 64, 1, 1, 'mish')\n",
    "\n",
    "        self.conv5 = Conv_Bn_Activation(64, 32, 1, 1, 'mish')\n",
    "        self.conv6 = Conv_Bn_Activation(32, 64, 3, 1, 'mish')\n",
    "        # [shortcut]\n",
    "        # from=-3\n",
    "        # activation = linear\n",
    "\n",
    "        self.conv7 = Conv_Bn_Activation(64, 64, 1, 1, 'mish')\n",
    "        # [route]\n",
    "        # layers = -1, -7\n",
    "        self.conv8 = Conv_Bn_Activation(128, 64, 1, 1, 'mish')\n",
    "\n",
    "    def forward(self, input):\n",
    "        x1 = self.conv1(input)\n",
    "        x2 = self.conv2(x1)\n",
    "        x3 = self.conv3(x2)\n",
    "        # route -2\n",
    "        x4 = self.conv4(x2)\n",
    "        x5 = self.conv5(x4)\n",
    "        x6 = self.conv6(x5)\n",
    "        # shortcut -3\n",
    "        x6 = x6 + x4\n",
    "\n",
    "        x7 = self.conv7(x6)\n",
    "        # [route]\n",
    "        # layers = -1, -7\n",
    "        x7 = torch.cat([x7, x3], dim=1)\n",
    "        x8 = self.conv8(x7)\n",
    "        return x8\n",
    "\n",
    "\n",
    "class DownSample2(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = Conv_Bn_Activation(64, 128, 3, 2, 'mish')\n",
    "        self.conv2 = Conv_Bn_Activation(128, 64, 1, 1, 'mish')\n",
    "        # r -2\n",
    "        self.conv3 = Conv_Bn_Activation(128, 64, 1, 1, 'mish')\n",
    "\n",
    "        self.resblock = ResBlock(ch=64, nblocks=2)\n",
    "\n",
    "        # s -3\n",
    "        self.conv4 = Conv_Bn_Activation(64, 64, 1, 1, 'mish')\n",
    "        # r -1 -10\n",
    "        self.conv5 = Conv_Bn_Activation(128, 128, 1, 1, 'mish')\n",
    "\n",
    "    def forward(self, input):\n",
    "        x1 = self.conv1(input)\n",
    "        x2 = self.conv2(x1)\n",
    "        x3 = self.conv3(x1)\n",
    "\n",
    "        r = self.resblock(x3)\n",
    "        x4 = self.conv4(r)\n",
    "\n",
    "        x4 = torch.cat([x4, x2], dim=1)\n",
    "        x5 = self.conv5(x4)\n",
    "        return x5\n",
    "\n",
    "\n",
    "class DownSample3(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = Conv_Bn_Activation(128, 256, 3, 2, 'mish')\n",
    "        self.conv2 = Conv_Bn_Activation(256, 128, 1, 1, 'mish')\n",
    "        self.conv3 = Conv_Bn_Activation(256, 128, 1, 1, 'mish')\n",
    "\n",
    "        self.resblock = ResBlock(ch=128, nblocks=8)\n",
    "        self.conv4 = Conv_Bn_Activation(128, 128, 1, 1, 'mish')\n",
    "        self.conv5 = Conv_Bn_Activation(256, 256, 1, 1, 'mish')\n",
    "\n",
    "    def forward(self, input):\n",
    "        x1 = self.conv1(input)\n",
    "        x2 = self.conv2(x1)\n",
    "        x3 = self.conv3(x1)\n",
    "\n",
    "        r = self.resblock(x3)\n",
    "        x4 = self.conv4(r)\n",
    "\n",
    "        x4 = torch.cat([x4, x2], dim=1)\n",
    "        x5 = self.conv5(x4)\n",
    "        return x5\n",
    "\n",
    "\n",
    "class DownSample4(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = Conv_Bn_Activation(256, 512, 3, 2, 'mish')\n",
    "        self.conv2 = Conv_Bn_Activation(512, 256, 1, 1, 'mish')\n",
    "        self.conv3 = Conv_Bn_Activation(512, 256, 1, 1, 'mish')\n",
    "\n",
    "        self.resblock = ResBlock(ch=256, nblocks=8)\n",
    "        self.conv4 = Conv_Bn_Activation(256, 256, 1, 1, 'mish')\n",
    "        self.conv5 = Conv_Bn_Activation(512, 512, 1, 1, 'mish')\n",
    "\n",
    "    def forward(self, input):\n",
    "        x1 = self.conv1(input)\n",
    "        x2 = self.conv2(x1)\n",
    "        x3 = self.conv3(x1)\n",
    "\n",
    "        r = self.resblock(x3)\n",
    "        x4 = self.conv4(r)\n",
    "\n",
    "        x4 = torch.cat([x4, x2], dim=1)\n",
    "        x5 = self.conv5(x4)\n",
    "        return x5\n",
    "\n",
    "\n",
    "class DownSample5(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = Conv_Bn_Activation(512, 1024, 3, 2, 'mish')\n",
    "        self.conv2 = Conv_Bn_Activation(1024, 512, 1, 1, 'mish')\n",
    "        self.conv3 = Conv_Bn_Activation(1024, 512, 1, 1, 'mish')\n",
    "\n",
    "        self.resblock = ResBlock(ch=512, nblocks=4)\n",
    "        self.conv4 = Conv_Bn_Activation(512, 512, 1, 1, 'mish')\n",
    "        self.conv5 = Conv_Bn_Activation(1024, 1024, 1, 1, 'mish')\n",
    "\n",
    "    def forward(self, input):\n",
    "        x1 = self.conv1(input)\n",
    "        x2 = self.conv2(x1)\n",
    "        x3 = self.conv3(x1)\n",
    "\n",
    "        r = self.resblock(x3)\n",
    "        x4 = self.conv4(r)\n",
    "\n",
    "        x4 = torch.cat([x4, x2], dim=1)\n",
    "        x5 = self.conv5(x4)\n",
    "        return x5\n",
    "\n",
    "\n",
    "class Neck(nn.Module):\n",
    "    def __init__(self, inference=False):\n",
    "        super().__init__()\n",
    "        self.inference = inference\n",
    "\n",
    "        self.conv1 = Conv_Bn_Activation(1024, 512, 1, 1, 'leaky')\n",
    "        self.conv2 = Conv_Bn_Activation(512, 1024, 3, 1, 'leaky')\n",
    "        self.conv3 = Conv_Bn_Activation(1024, 512, 1, 1, 'leaky')\n",
    "        # SPP\n",
    "        self.maxpool1 = nn.MaxPool2d(kernel_size=5, stride=1, padding=5 // 2)\n",
    "        self.maxpool2 = nn.MaxPool2d(kernel_size=9, stride=1, padding=9 // 2)\n",
    "        self.maxpool3 = nn.MaxPool2d(kernel_size=13, stride=1, padding=13 // 2)\n",
    "\n",
    "        # R -1 -3 -5 -6\n",
    "        # SPP\n",
    "        self.conv4 = Conv_Bn_Activation(2048, 512, 1, 1, 'leaky')\n",
    "        self.conv5 = Conv_Bn_Activation(512, 1024, 3, 1, 'leaky')\n",
    "        self.conv6 = Conv_Bn_Activation(1024, 512, 1, 1, 'leaky')\n",
    "        self.conv7 = Conv_Bn_Activation(512, 256, 1, 1, 'leaky')\n",
    "        # UP\n",
    "        self.upsample1 = Upsample()\n",
    "        # R 85\n",
    "        self.conv8 = Conv_Bn_Activation(512, 256, 1, 1, 'leaky')\n",
    "        # R -1 -3\n",
    "        self.conv9 = Conv_Bn_Activation(512, 256, 1, 1, 'leaky')\n",
    "        self.conv10 = Conv_Bn_Activation(256, 512, 3, 1, 'leaky')\n",
    "        self.conv11 = Conv_Bn_Activation(512, 256, 1, 1, 'leaky')\n",
    "        self.conv12 = Conv_Bn_Activation(256, 512, 3, 1, 'leaky')\n",
    "        self.conv13 = Conv_Bn_Activation(512, 256, 1, 1, 'leaky')\n",
    "        self.conv14 = Conv_Bn_Activation(256, 128, 1, 1, 'leaky')\n",
    "        # UP\n",
    "        self.upsample2 = Upsample()\n",
    "        # R 54\n",
    "        self.conv15 = Conv_Bn_Activation(256, 128, 1, 1, 'leaky')\n",
    "        # R -1 -3\n",
    "        self.conv16 = Conv_Bn_Activation(256, 128, 1, 1, 'leaky')\n",
    "        self.conv17 = Conv_Bn_Activation(128, 256, 3, 1, 'leaky')\n",
    "        self.conv18 = Conv_Bn_Activation(256, 128, 1, 1, 'leaky')\n",
    "        self.conv19 = Conv_Bn_Activation(128, 256, 3, 1, 'leaky')\n",
    "        self.conv20 = Conv_Bn_Activation(256, 128, 1, 1, 'leaky')\n",
    "\n",
    "    def forward(self, input, downsample4, downsample3, inference=False):\n",
    "        x1 = self.conv1(input)\n",
    "        x2 = self.conv2(x1)\n",
    "        x3 = self.conv3(x2)\n",
    "        # SPP\n",
    "        m1 = self.maxpool1(x3)\n",
    "        m2 = self.maxpool2(x3)\n",
    "        m3 = self.maxpool3(x3)\n",
    "        spp = torch.cat([m3, m2, m1, x3], dim=1)\n",
    "        # SPP end\n",
    "        x4 = self.conv4(spp)\n",
    "        x5 = self.conv5(x4)\n",
    "        x6 = self.conv6(x5)\n",
    "        x7 = self.conv7(x6)\n",
    "        # UP\n",
    "        up = self.upsample1(x7, downsample4.size(), self.inference)\n",
    "        # R 85\n",
    "        x8 = self.conv8(downsample4)\n",
    "        # R -1 -3\n",
    "        x8 = torch.cat([x8, up], dim=1)\n",
    "\n",
    "        x9 = self.conv9(x8)\n",
    "        x10 = self.conv10(x9)\n",
    "        x11 = self.conv11(x10)\n",
    "        x12 = self.conv12(x11)\n",
    "        x13 = self.conv13(x12)\n",
    "        x14 = self.conv14(x13)\n",
    "\n",
    "        # UP\n",
    "        up = self.upsample2(x14, downsample3.size(), self.inference)\n",
    "        # R 54\n",
    "        x15 = self.conv15(downsample3)\n",
    "        # R -1 -3\n",
    "        x15 = torch.cat([x15, up], dim=1)\n",
    "\n",
    "        x16 = self.conv16(x15)\n",
    "        x17 = self.conv17(x16)\n",
    "        x18 = self.conv18(x17)\n",
    "        x19 = self.conv19(x18)\n",
    "        x20 = self.conv20(x19)\n",
    "        return x20, x13, x6\n",
    "\n",
    "\n",
    "class Yolov4Head(nn.Module):\n",
    "    def __init__(self, output_ch, n_classes, inference=False):\n",
    "        super().__init__()\n",
    "        self.inference = inference\n",
    "\n",
    "        self.conv1 = Conv_Bn_Activation(128, 256, 3, 1, 'leaky')\n",
    "        self.conv2 = Conv_Bn_Activation(256, output_ch, 1, 1, 'linear', bn=False, bias=True)\n",
    "\n",
    "        self.yolo1 = YoloLayer(\n",
    "                                anchor_mask=[0, 1, 2], num_classes=n_classes,\n",
    "                                anchors=[12, 16, 19, 36, 40, 28, 36, 75, 76, 55, 72, 146, 142, 110, 192, 243, 459, 401],\n",
    "                                num_anchors=9, stride=8)\n",
    "\n",
    "        # R -4\n",
    "        self.conv3 = Conv_Bn_Activation(128, 256, 3, 2, 'leaky')\n",
    "\n",
    "        # R -1 -16\n",
    "        self.conv4 = Conv_Bn_Activation(512, 256, 1, 1, 'leaky')\n",
    "        self.conv5 = Conv_Bn_Activation(256, 512, 3, 1, 'leaky')\n",
    "        self.conv6 = Conv_Bn_Activation(512, 256, 1, 1, 'leaky')\n",
    "        self.conv7 = Conv_Bn_Activation(256, 512, 3, 1, 'leaky')\n",
    "        self.conv8 = Conv_Bn_Activation(512, 256, 1, 1, 'leaky')\n",
    "        self.conv9 = Conv_Bn_Activation(256, 512, 3, 1, 'leaky')\n",
    "        self.conv10 = Conv_Bn_Activation(512, output_ch, 1, 1, 'linear', bn=False, bias=True)\n",
    "        \n",
    "        self.yolo2 = YoloLayer(\n",
    "                                anchor_mask=[3, 4, 5], num_classes=n_classes,\n",
    "                                anchors=[12, 16, 19, 36, 40, 28, 36, 75, 76, 55, 72, 146, 142, 110, 192, 243, 459, 401],\n",
    "                                num_anchors=9, stride=16)\n",
    "\n",
    "        # R -4\n",
    "        self.conv11 = Conv_Bn_Activation(256, 512, 3, 2, 'leaky')\n",
    "\n",
    "        # R -1 -37\n",
    "        self.conv12 = Conv_Bn_Activation(1024, 512, 1, 1, 'leaky')\n",
    "        self.conv13 = Conv_Bn_Activation(512, 1024, 3, 1, 'leaky')\n",
    "        self.conv14 = Conv_Bn_Activation(1024, 512, 1, 1, 'leaky')\n",
    "        self.conv15 = Conv_Bn_Activation(512, 1024, 3, 1, 'leaky')\n",
    "        self.conv16 = Conv_Bn_Activation(1024, 512, 1, 1, 'leaky')\n",
    "        self.conv17 = Conv_Bn_Activation(512, 1024, 3, 1, 'leaky')\n",
    "        self.conv18 = Conv_Bn_Activation(1024, output_ch, 1, 1, 'linear', bn=False, bias=True)\n",
    "        \n",
    "        self.yolo3 = YoloLayer(\n",
    "                                anchor_mask=[6, 7, 8], num_classes=n_classes,\n",
    "                                anchors=[12, 16, 19, 36, 40, 28, 36, 75, 76, 55, 72, 146, 142, 110, 192, 243, 459, 401],\n",
    "                                num_anchors=9, stride=32)\n",
    "\n",
    "    def forward(self, input1, input2, input3):\n",
    "        x1 = self.conv1(input1)\n",
    "        x2 = self.conv2(x1)\n",
    "\n",
    "        x3 = self.conv3(input1)\n",
    "        # R -1 -16\n",
    "        x3 = torch.cat([x3, input2], dim=1)\n",
    "        x4 = self.conv4(x3)\n",
    "        x5 = self.conv5(x4)\n",
    "        x6 = self.conv6(x5)\n",
    "        x7 = self.conv7(x6)\n",
    "        x8 = self.conv8(x7)\n",
    "        x9 = self.conv9(x8)\n",
    "        x10 = self.conv10(x9)\n",
    "\n",
    "        # R -4\n",
    "        x11 = self.conv11(x8)\n",
    "        # R -1 -37\n",
    "        x11 = torch.cat([x11, input3], dim=1)\n",
    "\n",
    "        x12 = self.conv12(x11)\n",
    "        x13 = self.conv13(x12)\n",
    "        x14 = self.conv14(x13)\n",
    "        x15 = self.conv15(x14)\n",
    "        x16 = self.conv16(x15)\n",
    "        x17 = self.conv17(x16)\n",
    "        x18 = self.conv18(x17)\n",
    "        \n",
    "        if self.inference:\n",
    "            y1 = self.yolo1(x2)\n",
    "            y2 = self.yolo2(x10)\n",
    "            y3 = self.yolo3(x18)\n",
    "\n",
    "            return get_region_boxes([y1, y2, y3])\n",
    "        \n",
    "        else:\n",
    "            return [x2, x10, x18]\n",
    "\n",
    "\n",
    "class Yolov4(nn.Module):\n",
    "    def __init__(self, yolov4conv137weight=None, n_classes=80, inference=False):\n",
    "        super().__init__()\n",
    "\n",
    "        output_ch = (4 + 1 + n_classes) * 3\n",
    "\n",
    "        # backbone\n",
    "        self.down1 = DownSample1()\n",
    "        self.down2 = DownSample2()\n",
    "        self.down3 = DownSample3()\n",
    "        self.down4 = DownSample4()\n",
    "        self.down5 = DownSample5()\n",
    "        # neck\n",
    "        self.neek = Neck(inference)\n",
    "        # yolov4conv137\n",
    "        if yolov4conv137weight:\n",
    "            _model = nn.Sequential(self.down1, self.down2, self.down3, self.down4, self.down5, self.neek)\n",
    "            pretrained_dict = torch.load(yolov4conv137weight)\n",
    "\n",
    "            model_dict = _model.state_dict()\n",
    "            # 1. filter out unnecessary keys\n",
    "            pretrained_dict = {k1: v for (k, v), k1 in zip(pretrained_dict.items(), model_dict)}\n",
    "            # 2. overwrite entries in the existing state dict\n",
    "            model_dict.update(pretrained_dict)\n",
    "            _model.load_state_dict(model_dict)\n",
    "        \n",
    "        # head\n",
    "        self.head = Yolov4Head(output_ch, n_classes, inference)\n",
    "\n",
    "\n",
    "    def forward(self, input):\n",
    "        d1 = self.down1(input)\n",
    "        d2 = self.down2(d1)\n",
    "        d3 = self.down3(d2)\n",
    "        d4 = self.down4(d3)\n",
    "        d5 = self.down5(d4)\n",
    "\n",
    "        x20, x13, x6 = self.neek(d5, d4, d3)\n",
    "\n",
    "        output = self.head(x20, x13, x6)\n",
    "        return output\n",
    "\n",
    "\n",
    "def yolo_forward_dynamic(output, conf_thresh, num_classes, anchors, num_anchors, scale_x_y, only_objectness=1,\n",
    "                              validation=False):\n",
    "    # Output would be invalid if it does not satisfy this assert\n",
    "    # assert (output.size(1) == (5 + num_classes) * num_anchors)\n",
    "\n",
    "    # print(output.size())\n",
    "\n",
    "    # Slice the second dimension (channel) of output into:\n",
    "    # [ 2, 2, 1, num_classes, 2, 2, 1, num_classes, 2, 2, 1, num_classes ]\n",
    "    # And then into\n",
    "    # bxy = [ 6 ] bwh = [ 6 ] det_conf = [ 3 ] cls_conf = [ num_classes * 3 ]\n",
    "    # batch = output.size(0)\n",
    "    # H = output.size(2)\n",
    "    # W = output.size(3)\n",
    "\n",
    "    bxy_list = []\n",
    "    bwh_list = []\n",
    "    det_confs_list = []\n",
    "    cls_confs_list = []\n",
    "\n",
    "    for i in range(num_anchors):\n",
    "        begin = i * (5 + num_classes)\n",
    "        end = (i + 1) * (5 + num_classes)\n",
    "        \n",
    "        bxy_list.append(output[:, begin : begin + 2])\n",
    "        bwh_list.append(output[:, begin + 2 : begin + 4])\n",
    "        det_confs_list.append(output[:, begin + 4 : begin + 5])\n",
    "        cls_confs_list.append(output[:, begin + 5 : end])\n",
    "\n",
    "    # Shape: [batch, num_anchors * 2, H, W]\n",
    "    bxy = torch.cat(bxy_list, dim=1)\n",
    "    # Shape: [batch, num_anchors * 2, H, W]\n",
    "    bwh = torch.cat(bwh_list, dim=1)\n",
    "\n",
    "    # Shape: [batch, num_anchors, H, W]\n",
    "    det_confs = torch.cat(det_confs_list, dim=1)\n",
    "    # Shape: [batch, num_anchors * H * W]\n",
    "    det_confs = det_confs.view(output.size(0), num_anchors * output.size(2) * output.size(3))\n",
    "\n",
    "    # Shape: [batch, num_anchors * num_classes, H, W]\n",
    "    cls_confs = torch.cat(cls_confs_list, dim=1)\n",
    "    # Shape: [batch, num_anchors, num_classes, H * W]\n",
    "    cls_confs = cls_confs.view(output.size(0), num_anchors, num_classes, output.size(2) * output.size(3))\n",
    "    # Shape: [batch, num_anchors, num_classes, H * W] --> [batch, num_anchors * H * W, num_classes] \n",
    "    cls_confs = cls_confs.permute(0, 1, 3, 2).reshape(output.size(0), num_anchors * output.size(2) * output.size(3), num_classes)\n",
    "\n",
    "    # Apply sigmoid(), exp() and softmax() to slices\n",
    "    #\n",
    "    bxy = torch.sigmoid(bxy) * scale_x_y - 0.5 * (scale_x_y - 1)\n",
    "    bwh = torch.exp(bwh)\n",
    "    det_confs = torch.sigmoid(det_confs)\n",
    "    cls_confs = torch.sigmoid(cls_confs)\n",
    "\n",
    "    # Prepare C-x, C-y, P-w, P-h (None of them are torch related)\n",
    "    grid_x = np.expand_dims(np.expand_dims(np.expand_dims(np.linspace(0, output.size(3) - 1, output.size(3)), axis=0).repeat(output.size(2), 0), axis=0), axis=0)\n",
    "    grid_y = np.expand_dims(np.expand_dims(np.expand_dims(np.linspace(0, output.size(2) - 1, output.size(2)), axis=1).repeat(output.size(3), 1), axis=0), axis=0)\n",
    "    # grid_x = torch.linspace(0, W - 1, W).reshape(1, 1, 1, W).repeat(1, 1, H, 1)\n",
    "    # grid_y = torch.linspace(0, H - 1, H).reshape(1, 1, H, 1).repeat(1, 1, 1, W)\n",
    "\n",
    "    anchor_w = []\n",
    "    anchor_h = []\n",
    "    for i in range(num_anchors):\n",
    "        anchor_w.append(anchors[i * 2])\n",
    "        anchor_h.append(anchors[i * 2 + 1])\n",
    "\n",
    "    device = None\n",
    "    cuda_check = output.is_cuda\n",
    "    if cuda_check:\n",
    "        device = output.get_device()\n",
    "\n",
    "    bx_list = []\n",
    "    by_list = []\n",
    "    bw_list = []\n",
    "    bh_list = []\n",
    "\n",
    "    # Apply C-x, C-y, P-w, P-h\n",
    "    for i in range(num_anchors):\n",
    "        ii = i * 2\n",
    "        # Shape: [batch, 1, H, W]\n",
    "        bx = bxy[:, ii : ii + 1] + torch.tensor(grid_x, device=device, dtype=torch.float32) # grid_x.to(device=device, dtype=torch.float32)\n",
    "        # Shape: [batch, 1, H, W]\n",
    "        by = bxy[:, ii + 1 : ii + 2] + torch.tensor(grid_y, device=device, dtype=torch.float32) # grid_y.to(device=device, dtype=torch.float32)\n",
    "        # Shape: [batch, 1, H, W]\n",
    "        bw = bwh[:, ii : ii + 1] * anchor_w[i]\n",
    "        # Shape: [batch, 1, H, W]\n",
    "        bh = bwh[:, ii + 1 : ii + 2] * anchor_h[i]\n",
    "\n",
    "        bx_list.append(bx)\n",
    "        by_list.append(by)\n",
    "        bw_list.append(bw)\n",
    "        bh_list.append(bh)\n",
    "\n",
    "\n",
    "    ########################################\n",
    "    #   Figure out bboxes from slices     #\n",
    "    ########################################\n",
    "    \n",
    "    # Shape: [batch, num_anchors, H, W]\n",
    "    bx = torch.cat(bx_list, dim=1)\n",
    "    # Shape: [batch, num_anchors, H, W]\n",
    "    by = torch.cat(by_list, dim=1)\n",
    "    # Shape: [batch, num_anchors, H, W]\n",
    "    bw = torch.cat(bw_list, dim=1)\n",
    "    # Shape: [batch, num_anchors, H, W]\n",
    "    bh = torch.cat(bh_list, dim=1)\n",
    "\n",
    "    # Shape: [batch, 2 * num_anchors, H, W]\n",
    "    bx_bw = torch.cat((bx, bw), dim=1)\n",
    "    # Shape: [batch, 2 * num_anchors, H, W]\n",
    "    by_bh = torch.cat((by, bh), dim=1)\n",
    "\n",
    "    # normalize coordinates to [0, 1]\n",
    "    bx_bw /= output.size(3)\n",
    "    by_bh /= output.size(2)\n",
    "\n",
    "    # Shape: [batch, num_anchors * H * W, 1]\n",
    "    bx = bx_bw[:, :num_anchors].view(output.size(0), num_anchors * output.size(2) * output.size(3), 1)\n",
    "    by = by_bh[:, :num_anchors].view(output.size(0), num_anchors * output.size(2) * output.size(3), 1)\n",
    "    bw = bx_bw[:, num_anchors:].view(output.size(0), num_anchors * output.size(2) * output.size(3), 1)\n",
    "    bh = by_bh[:, num_anchors:].view(output.size(0), num_anchors * output.size(2) * output.size(3), 1)\n",
    "\n",
    "    bx1 = bx - bw * 0.5\n",
    "    by1 = by - bh * 0.5\n",
    "    bx2 = bx1 + bw\n",
    "    by2 = by1 + bh\n",
    "\n",
    "    # Shape: [batch, num_anchors * h * w, 4] -> [batch, num_anchors * h * w, 1, 4]\n",
    "    boxes = torch.cat((bx1, by1, bx2, by2), dim=2).view(output.size(0), num_anchors * output.size(2) * output.size(3), 1, 4)\n",
    "    # boxes = boxes.repeat(1, 1, num_classes, 1)\n",
    "\n",
    "    # boxes:     [batch, num_anchors * H * W, 1, 4]\n",
    "    # cls_confs: [batch, num_anchors * H * W, num_classes]\n",
    "    # det_confs: [batch, num_anchors * H * W]\n",
    "\n",
    "    det_confs = det_confs.view(output.size(0), num_anchors * output.size(2) * output.size(3), 1)\n",
    "    confs = cls_confs * det_confs\n",
    "\n",
    "    # boxes: [batch, num_anchors * H * W, 1, 4]\n",
    "    # confs: [batch, num_anchors * H * W, num_classes]\n",
    "\n",
    "    return  boxes, confs\n",
    "\n",
    "class YoloLayer(nn.Module):\n",
    "    ''' Yolo layer\n",
    "    model_out: while inference,is post-processing inside or outside the model\n",
    "        true:outside\n",
    "    '''\n",
    "    def __init__(self, anchor_mask=[], num_classes=0, anchors=[], num_anchors=1, stride=32, model_out=False):\n",
    "        super(YoloLayer, self).__init__()\n",
    "        self.anchor_mask = anchor_mask\n",
    "        self.num_classes = num_classes\n",
    "        self.anchors = anchors\n",
    "        self.num_anchors = num_anchors\n",
    "        self.anchor_step = len(anchors) // num_anchors\n",
    "        self.coord_scale = 1\n",
    "        self.noobject_scale = 1\n",
    "        self.object_scale = 5\n",
    "        self.class_scale = 1\n",
    "        self.thresh = 0.6\n",
    "        self.stride = stride\n",
    "        self.seen = 0\n",
    "        self.scale_x_y = 1\n",
    "\n",
    "        self.model_out = model_out\n",
    "\n",
    "    def forward(self, output, target=None):\n",
    "        if self.training:\n",
    "            return output\n",
    "        masked_anchors = []\n",
    "        for m in self.anchor_mask:\n",
    "            masked_anchors += self.anchors[m * self.anchor_step:(m + 1) * self.anchor_step]\n",
    "        masked_anchors = [anchor / self.stride for anchor in masked_anchors]\n",
    "\n",
    "        return yolo_forward_dynamic(output, self.thresh, self.num_classes, masked_anchors, len(self.anchor_mask),scale_x_y=self.scale_x_y)\n",
    "\n",
    "\n",
    "def get_region_boxes(boxes_and_confs):\n",
    "\n",
    "    # print('Getting boxes from boxes and confs ...')\n",
    "\n",
    "    boxes_list = []\n",
    "    confs_list = []\n",
    "\n",
    "    for item in boxes_and_confs:\n",
    "        boxes_list.append(item[0])\n",
    "        confs_list.append(item[1])\n",
    "\n",
    "    # boxes: [batch, num1 + num2 + num3, 1, 4]\n",
    "    # confs: [batch, num1 + num2 + num3, num_classes]\n",
    "    boxes = torch.cat(boxes_list, dim=1)\n",
    "    confs = torch.cat(confs_list, dim=1)\n",
    "        \n",
    "    return boxes, confs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Download the COCO 2017 evaluation dataset and define the data loader function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!curl -LO http://images.cocodataset.org/zips/val2017.zip\n",
    "!curl -LO http://images.cocodataset.org/annotations/annotations_trainval2017.zip\n",
    "!unzip -q val2017.zip\n",
    "!unzip annotations_trainval2017.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define data loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import time\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.datasets as dset\n",
    "from pycocotools.coco import COCO\n",
    "\n",
    "\n",
    "def get_coco_dataloader(coco2017_root, transform, subset_indices=None):\n",
    "    \"\"\"\n",
    "    Create the dataset loader and ground truth coco dataset\n",
    "    input: dataset_path path and subset_indices (optional)\n",
    "    dataset_path - where the dataset is created\n",
    "    subset_indices - list of indices that used to create a subset of the dataset \n",
    "    returns: coco_val_data_loader, cocoGt, label_info\n",
    "    \"\"\"\n",
    "\n",
    "    # Create the dataset\n",
    "    coco2017_img_path = os.path.join(coco2017_root, 'val2017')\n",
    "    coco2017_ann_path = os.path.join(\n",
    "        coco2017_root, 'annotations/instances_val2017.json')\n",
    "\n",
    "    # check the number of images in val2017 - Should be 5000\n",
    "    num_files = 0\n",
    "    for root, dirs, files in os.walk(coco2017_img_path):\n",
    "        for filename in files:\n",
    "            num_files += 1\n",
    "    print('\\nNumber of images in val2017 = {}\\n'.format(num_files))\n",
    "\n",
    "    # load annotations to decode classification results\n",
    "    with open(coco2017_ann_path) as f:\n",
    "        annotate_json = json.load(f)\n",
    "    label_info = {label[\"id\"]: label[\"name\"]\n",
    "                  for label in annotate_json['categories']}\n",
    "\n",
    "    # initialize COCO ground truth dataset\n",
    "    cocoGt = COCO(coco2017_ann_path)\n",
    "\n",
    "    # create the dataset using torchvision's coco detection dataset\n",
    "    coco_val_data = dset.CocoDetection(\n",
    "        root=coco2017_img_path, annFile=coco2017_ann_path, transform=transform)\n",
    "\n",
    "    if subset_indices is not None:\n",
    "        # Create a smaller subset of the data for testing - e.g. to pinpoint error at image 516\n",
    "        coco_val_data = torch.utils.data.Subset(coco_val_data, subset_indices)\n",
    "\n",
    "    # create the dataloader using torch dataloader\n",
    "    coco_val_data_loader = torch.utils.data.DataLoader(coco_val_data, batch_size=1, shuffle=False)\n",
    "\n",
    "    return coco_val_data_loader, cocoGt, label_info\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coco2017_root = './'\n",
    "orig_coco_val_data_loader, *_ = get_coco_dataloader(coco2017_root, transforms.ToTensor())\n",
    "transform = transforms.Compose([transforms.Resize([608, 608]), transforms.ToTensor()])\n",
    "coco_val_data_loader, cocoGt, label_info = get_coco_dataloader(coco2017_root, transform)\n",
    "image_orig, _ = next(iter(orig_coco_val_data_loader))\n",
    "print(image_orig.shape)\n",
    "image, image_info = next(iter(coco_val_data_loader))\n",
    "image_id = image_info[0][\"image_id\"].item()\n",
    "print(image.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define some helper functions for inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def post_processing(img, conf_thresh, nms_thresh, output):\n",
    "\n",
    "    # anchors = [12, 16, 19, 36, 40, 28, 36, 75, 76, 55, 72, 146, 142, 110, 192, 243, 459, 401]\n",
    "    # num_anchors = 9\n",
    "    # anchor_masks = [[0, 1, 2], [3, 4, 5], [6, 7, 8]]\n",
    "    # strides = [8, 16, 32]\n",
    "    # anchor_step = len(anchors) // num_anchors\n",
    "\n",
    "    # [batch, num, 1, 4]\n",
    "    box_array = output[0]\n",
    "    # [batch, num, num_classes]\n",
    "    confs = output[1]\n",
    "\n",
    "    t1 = time.time()\n",
    "\n",
    "    if type(box_array).__name__ != 'ndarray':\n",
    "        box_array = box_array.cpu().detach().numpy()\n",
    "        confs = confs.cpu().detach().numpy()\n",
    "\n",
    "    num_classes = confs.shape[2]\n",
    "\n",
    "    # [batch, num, 4]\n",
    "    box_array = box_array[:, :, 0]\n",
    "\n",
    "    # [batch, num, num_classes] --> [batch, num]\n",
    "    max_conf = np.max(confs, axis=2)\n",
    "    max_id = np.argmax(confs, axis=2)\n",
    "\n",
    "    t2 = time.time()\n",
    "\n",
    "    bboxes_batch = []\n",
    "    for i in range(box_array.shape[0]):\n",
    "       \n",
    "        argwhere = max_conf[i] > conf_thresh\n",
    "        l_box_array = box_array[i, argwhere, :]\n",
    "        l_max_conf = max_conf[i, argwhere]\n",
    "        l_max_id = max_id[i, argwhere]\n",
    "\n",
    "        bboxes = []\n",
    "        # nms for each class\n",
    "        for j in range(num_classes):\n",
    "\n",
    "            cls_argwhere = l_max_id == j\n",
    "            ll_box_array = l_box_array[cls_argwhere, :]\n",
    "            ll_max_conf = l_max_conf[cls_argwhere]\n",
    "            ll_max_id = l_max_id[cls_argwhere]\n",
    "\n",
    "            keep = nms_cpu(ll_box_array, ll_max_conf, nms_thresh)\n",
    "            \n",
    "            if (keep.size > 0):\n",
    "                ll_box_array = ll_box_array[keep, :]\n",
    "                ll_max_conf = ll_max_conf[keep]\n",
    "                ll_max_id = ll_max_id[keep]\n",
    "\n",
    "                for k in range(ll_box_array.shape[0]):\n",
    "                    bboxes.append([ll_box_array[k, 0], ll_box_array[k, 1], ll_box_array[k, 2], ll_box_array[k, 3], ll_max_conf[k], ll_max_conf[k], ll_max_id[k]])\n",
    "        \n",
    "        bboxes_batch.append(bboxes)\n",
    "\n",
    "    t3 = time.time()\n",
    "\n",
    "    print('-----------------------------------')\n",
    "    print('       max and argmax : %f' % (t2 - t1))\n",
    "    print('                  nms : %f' % (t3 - t2))\n",
    "    print('Post processing total : %f' % (t3 - t1))\n",
    "    print('-----------------------------------')\n",
    "    \n",
    "    return bboxes_batch\n",
    "\n",
    "\n",
    "def nms_cpu(boxes, confs, nms_thresh=0.5, min_mode=False):\n",
    "    # print(boxes.shape)\n",
    "    x1 = boxes[:, 0]\n",
    "    y1 = boxes[:, 1]\n",
    "    x2 = boxes[:, 2]\n",
    "    y2 = boxes[:, 3]\n",
    "\n",
    "    areas = (x2 - x1) * (y2 - y1)\n",
    "    order = confs.argsort()[::-1]\n",
    "\n",
    "    keep = []\n",
    "    while order.size > 0:\n",
    "        idx_self = order[0]\n",
    "        idx_other = order[1:]\n",
    "\n",
    "        keep.append(idx_self)\n",
    "\n",
    "        xx1 = np.maximum(x1[idx_self], x1[idx_other])\n",
    "        yy1 = np.maximum(y1[idx_self], y1[idx_other])\n",
    "        xx2 = np.minimum(x2[idx_self], x2[idx_other])\n",
    "        yy2 = np.minimum(y2[idx_self], y2[idx_other])\n",
    "\n",
    "        w = np.maximum(0.0, xx2 - xx1)\n",
    "        h = np.maximum(0.0, yy2 - yy1)\n",
    "        inter = w * h\n",
    "\n",
    "        if min_mode:\n",
    "            over = inter / np.minimum(areas[order[0]], areas[order[1:]])\n",
    "        else:\n",
    "            over = inter / (areas[order[0]] + areas[order[1:]] - inter)\n",
    "\n",
    "        inds = np.where(over <= nms_thresh)[0]\n",
    "        order = order[inds + 1]\n",
    "    \n",
    "    return np.array(keep)\n",
    "\n",
    "\n",
    "clsid2catid = {0: 1, 1: 2, 2: 3, 3: 4, 4: 5, 5: 6, 6: 7, 7: 8, 8: 9, 9: 10, 10: 11, 11: 13, 12: 14, 13: 15, 14: 16,\n",
    "               15: 17, 16: 18, 17: 19, 18: 20, 19: 21, 20: 22, 21: 23, 22: 24, 23: 25, 24: 27, 25: 28, 26: 31,\n",
    "               27: 32, 28: 33, 29: 34, 30: 35, 31: 36, 32: 37, 33: 38, 34: 39, 35: 40, 36: 41, 37: 42, 38: 43,\n",
    "               39: 44, 40: 46, 41: 47, 42: 48, 43: 49, 44: 50, 45: 51, 46: 52, 47: 53, 48: 54, 49: 55, 50: 56,\n",
    "               51: 57, 52: 58, 53: 59, 54: 60, 55: 61, 56: 62, 57: 63, 58: 64, 59: 65, 60: 67, 61: 70, 62: 72,\n",
    "               63: 73, 64: 74, 65: 75, 66: 76, 67: 77, 68: 78, 69: 79, 70: 80, 71: 81, 72: 82, 73: 84, 74: 85,\n",
    "               75: 86, 76: 87, 77: 88, 78: 89, 79: 90}\n",
    "\n",
    "\n",
    "def get_results_as_dict(results, image_orig):\n",
    "    h_size, w_size = image_orig.shape[-2:]\n",
    "    xywhs = []\n",
    "    labels = []\n",
    "    scores = []\n",
    "    for x1, y1, x2, y2, score, _, label in results[0]:\n",
    "        x1 *= w_size\n",
    "        y1 *= h_size\n",
    "        x2 *= w_size\n",
    "        y2 *= h_size\n",
    "        xywh = [x1, y1, x2 - x1, y2 - y1]\n",
    "        xywhs.append(xywh)\n",
    "        labels.append(clsid2catid[label])\n",
    "        scores.append(score)\n",
    "    result_dict = {'boxes': np.array(xywhs), 'labels': np.array(labels), 'scores': np.array(scores)}\n",
    "    return [result_dict]\n",
    "\n",
    "\n",
    "def prepare_for_coco_detection(predictions):\n",
    "    coco_results = []\n",
    "    for original_id, prediction in predictions.items():\n",
    "        if len(prediction) == 0:\n",
    "            continue\n",
    "\n",
    "        boxes = prediction[\"boxes\"].tolist()\n",
    "        scores = prediction[\"scores\"].tolist()\n",
    "        labels = prediction[\"labels\"].tolist()\n",
    "\n",
    "        coco_results.extend(\n",
    "            [\n",
    "                {\n",
    "                    \"image_id\": original_id,\n",
    "                    \"category_id\": labels[k],\n",
    "                    \"bbox\": box,\n",
    "                    \"score\": scores[k],\n",
    "                }\n",
    "                for k, box in enumerate(boxes)\n",
    "            ]\n",
    "        )\n",
    "    return coco_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download pretrained checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "def download_file_from_google_drive(id, destination):\n",
    "    URL = \"https://docs.google.com/uc?export=download\"\n",
    "\n",
    "    session = requests.Session()\n",
    "\n",
    "    response = session.get(URL, params={'id': id}, stream=True)\n",
    "    token = get_confirm_token(response)\n",
    "\n",
    "    if token:\n",
    "        params = {'id': id, 'confirm': token}\n",
    "        response = session.get(URL, params=params, stream=True)\n",
    "\n",
    "    save_response_content(response, destination)\n",
    "\n",
    "def get_confirm_token(response):\n",
    "    for key, value in response.cookies.items():\n",
    "        if key.startswith('download_warning'):\n",
    "            return value\n",
    "\n",
    "    return None\n",
    "\n",
    "def save_response_content(response, destination):\n",
    "    CHUNK_SIZE = 32768\n",
    "\n",
    "    with open(destination, \"wb\") as f:\n",
    "        for chunk in response.iter_content(CHUNK_SIZE):\n",
    "            if chunk:  # filter out keep-alive new chunks\n",
    "                f.write(chunk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "download_file_from_google_drive('1wv_LiFeCRYwtpkqREPeI13-gPELBDwuJ', './yolo_v4.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Build, Compile, and Save Neuron-Optimized YOLO v4 TorchScript\n",
    "### Construct model and load pretrained checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Yolov4(yolov4conv137weight=None, n_classes=80, inference=True)\n",
    "weightfile = \"./yolo_v4.pth\"\n",
    "pretrained_dict = torch.load(weightfile, map_location=torch.device('cpu'))\n",
    "model.load_state_dict(pretrained_dict)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "\n",
    "image_orig, _ = next(iter(orig_coco_val_data_loader))\n",
    "image, _ = next(iter(coco_val_data_loader))\n",
    "model_result = model(image)\n",
    "nms_model_result = post_processing(image, conf_thresh=0.05, nms_thresh=0.5, output=model_result)\n",
    "result_dict = get_results_as_dict(nms_model_result, image_orig)[0]\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 10))\n",
    "ax.imshow(image_orig.numpy().squeeze(0).transpose(1, 2, 0))\n",
    "for xywh, _ in zip(result_dict['boxes'], result_dict['labels']):\n",
    "    x, y, w, h = xywh\n",
    "    rect = patches.Rectangle((x, y), w, h, linewidth=1, edgecolor='g', facecolor='none')\n",
    "    ax.add_patch(rect)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run tentative compilation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.neuron\n",
    "\n",
    "model_neuron_tentative = torch.neuron.trace(model, image)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**THIS TENTATIVE COMPILATION WILL FAIL**, because neuron-cc generates some inefficient code in the middle of the compilation process. To get a successful compilation as well as improve the compiled model performance, we may inspect the compiler representation of the model in the following way."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_neuron_for_inspection = torch.neuron.trace(model, image, skip_compiler=True)\n",
    "print(model_neuron_for_inspection)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We discover that there are many `aten::slice` operations in some submodules called `YoloLayer`. Although these operations are supported by the neuron-cc compiler, they are not going to run efficiently on the Inferentia hardware. To work it around, we recommend to manually place these operators on CPU."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run compilation with manually specified device placement"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To manually place `YoloLayer` on CPU, we may make use of the `subgraph_builder_function` argument in `torch.neuron.trace`. It is a callback function that returns `True` or `False` based on information available in `node`. The typical use is a condition based on either `node.name` or `node.type_string`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def subgraph_builder_function(node):\n",
    "    return 'YoloLayer' not in node.name\n",
    "\n",
    "model_neuron = torch.neuron.trace(model, image, subgraph_builder_function=subgraph_builder_function)\n",
    "model_neuron.save('yolo_v4_neuron.pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compilation is now finished and the compiled model has been saved to a local file called 'yolo_v4_neuron.pt'. Saving is important due to the slow compilation process."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4: Evaluate Accuracy on the COCO 2017 Dataset\n",
    "### Load compiled model and run inference\n",
    "To validate accuracy of the compiled model, lets run inference on the COCO 2017 validation dataset. We start by defining a helper function `run_inference`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_inference(dataloader, dataloader_orig, model, convert=True, modelName=''):\n",
    "    \"\"\"\n",
    "    run inference on the dataset\n",
    "    input: dataloader (containing images and annotations), model, convert\n",
    "    convert should be set to False if it's the vanilla torchvision model and doesn't need to be transformed into coco format\n",
    "    returns: imgIds and cocoDt \n",
    "    cocoDt contains the predictions from the model in coco format\n",
    "    \"\"\"\n",
    "    print('\\n================ Starting Inference on {} Images using {} model ================\\n'.format(\n",
    "        len(dataloader), modelName))\n",
    "\n",
    "    modelName = str(modelName).replace(\" \", \"_\")\n",
    "\n",
    "    # convert predicition to cocoDt\n",
    "    # code from def evaluate in https://github.com/pytorch/vision/blob/master/references/detection/engine.py\n",
    "    imgIds = []\n",
    "    results = []\n",
    "    skippedImages = []\n",
    "\n",
    "    # time inference\n",
    "    start_time = time.time()\n",
    "    for idx, ((image, targets), (image_orig, _)) in enumerate(zip(dataloader, dataloader_orig)):\n",
    "        # if target is empty, skip the image because it breaks the scripted model\n",
    "        if not targets:\n",
    "            skippedImages.append(idx)\n",
    "            continue\n",
    "\n",
    "        # get the predictions\n",
    "        model_result = model(image)\n",
    "        nms_model_result = post_processing(image, conf_thresh=0.05, nms_thresh=0.5, output=model_result)\n",
    "        outputs = get_results_as_dict(nms_model_result, image_orig)\n",
    "\n",
    "        res = {target[\"image_id\"].item(): output for target,\n",
    "               output in zip(targets, outputs)}\n",
    "\n",
    "        # add the image id to imgIds\n",
    "        image_id = targets[0][\"image_id\"].item()\n",
    "        imgIds.append(image_id)\n",
    "\n",
    "        # convert the predicition into cocoDt results\n",
    "        pred = prepare_for_coco_detection(res)\n",
    "        results.extend(pred)\n",
    "\n",
    "    print('\\n==================== Performance Measurement ====================')\n",
    "    print('Finished inference on {} images in {:.2f} seconds'.format(\n",
    "        len(dataloader), time.time() - start_time))\n",
    "    print('=================================================================\\n')\n",
    "\n",
    "    # create bbox detections file\n",
    "    # following code in https://github.com/aws/aws-neuron-sdk/blob/master/src/examples/tensorflow/yolo_v4_demo/evaluate.ipynb\n",
    "    resultsfile = modelName + '_bbox_detections.json'\n",
    "    print('Generating json file...')\n",
    "    with open(resultsfile, 'w') as f:\n",
    "        json.dump(results, f)\n",
    "\n",
    "    # return COCO api object with loadRes\n",
    "    cocoDt = cocoGt.loadRes(resultsfile)\n",
    "\n",
    "    return imgIds, cocoDt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next step is to simply load the compiled model from disk and then run inference. This step needs to be done on a `inf1` type instance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.neuron\n",
    "\n",
    "model_neuron = torch.jit.load('yolo_v4_neuron.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imgIds, cocoDt = run_inference(coco_val_data_loader, orig_coco_val_data_loader, model_neuron)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then use the standard `pycocotools` routines to generate a report of bounding box precision/recall."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pycocotools.cocoeval import COCOeval\n",
    "\n",
    "cocoEval = COCOeval(cocoGt, cocoDt, 'bbox')\n",
    "cocoEval.params.imgIds = imgIds\n",
    "cocoEval.evaluate()\n",
    "cocoEval.accumulate()\n",
    "cocoEval.summarize()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For reference, we may perform the same evaluation on the CPU model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imgIdsRef, cocoDtRef = run_inference(coco_val_data_loader, orig_coco_val_data_loader, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cocoEval = COCOeval(cocoGt, cocoDtRef, 'bbox')\n",
    "cocoEval.params.imgIds = imgIdsRef\n",
    "cocoEval.evaluate()\n",
    "cocoEval.accumulate()\n",
    "cocoEval.summarize()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 5: Benchmark Hardware-Only Performance of the Neuron-Optimized TorchScript\n",
    "The following code snippet sets up data parallel on 4 Neuron cores and runs saturated multi-threaded inference on the Inferentia accelerator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "\n",
    "# prepare some sample images\n",
    "sample_images = []\n",
    "for idx, (image, _) in enumerate(coco_val_data_loader):\n",
    "    if len(sample_images) == 4:\n",
    "        break\n",
    "    sample_images.append(image)\n",
    "\n",
    "# setup data parallel by loading the same model 4 times\n",
    "model_neuron_list = [torch.jit.load('yolo_v4_neuron.pt') for _ in range(4)]\n",
    "\n",
    "# warmup the loaded models\n",
    "for model_neuron, image in zip(model_neuron_list, sample_images):\n",
    "    model_neuron(image)\n",
    "\n",
    "# run multithreaded inference, with directing images to models in a round-robin fashion\n",
    "num_threads = 8\n",
    "queue_depth = 8\n",
    "with ThreadPoolExecutor(num_threads) as exe:\n",
    "    fut_list = []\n",
    "    start = time.time()\n",
    "    num_inferences = 0\n",
    "    for idx in range(1000):\n",
    "        model_neuron = model_neuron_list[idx % 4]\n",
    "        image = sample_images[idx % 4]\n",
    "        fut = exe.submit(model_neuron, image)\n",
    "        fut_list.append(fut)\n",
    "        if idx > queue_depth:\n",
    "            fut0 = fut_list.pop(0)\n",
    "            fut0.result()\n",
    "            num_inferences += 1\n",
    "            if num_inferences % 100 == 0:\n",
    "                print(num_inferences)\n",
    "    for fut in fut_list:\n",
    "        fut.result()\n",
    "        num_inferences += 1\n",
    "    end = time.time()\n",
    "print('finished {} inferences in {} seconds'.format(num_inferences, end - start))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The expectation is that 1000 inferences will finish in ~8.3 seconds, corresponding to a throughput of ~120 images/second."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
