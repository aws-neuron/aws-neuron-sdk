{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "variable-character",
   "metadata": {},
   "source": [
    "# Using NeuronCore Pipeline with PyTorch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "valued-economics",
   "metadata": {},
   "source": [
    "In this tutorial you compile a pretrained BERT base model from HuggingFace ðŸ¤— Transformers, using the NeuronCore Pipeline feature of the AWS Neuron SDK. You benchmark model latency of the pipeline parallel mode and compare with the usual data parallel (multi-worker) deployment.\n",
    "\n",
    "This tutorial is intended to run in an inf1.6xlarge, running the latest AWS Deep Learning AMI (DLAMI). The inf1.6xlarge instance size has AWS Inferentia chips for a total of 16 NeuronCores.\n",
    "\n",
    "Verify that this Jupyter notebook is running the Python or Conda kernel environment that was set up according to the [PyTorch Installation Guide](../../../../frameworks/torch/torch-neuron/setup/pytorch-install.html). You can select the kernel from the \"Kernel -> Change Kernel\" option on the top of this Jupyter notebook page.\n",
    "\n",
    "> __Note:__ Do not execute this tutorial using \"Run -> Run all cells\" option.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "private-authentication",
   "metadata": {},
   "source": [
    "## Install Dependencies:\n",
    "This tutorial requires the following pip packages:\n",
    "\n",
    "- `torch-neuron`\n",
    "- `neuron-cc[tensorflow]`\n",
    "- `transformers`\n",
    "\n",
    "Most of these packages will be installed when configuring your environment using the Neuron PyTorch setup guide. The additional HuggingFace ðŸ¤— Transformers dependency must be installed here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "romantic-accident",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install --upgrade \"transformers==4.6.0\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "prompt-australian",
   "metadata": {},
   "source": [
    "## Compiling a BERT base model for a single NeuronCore"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aging-biodiversity",
   "metadata": {},
   "source": [
    "To run a HuggingFace [BERTModel](https://huggingface.co/transformers/model_doc/bert.html#bertmodel) on Inferentia, you only need to add a single extra line of code to the usual ðŸ¤— Transformers PyTorch implementation, after importing the torch_neuron framework. \n",
    "\n",
    "Add the argument `return_dict=False` to the BERT transformers model so it can be traced with [TorchScript](https://pytorch.org/docs/stable/jit.html). TorchScript is a way to create serializable and optimizable models from PyTorch code. \n",
    "\n",
    "Enable padding to a maximum sequence length of 128, to test the model's performance with a realistic payload size. You can adapt this sequence length to your application's requirement. \n",
    "\n",
    "You can adapt the original example on the [BertModel forward pass docstring](https://huggingface.co/transformers/model_doc/bert.html#transformers.BertModel.forward) according to the following cell\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "stretch-preview",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch_neuron\n",
    "from transformers import BertTokenizer, BertModel\n",
    "\n",
    "from joblib import Parallel, delayed  \n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "import os\n",
    "import time \n",
    "\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "model = BertModel.from_pretrained('bert-base-uncased',return_dict=False)\n",
    "\n",
    "inputs = tokenizer(\"Hello, my dog is cute\",return_tensors=\"pt\",max_length=128,padding='max_length',truncation=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "conceptual-aberdeen",
   "metadata": {},
   "source": [
    "The one extra line required is the call to torch.neuron.trace() method. This call compiles the model and returns the forwad method of the torch `nn.Model` method, which you can use to run inference. \n",
    "\n",
    "The compiled graph can be saved using the `torch.jit.save` function and restored using `torch.jit.load` function for inference on Inf1 instances. During inference, the previously compiled artifacts will be loaded into the Neuron Runtime for inference execution.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "secondary-exclusive",
   "metadata": {},
   "outputs": [],
   "source": [
    "neuron_model = torch.neuron.trace(model, \n",
    "                                  example_inputs = (inputs['input_ids'],inputs['attention_mask']),\n",
    "                                  verbose=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "atmospheric-stewart",
   "metadata": {},
   "source": [
    "## Running the BERT base model on a single NeuronCore\n",
    "With the model already available in memory, you can time one execution and check for the latency on the single inference call. You will load the model into Inferentia with a single inference call. A large \"wall time\" is expected when you first run the next cell, running the cell twice will show the actual inference latency:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "approved-reputation",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# The following line tests inference and should be executed on Inf1 instance family. \n",
    "outputs = neuron_model(*(inputs['input_ids'],inputs['attention_mask']))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "great-collective",
   "metadata": {},
   "source": [
    "You can also check for the throughput of the single model running on a single NeuronCore.\n",
    "\n",
    "The sequential inference test (for loop) does not measure all the performance one can achieve in an instance with multiple NeuronCores. To improve hardwar utilization you can run parallel inference requests over multiple model workers, which you'll test in the Data Parallel Bonus Section below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "framed-reference",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "for _ in tqdm(range(100)):\n",
    "    outputs = neuron_model(*(inputs['input_ids'],inputs['attention_mask'])) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "super-innocent",
   "metadata": {},
   "source": [
    "Save the compiled model for later use:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "express-greensboro",
   "metadata": {},
   "outputs": [],
   "source": [
    "neuron_model.save('bert-base-uncased-neuron.pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "modified-government",
   "metadata": {},
   "source": [
    "## Compiling a BERT base model for 16 NeuronCores\n",
    "\n",
    "Our next step is to compile the same model for all 16 NeuronCores available in the inf1.6xlarge and check the performance difference when running pipeline parallel inferences.. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "compound-initial",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch_neuron\n",
    "from transformers import BertTokenizer, BertModel\n",
    "\n",
    "from joblib import Parallel, delayed  \n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "import os\n",
    "import time \n",
    "\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "model = BertModel.from_pretrained('bert-base-uncased',return_dict=False)\n",
    "\n",
    "inputs = tokenizer(\"Hello, my dog is cute\",return_tensors=\"pt\",max_length=128,padding='max_length',truncation=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "universal-desperate",
   "metadata": {},
   "source": [
    "To enable pipeline mode during compilation, you need only to add the compiler flag `--neuroncore-pipeline-cores` and set the number of desired cores. The cell below sets up a  `neuroncore_pipeline_cores` string, which you can set for the available number of NeuronCores on the instance: _inf1.6xlarge_ has 16 NeuronCores in 4 Inferentia chips. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "passing-masters",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of Cores in the Pipeline Mode\n",
    "neuroncore_pipeline_cores = 16 # This string should be '4' on an inf1.xlarge\n",
    "\n",
    "# Compiling for neuroncore-pipeline-cores='16'\n",
    "neuron_pipeline_model = torch.neuron.trace(model,\n",
    "                                           example_inputs = (inputs['input_ids'],inputs['attention_mask']),\n",
    "                                           verbose=1,\n",
    "                                           compiler_args = ['--neuroncore-pipeline-cores', str(neuroncore_pipeline_cores)]\n",
    "                                          )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "enhanced-swedish",
   "metadata": {},
   "source": [
    "## Running the BERT base model on 16 NeuronCores\n",
    "Next, time one execution and check for the latency on the single inference call over 16 cores. You will load the model into Inferentia with a single inference call. A large \"wall time\" is expected when you first run the next cell, running the cell twice will show the actual inference latency:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "expressed-trinity",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# The following line tests inference and should be executed on Inf1 instance family. \n",
    "outputs = neuron_pipeline_model(*(inputs['input_ids'],inputs['attention_mask']))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "located-graphic",
   "metadata": {},
   "source": [
    "Check also for the throughput of the single model running over a 16 NeuronCores. \n",
    "\n",
    "The sequential inference test (for loop) does not measure all the performance one can achieve with Pipeline mode. As the inference runs in streaming fashion, at least 15 cores are waiting for a new call until the last one processes the first call. This results in low NeuronCore utilization. To improve hardware utilization you will require parallel inference requests, which you'll test in the next section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "hydraulic-calcium",
   "metadata": {},
   "outputs": [],
   "source": [
    "for _ in tqdm(range(100)):\n",
    "    outputs = neuron_pipeline_model(*(inputs['input_ids'],inputs['attention_mask']))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "patent-victoria",
   "metadata": {},
   "source": [
    "## Load Testing the Pipeline Parallel Mode\n",
    "\n",
    "To put the 16 NeuronCores group to test, a client has to run concurrent requests to the model. In this Notebook setup you achieve it by creating a thread pool with `Joblib.Parallel`, with all workers on the pool runing one inference call. \n",
    "\n",
    "You can define a new method called `inference_latency()` so that you measure the amount of time each inference calls take."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "appointed-adventure",
   "metadata": {},
   "outputs": [],
   "source": [
    "def inference_latency(model,*inputs):\n",
    "    \"\"\"\n",
    "    infetence_time is a simple method to return the latency of a model inference.\n",
    "        \n",
    "        Parameters:\n",
    "            model: torch model onbject loaded using torch.jit.load\n",
    "            inputs: model() args\n",
    "        \n",
    "        Returns:\n",
    "            latency in seconds\n",
    "    \"\"\"\n",
    "    start = time.time()\n",
    "    _ = model(*inputs)\n",
    "    return time.time() - start"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "environmental-guinea",
   "metadata": {},
   "source": [
    "Use `tqdm` to measure total throughput of your experiment, with a nice side-effect of \"cool progress bar!\". The total throughput is expected to be high, so set your experiment range to a large number, here 30k inferences. \n",
    "\n",
    "To calculate the latency statistics over the returned 30k list of latencies use `numpy.qunatile()` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "played-catch",
   "metadata": {},
   "outputs": [],
   "source": [
    "t = tqdm(range(30000), position=0, leave=True)\n",
    "latency = Parallel(n_jobs=12,prefer=\"threads\")(delayed(inference_latency)(neuron_pipeline_model,*(inputs['input_ids'],inputs['attention_mask'])) for i in t)\n",
    "\n",
    "p50 = np.quantile(latency[-10000:],0.50) * 1000\n",
    "p95 = np.quantile(latency[-10000:],0.95) * 1000\n",
    "p99 = np.quantile(latency[-10000:],0.99) * 1000\n",
    "avg_throughput = t.total/t.format_dict['elapsed']\n",
    "print(f'Avg Throughput: :{avg_throughput:.1f}')\n",
    "print(f'50th Percentile Latency:{p50:.1f} ms')\n",
    "print(f'95th Percentile Latency:{p95:.1f} ms')\n",
    "print(f'99th Percentile Latency:{p99:.1f} ms')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "exposed-northern",
   "metadata": {},
   "source": [
    "Save compile model for later use:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "imperial-complex",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the TorchScript graph\n",
    "neuron_pipeline_model.save('bert-base-uncased-neuron-pipeline.pt')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "abroad-earthquake",
   "metadata": {},
   "source": [
    "## Bonus Section - Load Testing Data Parallel Mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "therapeutic-detector",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch_neuron\n",
    "from transformers import BertTokenizer \n",
    "\n",
    "from joblib import Parallel, delayed  \n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "import os\n",
    "import time \n",
    "\n",
    "def inference_latency(model,*inputs):\n",
    "    \"\"\"\n",
    "    infetence_time is a simple method to return the latency of a model inference.\n",
    "        \n",
    "        Parameters:\n",
    "            model: torch model onbject loaded using torch.jit.load\n",
    "            inputs: model() args\n",
    "        \n",
    "        Returns:\n",
    "            latency in seconds\n",
    "    \"\"\"\n",
    "    start = time.time()\n",
    "    _ = model(*inputs)\n",
    "    return time.time() - start\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "inputs = tokenizer(\"Hello, my dog is cute\",return_tensors=\"pt\",max_length=128,padding='max_length',truncation=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "legal-terrorist",
   "metadata": {},
   "source": [
    "You use the `'NEURON_RT_NUM_CORES'` environment variable to define how many Neuron cores to be used. Set the environment variable to the number of individual workers you want to test in parallel.\n",
    "\n",
    "`torch_neuron` will load one model per NeuronCore group until it runs out of cores. At that point, if the Python process continues to spawn more model objest using `torch.jit.load`, `torch_neuron` will start stacking more than one model per core, until the Inferentia chip memory is full. \n",
    "\n",
    "Inferentia is able to run inference over all the loaded models, but only one at a time. The Neuron Runtime takes care of dynamically switching the model context as requests come in, no extra worker process management required. Use 1 model per NeuronCore to achieve maximum performance.\n",
    "\n",
    "The following cell creates a list with as many models as NeuronCore Groups and execute one single dummy inference to load the models into Inferentia. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "current-mechanics",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "# Number of data parallel workers\n",
    "number_of_workers=16 # This number should be 4 on an inf1.xlarge\n",
    "\n",
    "# Setting up a data parallel group\n",
    "os.environ['NEURON_RT_NUM_CORES'] = str(number_of_workers)\n",
    "\n",
    "# Loading 'number_of_workers' amount of models in Python memory\n",
    "model_list = [torch.jit.load('bert-base-uncased-neuron.pt') for _ in range(number_of_workers)]\n",
    "\n",
    "# Dummy inference to load models to Inferentia\n",
    "_ = [mod(*(inputs['input_ids'],inputs['attention_mask'])) for mod in model_list]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "threatened-swaziland",
   "metadata": {},
   "source": [
    "Adapt the call to `joblib.Parallel()` iterating over a concatenated version of the `model_list`, to run 'round-robin' calls to each of the model workers.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fleet-month",
   "metadata": {},
   "outputs": [],
   "source": [
    "t = tqdm(model_list*1500,position=0, leave=True)\n",
    "latency = Parallel(n_jobs=number_of_workers,prefer=\"threads\")(delayed(inference_latency)(mod,*(inputs['input_ids'],inputs['attention_mask'])) for mod in t)\n",
    "\n",
    "p50 = np.quantile(latency[-10000:],0.50) * 1000\n",
    "p95 = np.quantile(latency[-10000:],0.95) * 1000\n",
    "p99 = np.quantile(latency[-10000:],0.99) * 1000\n",
    "avg_throughput = t.total/t.format_dict['elapsed']\n",
    "print(f'Avg Throughput: :{avg_throughput:.1f}')\n",
    "print(f'50th Percentile Latency:{p50:.1f} ms')\n",
    "print(f'95th Percentile Latency:{p95:.1f} ms')\n",
    "print(f'99th Percentile Latency:{p99:.1f} ms')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aggressive-stevens",
   "metadata": {},
   "source": [
    "For this model, despite the larger number of workers, the per-worker latency increases when running a single model per core, which in turn reduces the total throughput. \n",
    "\n",
    "This behavior may not repeat if the model memory footprint or the input payload size changes, i.e batch size > 1. We encourage you to experiment with the data parallel and pipeline parallel modes to optimize your application performance. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Environment (conda_aws_neuron_pytorch_p36)",
   "language": "python",
   "name": "conda_aws_neuron_pytorch_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
