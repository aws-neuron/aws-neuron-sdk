{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6a30ffd9",
   "metadata": {},
   "source": [
    "# Compiling and Deploying ResNet50 on Trn1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea682fbe",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "In this tutorial we will compile and deploy a TorchVision ResNet50 model for accelerated inference on Neuron.\n",
    "\n",
    "This tutorial will use the [resnet50](https://pytorch.org/vision/main/models/generated/torchvision.models.resnet50.html) model, which is primarily used for arbitrary image classification tasks.\n",
    "\n",
    "This tutorial has the following main sections:\n",
    "\n",
    "1. Install dependencies\n",
    "1. Compile the ResNet model\n",
    "1. Run inference on Neuron and compare results to CPU\n",
    "1. Benchmark the model using multicore inference\n",
    "1. Finding the optimal batch size\n",
    "\n",
    "This Jupyter notebook should be run on a Trn1 instance (`trn1.2xlarge` or larger.)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f60760a",
   "metadata": {},
   "source": [
    "## Install Dependencies\n",
    "This tutorial requires the following pip packages:\n",
    "\n",
    "- `torch-neuronx`\n",
    "- `neuronx-cc`\n",
    "- `torchvision`\n",
    "- `Pillow`\n",
    "\n",
    "Most of these packages will be installed when configuring your environment using the Trn1 setup guide. The additional dependencies must be installed here:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c44c5df5",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install Pillow"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de2efba5",
   "metadata": {},
   "source": [
    "## Compile the model into an AWS Neuron optimized TorchScript\n",
    "\n",
    "In the following section, we load the model, get a sample input, run inference on CPU, compile the model for Neuron using `torch_neuronx.trace()`, and save the optimized model as `TorchScript`.\n",
    "\n",
    "`torch_neuronx.trace()` expects a tensor or tuple of tensor inputs to use for tracing, so we convert the input image into a tensor using the `get_image` function.\n",
    "\n",
    "The result of the trace stage will be a static executable where the operations to be run upon inference are determined during compilation. This means that when inferring, the resulting Neuron model must be executed with tensors that are the exact same shape as those provided at compilation time. If a model is given a tensor at inference time whose shape does not match the tensor given at compilation time, an error will occur. \n",
    "\n",
    "In the following section, we assume that we will receive an image shape of `[1, 3, 224, 224]` at inference time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1650de1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import urllib\n",
    "from PIL import Image\n",
    "\n",
    "import torch\n",
    "import torch_neuronx\n",
    "from torchvision import models\n",
    "from torchvision.transforms import functional\n",
    "\n",
    "\n",
    "def get_image(batch_size=1, image_shape=(224, 224)):\n",
    "    # Get an example input\n",
    "    filename = \"000000039769.jpg\"\n",
    "    if not os.path.exists(filename):\n",
    "        url = \"http://images.cocodataset.org/val2017/000000039769.jpg\"\n",
    "        urllib.request.urlretrieve(url, filename)\n",
    "    image = Image.open(filename).convert('RGB')\n",
    "    image = functional.resize(image, (image_shape))\n",
    "    image = functional.to_tensor(image)\n",
    "    image = torch.unsqueeze(image, 0)\n",
    "    image = torch.repeat_interleave(image, batch_size, 0)\n",
    "    return (image, )\n",
    "\n",
    "\n",
    "# Create the model\n",
    "model = models.resnet50(pretrained=True)\n",
    "model.eval()\n",
    "\n",
    "# Get an example input\n",
    "image = get_image()\n",
    "\n",
    "# Run inference on CPU\n",
    "output_cpu = model(*image)\n",
    "\n",
    "# Compile the model\n",
    "model_neuron = torch_neuronx.trace(model, image)\n",
    "\n",
    "# Save the TorchScript for inference deployment\n",
    "filename = 'model.pt'\n",
    "torch.jit.save(model_neuron, filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25f453f8",
   "metadata": {},
   "source": [
    "## Run inference and compare results\n",
    "\n",
    "In this section we load the compiled model, run inference on Neuron, and compare the CPU and Neuron outputs using the ImageNet classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4a203aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# Load the TorchScript compiled model\n",
    "model_neuron = torch.jit.load(filename)\n",
    "\n",
    "# Run inference using the Neuron model\n",
    "output_neuron = model_neuron(*image)\n",
    "\n",
    "# Compare the results\n",
    "print(f\"CPU tensor:    {output_cpu[0][0:10]}\")\n",
    "print(f\"Neuron tensor: {output_neuron[0][0:10]}\")\n",
    "\n",
    "# Download and read the ImageNet classes\n",
    "urllib.request.urlretrieve(\"https://s3.amazonaws.com/deep-learning-models/image-models/imagenet_class_index.json\",\"imagenet_class_index.json\")\n",
    "with open(\"imagenet_class_index.json\", \"r\") as file:\n",
    "    class_id = json.load(file)\n",
    "    id2label = [class_id[str(i)][1] for i in range(len(class_id))]\n",
    "\n",
    "# Lookup and print the top-5 labels\n",
    "top5_cpu = output_cpu[0].sort()[1][-5:]\n",
    "top5_neuron = output_neuron[0].sort()[1][-5:]\n",
    "top5_labels_cpu = [id2label[idx] for idx in top5_cpu]\n",
    "top5_labels_neuron = [id2label[idx] for idx in top5_neuron]\n",
    "print(f\"CPU top-5 labels:    {top5_labels_cpu}\")\n",
    "print(f\"Neuron top-5 labels: {top5_labels_neuron}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c96389ae",
   "metadata": {},
   "source": [
    "## Benchmarking\n",
    "\n",
    "In this section we benchmark the performance of the ResNet model on Neuron. By default, models compiled with `torch_neuronx` will always execute on a *single* NeuronCore. When loading *multiple* models, the default behavior of the Neuron runtime is to evenly distribute models across all available NeuronCores. The runtime places models on the NeuronCore that has the fewest models loaded to it first. In the following section, we will `torch.jit.load` multiple instances of the model which should each be loaded onto their own NeuronCore. It is not useful to load more copies of a model than the number of NeuronCores on the instance since an individual NeuronCore can only execute one model at a time.\n",
    "\n",
    "To ensure that we are maximizing hardware utilization, we must run inferences using multiple threads in parallel. It is nearly always recommended to use some form of threading/multiprocessing and some form of model replication since even the smallest Neuron EC2 instance has 2 NeuronCores available. Applications with no form of threading are only capable of `1 / num_neuron_cores` hardware utilization which becomes especially problematic on large instances.\n",
    "\n",
    "One way to view the hardware utilization is by executing the `neuron-top` application in the terminal while the benchmark is executing. If the monitor shows >90% utilization on all NeuronCores, this is a good indication that the hardware is being utilized effectively.\n",
    "\n",
    "In this example we load two models, which utilizes all NeuronCores (2) on a `trn1.2xlarge`. Additional models can be loaded and run in parallel on larger Trn1 instance sizes to increase throughput.\n",
    "\n",
    "We define a benchmarking function that loads two optimized ResNet models onto two separate NeuronCores, runs multithreaded inference, and calculates the corresponding latency and throughput."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9657ae4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import concurrent.futures\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def benchmark(filename, example, n_models=2, n_threads=2, batches_per_thread=1000):\n",
    "    \"\"\"\n",
    "    Record performance statistics for a serialized model and its input example.\n",
    "\n",
    "    Arguments:\n",
    "        filename: The serialized torchscript model to load for benchmarking.\n",
    "        example: An example model input.\n",
    "        n_models: The number of models to load.\n",
    "        n_threads: The number of simultaneous threads to execute inferences on.\n",
    "        batches_per_thread: The number of example batches to run per thread.\n",
    "\n",
    "    Returns:\n",
    "        A dictionary of performance statistics.\n",
    "    \"\"\"\n",
    "\n",
    "    # Load models\n",
    "    models = [torch.jit.load(filename) for _ in range(n_models)]\n",
    "\n",
    "    # Warmup\n",
    "    for _ in range(8):\n",
    "        for model in models:\n",
    "            model(*example)\n",
    "\n",
    "    latencies = []\n",
    "\n",
    "    # Thread task\n",
    "    def task(model):\n",
    "        for _ in range(batches_per_thread):\n",
    "            start = time.time()\n",
    "            model(*example)\n",
    "            finish = time.time()\n",
    "            latencies.append((finish - start) * 1000)\n",
    "\n",
    "    # Submit tasks\n",
    "    begin = time.time()\n",
    "    with concurrent.futures.ThreadPoolExecutor(max_workers=n_threads) as pool:\n",
    "        for i in range(n_threads):\n",
    "            pool.submit(task, models[i % len(models)])\n",
    "    end = time.time()\n",
    "\n",
    "    # Compute metrics\n",
    "    boundaries = [50, 95, 99]\n",
    "    percentiles = {}\n",
    "\n",
    "    for boundary in boundaries:\n",
    "        name = f'latency_p{boundary}'\n",
    "        percentiles[name] = np.percentile(latencies, boundary)\n",
    "    duration = end - begin\n",
    "    batch_size = 0\n",
    "    for tensor in example:\n",
    "        if batch_size == 0:\n",
    "            batch_size = tensor.shape[0]\n",
    "    inferences = len(latencies) * batch_size\n",
    "    throughput = inferences / duration\n",
    "\n",
    "    # Metrics\n",
    "    metrics = {\n",
    "        'filename': str(filename),\n",
    "        'batch_size': batch_size,\n",
    "        'batches': len(latencies),\n",
    "        'inferences': inferences,\n",
    "        'threads': n_threads,\n",
    "        'models': n_models,\n",
    "        'duration': duration,\n",
    "        'throughput': throughput,\n",
    "        **percentiles,\n",
    "    }\n",
    "\n",
    "    display(metrics)\n",
    "\n",
    "\n",
    "def display(metrics):\n",
    "    \"\"\"\n",
    "    Display the metrics produced by `benchmark` function.\n",
    "\n",
    "    Args:\n",
    "        metrics: A dictionary of performance statistics.\n",
    "    \"\"\"\n",
    "    pad = max(map(len, metrics)) + 1\n",
    "    for key, value in metrics.items():\n",
    "\n",
    "        parts = key.split('_')\n",
    "        parts = list(map(str.title, parts))\n",
    "        title = ' '.join(parts) + \":\"\n",
    "\n",
    "        if isinstance(value, float):\n",
    "            value = f'{value:0.3f}'\n",
    "\n",
    "        print(f'{title :<{pad}} {value}')\n",
    "\n",
    "\n",
    "# Benchmark ResNet on Neuron\n",
    "benchmark(filename, image)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "795d2fca",
   "metadata": {},
   "source": [
    "## Finding the optimal batch size\n",
    "\n",
    "Batch size has a direct impact on model performance. The NeuronCore architecture is optimized to maximize throughput with relatively small batch sizes. This means that a Neuron compiled model can outperform a GPU model, even if running single digit batch sizes.\n",
    "\n",
    "As a general best practice, we recommend optimizing your modelâ€™s throughput by compiling the model with a small batch size and gradually increasing it to find the peak throughput on Neuron. To minimize latency, using `batch size = 1` will nearly always be optimal. This batch size configuration is typically used for on-demand inference applications. To maximize throughput, *usually* `1 < batch_size < 10` is optimal. A configuration which uses a larger batch size is generally ideal for batched on-demand inference or offline batch processing.\n",
    "\n",
    "In the following section, we compile ResNet for multiple batch size inputs. We then run inference on each batch size and benchmark the performance. Notice that latency increases consistently as the batch size increases. Throughput increases as well, up until a certain point where the input size becomes too large to be efficient."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdef1805",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile ResNet for different batch sizes\n",
    "for batch_size in [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]:\n",
    "    model = models.resnet50(pretrained=True)\n",
    "    model.eval()\n",
    "    example = get_image(batch_size=batch_size)\n",
    "    model_neuron = torch_neuronx.trace(model, example)\n",
    "    filename = f'model_batch_size_{batch_size}.pt'\n",
    "    torch.jit.save(model_neuron, filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec244d4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Benchmark ResNet for different batch sizes\n",
    "for batch_size in [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]:\n",
    "    print('-'*50)\n",
    "    example = get_image(batch_size=batch_size)\n",
    "    filename = f'model_batch_size_{batch_size}.pt'\n",
    "    benchmark(filename, example)\n",
    "    print()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (Neuron PyTorch)",
   "language": "python",
   "name": "pytorch_venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
