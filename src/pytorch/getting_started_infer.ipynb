{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": 3
  },
  "orig_nbformat": 2
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "## Tutorial: Getting started with torch-neuron (resnet-50 tutorial - infer steps in code)\n",
    "\n",
    "**NOTE:** This notebook content represents the compilation parts of the [getting started tutorial](./getting_started.md) - it is not intended to used without reference to the tutorial. This is why we start at step 5 below :)."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## Step 5: Run inference\n",
    "\n",
    "In this step we run inference on Inf1 instances using the model compiled in Step 3 of [getting started compile](getting_started_compile.ipnb), which should have been copied to this machine\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import torch\n",
    "import torch_neuron\n",
    "import json\n",
    "import numpy as np\n",
    "from urllib import request\n",
    "from torchvision import models, transforms, datasets\n",
    "from time import time\n",
    "\n",
    "## Create an image directory containing a small kitten\n",
    "os.makedirs(\"./torch_neuron_test/images\", exist_ok=True)\n",
    "request.urlretrieve(\"https://raw.githubusercontent.com/awslabs/mxnet-model-server/master/docs/images/kitten_small.jpg\",\n",
    " \"./torch_neuron_test/images/kitten_small.jpg\")\n",
    "\n",
    "## Fetch labels to output the top classifications\n",
    "request.urlretrieve(\"https://s3.amazonaws.com/deep-learning-models/image-models/imagenet_class_index.json\",\"imagenet_class_index.json\")\n",
    "idx2label = []\n",
    "\n",
    "with open(\"imagenet_class_index.json\", \"r\") as read_file:\n",
    " class_idx = json.load(read_file)\n",
    " idx2label = [class_idx[str(k)][1] for k in range(len(class_idx))]\n",
    "\n",
    "## Import a sample image and normalize it into a tensor\n",
    "normalize = transforms.Normalize(\n",
    "    mean=[0.485, 0.456, 0.406],\n",
    "    std=[0.229, 0.224, 0.225])\n",
    "\n",
    "eval_dataset = datasets.ImageFolder(\n",
    "    os.path.dirname(\"./torch_neuron_test/\"),\n",
    "    transforms.Compose([\n",
    "        transforms.Resize([224, 224]),\n",
    "        transforms.ToTensor(),\n",
    "        normalize,\n",
    "    ])\n",
    ")\n",
    "image, _ = eval_dataset[0]\n",
    "image = torch.tensor(image.numpy()[np.newaxis, ...])\n",
    "\n",
    "## Load model\n",
    "model_neuron = torch.jit.load( 'resnet50_neuron.pt' )\n",
    "\n",
    "## Since the first inference also load the model let's exclude it \n",
    "## from timing\n",
    "results = model_neuron( image )\n",
    "\n",
    "## Predict for 100 loops\n",
    "start = time()\n",
    "\n",
    "loops = 100\n",
    "for _ in range(loops):\n",
    "    results = model_neuron( image )\n",
    "elapsed_time = time() - start\n",
    "images_sec = loops / float(elapsed_time)\n",
    "\n",
    "# Get the top 5 results\n",
    "top5_idx = results[0].sort()[1][-5:]\n",
    "\n",
    "# Lookup and print the top 5 labels\n",
    "top5_labels = [idx2label[idx] for idx in top5_idx]\n",
    "\n",
    "print(\"Top 5 labels:\\n {}\".format(top5_labels) )\n",
    "print(\"Completed {} operations in {} seconds => {} images / second\".format(loops, round(elapsed_time,2), round(images_sec,0) ) )\n"
   ]
  },
  {
   "source": [
    "## Step 6: Run on parallel neuron cores\n",
    "\n",
    "To full leverage the inferentia hardware we want to use all the cores.  On an inf1.xlarge or inf1.2xlarge we need to use 4. Here we use the futures library to create a simple class that runs four parallel inference threads\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from concurrent import futures\n",
    "import torch\n",
    "import torch.neuron\n",
    "import os\n",
    "\n",
    "class NeuronSimpleDataParallel():\n",
    "\n",
    "    def __init__(self, model_file, num_neuron_cores, batch_size=1):\n",
    "        # Construct a list of models\n",
    "        self.num_neuron_cores = num_neuron_cores\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "        class SimpleWrapper():\n",
    "\n",
    "            def __init__(self, model):\n",
    "                self.model = model\n",
    "\n",
    "            def eval(self):\n",
    "                self.model.eval()\n",
    "\n",
    "            def train(self):\n",
    "                self.model.train()\n",
    "\n",
    "            def __call__(self, *args):\n",
    "                results = self.model(*args)\n",
    "\n",
    "                # Make the output iterable - if it is not already a tuple or list\n",
    "                if not isinstance(results, tuple) or isinstance(results, list):\n",
    "                    results = [results]\n",
    "\n",
    "                return results\n",
    "\n",
    "        self.models = [SimpleWrapper(torch.jit.load(model_file))\n",
    "                       for i in range(num_neuron_cores)]\n",
    "\n",
    "        ## Important - please read:\n",
    "        ##     https://github.com/aws/aws-neuron-sdk/blob/master/docs/tensorflow-neuron/tutorial-NeuronCore-Group.md\n",
    "        ## For four cores we use \n",
    "        ##     os.environ['NEURONCORE_GROUP_SIZES'] = \"1,1,1,1\" \n",
    "        ## when launching four threads\n",
    "        ## In this logic exists in worker processes, each process should use \n",
    "        ##     os.environ['NEURONCORE_GROUP_SIZES'] = \"1\"\n",
    "        nc_env = ','.join(['1'] * num_neuron_cores)\n",
    "        os.environ['NEURONCORE_GROUP_SIZES'] = nc_env\n",
    "\n",
    "        self.executor = futures.ThreadPoolExecutor(\n",
    "            max_workers=self.num_neuron_cores)\n",
    "\n",
    "    def eval(self):\n",
    "        for m in self.models:\n",
    "            m.eval()\n",
    "\n",
    "    def train(self):\n",
    "        for m in self.models:\n",
    "            m.train()\n",
    "\n",
    "    def __call__(self, *args):\n",
    "        assert all(isinstance(a, torch.Tensor)\n",
    "                   for a in args), \"Non tensor input - tensors are needed to generate batches\"\n",
    "        assert all(a.shape[0] % self.num_neuron_cores ==\n",
    "                   0 for a in args), \"Batch size must be even multiple of the number of parallel neuron cores\"\n",
    "\n",
    "        args_per_core = [[] for i in range(self.num_neuron_cores)]\n",
    "\n",
    "        # Split args\n",
    "        for a in args:\n",
    "            # Based on batch size for arg\n",
    "            step_size = a.shape[0] // self.num_neuron_cores\n",
    "            for i in range(self.num_neuron_cores):\n",
    "                # Append a slice of a view\n",
    "                start = i * step_size\n",
    "                end = (i + 1) * step_size\n",
    "\n",
    "                # Slice\n",
    "                args_per_core[i].append(a[start:end])\n",
    "\n",
    "        # Call each core with their split and wait to complete\n",
    "        running = {self.executor.submit(\n",
    "            self.models[idx], *args_per_core[idx]): idx for idx in range(self.num_neuron_cores)}\n",
    "\n",
    "        results = []\n",
    "\n",
    "        for future in futures.as_completed(running):\n",
    "            running[future]\n",
    "\n",
    "            # Expect a tuple of tensors - convert to a list of tensors\n",
    "            results.append(future.result())\n",
    "\n",
    "        # Remove zero dimensional tensors (unsqueeze)\n",
    "        # Iterate results per core\n",
    "        for ic in range(len(results)):\n",
    "            # Iterate result tuples\n",
    "            for ir in range(len(results[ic])):\n",
    "                # Unsqueeze if zero dimensional or does not look batched (i.e. first dim does not match batch)\n",
    "                if len(results[ic][ir].size()) == 0 or results[ic][ir].shape[0] != self.batch_size:\n",
    "                    results[ic][ir] = torch.unsqueeze(\n",
    "                        results[ic][ir], 0)\n",
    "\n",
    "        # Concatenate\n",
    "        output = results[0][0]\n",
    "\n",
    "        for i in range(1, len(results)):\n",
    "            for j in range(len(results[i])):\n",
    "                output = torch.cat([output, results[i][j]], 0)\n",
    "\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from time import time\n",
    "import torch\n",
    "import torch_neuron\n",
    "import json\n",
    "import numpy as np\n",
    "from urllib import request\n",
    "from torchvision import models, transforms, datasets\n",
    "from parallel import NeuronSimpleDataParallel\n",
    "\n",
    "## Assuming you are working on and inf1.xlarge or inf1.2xlarge\n",
    "num_neuron_cores = 4\n",
    "\n",
    "## Create an image directory containing a small kitten\n",
    "os.makedirs(\"./torch_neuron_test/images\", exist_ok=True)\n",
    "request.urlretrieve(\"https://raw.githubusercontent.com/awslabs/mxnet-model-server/master/docs/images/kitten_small.jpg\",\n",
    "                    \"./torch_neuron_test/images/kitten_small.jpg\")\n",
    "\n",
    "## Fetch labels to output the top classifications\n",
    "request.urlretrieve(\"https://s3.amazonaws.com/deep-learning-models/image-models/imagenet_class_index.json\",\"imagenet_class_index.json\")\n",
    "idx2label = []\n",
    "\n",
    "with open(\"imagenet_class_index.json\", \"r\") as read_file:\n",
    "    class_idx = json.load(read_file)\n",
    "    idx2label = [class_idx[str(k)][1] for k in range(len(class_idx))]\n",
    "    \n",
    "## Import a sample image and normalize it into a tensor\n",
    "normalize = transforms.Normalize(\n",
    "    mean=[0.485, 0.456, 0.406],\n",
    "    std=[0.229, 0.224, 0.225])\n",
    "    \n",
    "eval_dataset = datasets.ImageFolder(\n",
    "    os.path.dirname(\"./torch_neuron_test/\"),\n",
    "    transforms.Compose([\n",
    "    transforms.Resize([224, 224]),\n",
    "    transforms.ToTensor(),\n",
    "    normalize,\n",
    "    ])\n",
    ")\n",
    "image, _ = eval_dataset[0]\n",
    "image = torch.tensor(image.numpy()[np.newaxis, ...])\n",
    "\n",
    "## Load model\n",
    "model_neuron = NeuronSimpleDataParallel( 'resnet50_neuron.pt', num_neuron_cores )\n",
    "\n",
    "## Create a \"batch\" image with enough images to go on each of the four cores\n",
    "batch_image = image\n",
    "\n",
    "for i in range(num_neuron_cores - 1):\n",
    "    batch_image = torch.cat( [batch_image, image], 0 )\n",
    "\n",
    "print(batch_image.shape)\n",
    "\n",
    "## Since the first inference also loads the model to the chip let's exclude it \n",
    "## from timing\n",
    "results = model_neuron( batch_image )\n",
    "\n",
    "## Predict\n",
    "loops = 100\n",
    "start = time()\n",
    "for _ in range(loops):\n",
    "    results = model_neuron( batch_image )\n",
    "elapsed_time = time() - start\n",
    "images_sec = loops * batch_image.size(0) / float(elapsed_time)\n",
    "\n",
    "# Get the top 5 results\n",
    "top5_idx = results[0].sort()[1][-5:]\n",
    "\n",
    "# Lookup and print the top 5 labels\n",
    "top5_labels = [idx2label[idx] for idx in top5_idx]\n",
    "print(\"Top 5 labels:\\n {}\".format(top5_labels) )\n",
    "print(\"Completed {} operations in {} seconds => {} images / second\".format(loops * batch_image.size(0), round(elapsed_time,2), round(images_sec,0) ) )"
   ]
  },
  {
   "source": [
    "## Step 7: Experiment with different batch sizes:\n",
    "\n",
    "Now that we are using all four cores we can experiment with compiling and running large batch sizes on each of our four cores\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "7.2 Modify the inference code"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from time import time\n",
    "import torch\n",
    "import torch_neuron\n",
    "import json\n",
    "import numpy as np\n",
    "from urllib import request\n",
    "from torchvision import models, transforms, datasets\n",
    "from parallel import NeuronSimpleDataParallel\n",
    "\n",
    "## Assuming you are working on and inf1.xlarge or inf1.2xlarge\n",
    "num_neuron_cores = 4\n",
    "batch_size = 5\n",
    "\n",
    "## Create an image directory containing a small kitten\n",
    "os.makedirs(\"./torch_neuron_test/images\", exist_ok=True)\n",
    "request.urlretrieve(\"https://raw.githubusercontent.com/awslabs/mxnet-model-server/master/docs/images/kitten_small.jpg\",\n",
    "                    \"./torch_neuron_test/images/kitten_small.jpg\")\n",
    "\n",
    "## Fetch labels to output the top classifications\n",
    "request.urlretrieve(\"https://s3.amazonaws.com/deep-learning-models/image-models/imagenet_class_index.json\",\"imagenet_class_index.json\")\n",
    "idx2label = []\n",
    "\n",
    "with open(\"imagenet_class_index.json\", \"r\") as read_file:\n",
    "    class_idx = json.load(read_file)\n",
    "    idx2label = [class_idx[str(k)][1] for k in range(len(class_idx))]\n",
    "    \n",
    "## Import a sample image and normalize it into a tensor\n",
    "normalize = transforms.Normalize(\n",
    "    mean=[0.485, 0.456, 0.406],\n",
    "    std=[0.229, 0.224, 0.225])\n",
    "    \n",
    "eval_dataset = datasets.ImageFolder(\n",
    "    os.path.dirname(\"./torch_neuron_test/\"),\n",
    "    transforms.Compose([\n",
    "    transforms.Resize([224, 224]),\n",
    "    transforms.ToTensor(),\n",
    "    normalize,\n",
    "    ])\n",
    ")\n",
    "image, _ = eval_dataset[0]\n",
    "image = torch.tensor(image.numpy()[np.newaxis, ...])\n",
    "\n",
    "## Load model\n",
    "model_neuron = NeuronSimpleDataParallel( 'resnet50_neuron_b{}.pt'.format(batch_size), num_neuron_cores, batch_size=batch_size )\n",
    "\n",
    "## Create a \"batch\" image with enough images to go on each of the four cores\n",
    "batch_image = image\n",
    "\n",
    "for i in range((num_neuron_cores * batch_size) - 1):\n",
    "    batch_image = torch.cat( [batch_image, image], 0 )\n",
    "\n",
    "## Since the first inference also loads the model to the chip let's exclude it \n",
    "## from timing\n",
    "results = model_neuron( batch_image )\n",
    "\n",
    "## Predict\n",
    "start = time()\n",
    "loops = 100\n",
    "for _ in range(loops):\n",
    "    results = model_neuron( batch_image )\n",
    "elapsed_time = time() - start\n",
    "images_sec = loops * batch_image.size(0) / elapsed_time\n",
    "\n",
    "# Get the top 5 results\n",
    "top5_idx = results[0].sort()[1][-5:]\n",
    "\n",
    "# Lookup and print the top 5 labels\n",
    "top5_labels = [idx2label[idx] for idx in top5_idx]\n",
    "print(\"Top 5 labels:\\n {}\".format(top5_labels) )\n",
    "print(\"Completed {} operations in {} seconds => {} images / second\".format( \n",
    "    loops * batch_image.size(0), round(elapsed_time, 2), round(images_sec,0) ) )"
   ]
  }
 ]
}
