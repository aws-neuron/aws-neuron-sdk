{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modify Hugging Face BERT\n",
    "Before we move on it may be worth reviewing the BERT paper: https://arxiv.org/abs/1810.04805\n",
    "\n",
    "Please ensure that you are running this notebook with reference to https://github.com/aws/aws-neuron-sdk/tree/master/src/examples/pytorch/bert_tutorial/README.md, or you may miss key installation steps and fail to produce required artifacts in prior steps.\n",
    "\n",
    "In particular we can note that we can think for BERT as having three stages:  Embeddings, encoding and output processing.  We can think of the Neuron hardware as providing a lot of acceleration where we are doing mostly linear algebra.\n",
    "\n",
    "The embedding stages (which are basically lookup tables at runtime) don't get much benefit on Neuron hardware, but the Encoding step does.  So we want to make minimally invasive changes to just override the Encoder stage to run on Neuron.\n",
    "\n",
    "To do this we inherit classes from BertEncoder, BertModel and BertForSequenceClassification classes.  This allows for a modified implementation of the NeuronBertEncoder with the same top level semantics\n",
    "\n",
    "We'll do the following:\n",
    "\n",
    "* Create a child class for BertForSequenceClassification which *just* changes the construction\n",
    "* Create a child class for BertModel.  \n",
    "  * Modify the constructor to use our derived BertEncoder\n",
    "  * Modify the forward method by copying the original, but adding a torch.neuron trace to the enncoder\n",
    "* Create a child class for the NeuronBertEncoder\n",
    "  * We need to do this since Hugging Face BERT uses some None variables and lists of None which the PyTorch jit compiler does not like (torch.jit.trace).  Since we leverage PyTorch JIT trace in torch.neuron.trace we need to remove some of the arguments we aren't using as we compiile our MRPC runtime."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Before you start!\n",
    "\n",
    "**Check you have run aws configure and setup your user credentials - otherwise these steps will fail**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check your region"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## If you do not want to train and operate in us-east-1 change this region\n",
    "## Make sure that all steps of the tutorial use the same region\n",
    "REGION=\"us-east-1\"\n",
    "\n",
    "FOLDER=\"bert_tutorial\"\n",
    "CANONICAL_LOCATION=\"s3://aws-neuron-public-tutorial-content-us-east-1/frameworks/pytorch/bert/bert_tutorial/bert-large-uncased-mrpc.tar.gz\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import some required modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from urllib.parse import urlsplit\n",
    "from urllib.parse import urlparse\n",
    "from botocore.exceptions import ClientError\n",
    "from datetime import date\n",
    "\n",
    "import boto3\n",
    "import botocore\n",
    "import os\n",
    "import time\n",
    "\n",
    "import torch\n",
    "import torch.neuron\n",
    "\n",
    "from transformers import BertModel, BertForSequenceClassification, BertTokenizer\n",
    "from transformers.modeling_bert import BertEncoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-step, check for adapted model and copy if needed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Routine to upload files to S3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def upload_and_check_file( s3_location, filename ):\n",
    "    \n",
    "    try:\n",
    "        boto3_sess = boto3.session.Session()\n",
    "    except botocore.exceptions.NoCredentialsError:\n",
    "        print(\"No credentials:  Use 'aws confgure' to setup credentials or configure isengard (Amazon internal)\")\n",
    "        raise\n",
    "\n",
    "    o = urlsplit(s3_location, allow_fragments = True)\n",
    "    mod_path = os.path.dirname( o.path )\n",
    "    mod_path = mod_path.lstrip('/')\n",
    "\n",
    "    print()\n",
    "    print(\"Copy model to: s3://\" + o.netloc + \"/\" + mod_path + \"/\")\n",
    "\n",
    "    assert( os.path.exists(filename) )\n",
    "\n",
    "    try:\n",
    "        s3_client = boto3_sess.client('s3')\n",
    "        print(\"Uploading ...\")\n",
    "        response = s3_client.upload_file(filename, o.netloc, mod_path + \"/\" + filename )\n",
    "        if response == None:\n",
    "            print(\" ... no errors\")\n",
    "        else:\n",
    "            print(\"Response: {}\".format(response))\n",
    "    except ClientError as e:\n",
    "        print(e)\n",
    "        raise\n",
    "    except:\n",
    "        raise\n",
    "\n",
    "    print()\n",
    "    print(\"Check the file uploaded OK ...\")\n",
    "    s3_resource = boto3_sess.resource('s3')\n",
    "    bucket = s3_resource.Bucket(o.netloc)\n",
    "    key = mod_path + \"/\" + filename\n",
    "    full_name = \"s3://\" + bucket.name + \"/\" + key\n",
    "\n",
    "    objs = list(bucket.objects.filter(Prefix=key))\n",
    "\n",
    "    print()\n",
    "    if len(objs) > 0 and objs[0].key == key:\n",
    "        print(\"{} exists!\".format(full_name))\n",
    "    else:\n",
    "        print(\"{} doesn't exist\".format(full_name))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Routine to check S3 locations exist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_s3_location( bucket_name, key ):\n",
    "    print(\"Check the S3 files exist ...\")\n",
    "    s3_resource = boto3_sess.resource('s3')\n",
    "    bucket = s3_resource.Bucket(bucket_name)\n",
    "\n",
    "    try:\n",
    "        objs = list(bucket.objects.filter(Prefix=key))\n",
    "    except s3_resource.meta.client.exceptions.NoSuchBucket:\n",
    "        return False\n",
    "    \n",
    "    if len(objs) > 0 and objs[0].key == key:\n",
    "        return True\n",
    "\n",
    "    return False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check that an S3 bucket and known location exist for this account\n",
    "If you didn't run Stage 1 (optional), and don't have the adapted MRPC model this code will:\n",
    "* Download one prepared earlier\n",
    "* Create a new S3 bucket in your account\n",
    "* Upload the adapted MRPC model\n",
    "\n",
    "If the expected file already exists this step will proceed without any additional downloads or uploads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import sys\n",
    "\n",
    "def eprint(*args, **kwargs):\n",
    "    print(*args, file=sys.stderr, **kwargs)\n",
    "\n",
    "if REGION == None or FOLDER == None or CANONICAL_LOCATION == None:\n",
    "    eprint(\"The following variables need to be set:\")\n",
    "    eprint(\"REGION = {}\".format(REGION))\n",
    "    eprint(\"FOLDER = {}\".format(FOLDER))\n",
    "    eprint(\"CANONICAL_LOCATION = {}\".format(CANONICAL_LOCATION))\n",
    "    eprint()\n",
    "    eprint(\"Did you forget to execute the top cell?\")\n",
    "\n",
    "    raise \n",
    "\n",
    "bucket_prefix=\"inferentia-test-\"\n",
    "\n",
    "try:\n",
    "    boto3_sess = boto3.session.Session()\n",
    "except botocore.exceptions.NoCredentialsError:\n",
    "    print(\"No credentials:  Use 'aws confgure' to setup credentials or configure isengard (Amazon internal)\")\n",
    "    raise\n",
    "\n",
    "try:\n",
    "    sts_client = boto3.client('sts')\n",
    "    response = sts_client.get_caller_identity()\n",
    "    #if response == None:\n",
    "    #    print(\" ... no errors\")\n",
    "    #else:\n",
    "    #    print(\"Response: {}\".format(response))\n",
    "except ClientError as e:\n",
    "    print(e)\n",
    "    raise\n",
    "except:\n",
    "    raise\n",
    "\n",
    "ACCOUNT=response['Account']\n",
    "#TIMESTAMP=date.today().strftime(\"%Y-%m-%d\")\n",
    "\n",
    "bucket_name=bucket_prefix + ACCOUNT\n",
    "bucket_path=FOLDER\n",
    "filename=\"bert-large-uncased-mrpc.tar.gz\"\n",
    "key = bucket_path + \"/\" + filename\n",
    "s3_location = \"s3://\" + bucket_name + \"/\" + key\n",
    "\n",
    "if test_s3_location( bucket_name, key ):\n",
    "    print(\"{} exists!  using your adapted model\".format(s3_location))\n",
    "else:\n",
    "    print(\"{} doesn't exist! Copying the AWS Neuron default model, and upload!\".format(s3_location))\n",
    "    \n",
    "    ## Create a bucket in your account!\n",
    "    print(\"Using region '{}'\".format(REGION))\n",
    "    s3_client = boto3.client('s3', region_name=REGION)\n",
    "    \n",
    "    response = None\n",
    "    if REGION != \"us-east-1\":\n",
    "        location = {'LocationConstraint': REGION}   \n",
    "        response = s3_client.create_bucket(Bucket=bucket_name,\n",
    "                                CreateBucketConfiguration=location)\n",
    "    else:\n",
    "        response = s3_client.create_bucket(Bucket=bucket_name)\n",
    "    \n",
    "    print(\"Created bucket at {} response = {}\".format(bucket_name, response))\n",
    "    \n",
    "    ## Download the model\n",
    "    s3_resource = boto3_sess.resource('s3')\n",
    "    parsed = urlparse(CANONICAL_LOCATION)\n",
    "    path = parsed.path.lstrip('/')\n",
    "    adapted_model_file = os.path.basename(path)\n",
    "    s3_resource.Bucket(parsed.netloc).download_file(path, adapted_model_file)\n",
    "    os.path.exists(adapted_model_file)\n",
    "    \n",
    "    print(\"Downloaded file\")\n",
    "    \n",
    "    ## Upload the adapted model to your S3 (for later use)\n",
    "    upload_and_check_file( s3_location=s3_location, filename=adapted_model_file)\n",
    "    \n",
    "assert(test_s3_location(bucket_name, key))\n",
    "\n",
    "print(\"S3 location = {} confirmed!\".format(s3_location))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modify BertEncoder\n",
    "\n",
    "This is a modified Encoder class.  If you compare with the original code you can see that we have stripped off two unused arguments in the forward method.  Instead we will initialize them in the body of the forward method.  This change was required to allow torch.jit.trace to trace the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuronForwardBertEncoder(BertEncoder):\n",
    "    def __init__(self, config):\n",
    "        super().__init__(config)\n",
    "        self.opt_encoder = None\n",
    "        \n",
    "    def forward(\n",
    "        self,\n",
    "        hidden_states,\n",
    "        attention_mask\n",
    "    ):\n",
    "        ## Changes to allow torch.jit.trace to run\n",
    "        head_mask=[None] * len(self.layer)\n",
    "        encoder_hidden_states=None\n",
    "        encoder_attention_mask=None\n",
    "        \n",
    "        all_hidden_states = ()\n",
    "        all_attentions = ()\n",
    "        for i, layer_module in enumerate(self.layer):\n",
    "            if self.output_hidden_states:\n",
    "                all_hidden_states = all_hidden_states + (hidden_states,)\n",
    "\n",
    "            layer_outputs = layer_module(\n",
    "                hidden_states, attention_mask, head_mask[i], encoder_hidden_states, encoder_attention_mask\n",
    "            )\n",
    "            hidden_states = layer_outputs[0]\n",
    "\n",
    "            if self.output_attentions:\n",
    "                all_attentions = all_attentions + (layer_outputs[1],)\n",
    "\n",
    "        # Add last layer\n",
    "        if self.output_hidden_states:\n",
    "            all_hidden_states = all_hidden_states + (hidden_states,)\n",
    "\n",
    "        outputs = (hidden_states,)\n",
    "        if self.output_hidden_states:\n",
    "            outputs = outputs + (all_hidden_states,)\n",
    "        if self.output_attentions:\n",
    "            outputs = outputs + (all_attentions,)\n",
    "        return outputs  # last-layer hidden state, (all hidden states), (all attentions)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modify BertModel\n",
    "\n",
    "Here we override the BERT model class.  This is a straight copy of the original code with three simple modifications.\n",
    "\n",
    "1. We are using the the modified BertEncoder forward function\n",
    "1. We are invoking neuron trace in the forward path\n",
    "1. We are passing some additional arguments we'll use for compilation\n",
    "\n",
    "Additionally, there is also some commented code which will output the torchscript sub-graph (and human readable format) and example input to the sub-graph\n",
    "\n",
    "When we construct and invoke our top level class it will create a compiled module.  We'll see below how we can then save that into an Neuron optimized PyTorch file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuronForwardBertModel(BertModel):\n",
    "    \n",
    "    def __init__(self, config, compiler_options, optimization=None, use_cached_compiler_output=False ):\n",
    "        super().__init__(config)\n",
    "        assert(self.config.torchscript == True)\n",
    "        self.compiler_options = compiler_options\n",
    "        self.optimization=optimization\n",
    "        self.use_cached_compiler_output=use_cached_compiler_output\n",
    "        self.encoder = NeuronForwardBertEncoder(config)\n",
    "\n",
    "        self.init_weights()\n",
    "    \n",
    "    def forward(\n",
    "        self,\n",
    "        input_ids=None,\n",
    "        attention_mask=None,\n",
    "        token_type_ids=None,\n",
    "        position_ids=None,\n",
    "        head_mask=None,\n",
    "        inputs_embeds=None,\n",
    "        encoder_hidden_states=None,\n",
    "        encoder_attention_mask=None,\n",
    "    ):\n",
    "        \n",
    "        ## Now use copied code from BertModel.forward\n",
    "        if input_ids is not None and inputs_embeds is not None:\n",
    "            raise ValueError(\"You cannot specify both input_ids and inputs_embeds at the same time\")\n",
    "        elif input_ids is not None:\n",
    "            input_shape = input_ids.size()\n",
    "        elif inputs_embeds is not None:\n",
    "            input_shape = inputs_embeds.size()[:-1]\n",
    "        else:\n",
    "            raise ValueError(\"You have to specify either input_ids or inputs_embeds\")\n",
    "\n",
    "        device = input_ids.device if input_ids is not None else inputs_embeds.device\n",
    "\n",
    "        if attention_mask is None:\n",
    "            attention_mask = torch.ones(input_shape, device=device)\n",
    "        if token_type_ids is None:\n",
    "            token_type_ids = torch.zeros(input_shape, dtype=torch.long, device=device)\n",
    "\n",
    "        # We can provide a self-attention mask of dimensions [batch_size, from_seq_length, to_seq_length]\n",
    "        # ourselves in which case we just need to make it broadcastable to all heads.\n",
    "        if attention_mask.dim() == 3:\n",
    "            extended_attention_mask = attention_mask[:, None, :, :]\n",
    "        elif attention_mask.dim() == 2:\n",
    "            # Provided a padding mask of dimensions [batch_size, seq_length]\n",
    "            # - if the model is a decoder, apply a causal mask in addition to the padding mask\n",
    "            # - if the model is an encoder, make the mask broadcastable to [batch_size, num_heads, seq_length, seq_length]\n",
    "            if self.config.is_decoder:\n",
    "                batch_size, seq_length = input_shape\n",
    "                seq_ids = torch.arange(seq_length, device=device)\n",
    "                causal_mask = seq_ids[None, None, :].repeat(batch_size, seq_length, 1) <= seq_ids[None, :, None]\n",
    "                causal_mask = causal_mask.to(\n",
    "                    attention_mask.dtype\n",
    "                )  # causal and attention masks must have same type with pytorch version < 1.3\n",
    "                extended_attention_mask = causal_mask[:, None, :, :] * attention_mask[:, None, None, :]\n",
    "            else:\n",
    "                extended_attention_mask = attention_mask[:, None, None, :]\n",
    "        else:\n",
    "            raise ValueError(\n",
    "                \"Wrong shape for input_ids (shape {}) or attention_mask (shape {})\".format(\n",
    "                    input_shape, attention_mask.shape\n",
    "                )\n",
    "            )\n",
    "\n",
    "        # Since attention_mask is 1.0 for positions we want to attend and 0.0 for\n",
    "        # masked positions, this operation will create a tensor which is 0.0 for\n",
    "        # positions we want to attend and -10000.0 for masked positions.\n",
    "        # Since we are adding it to the raw scores before the softmax, this is\n",
    "        # effectively the same as removing these entirely.\n",
    "        extended_attention_mask = extended_attention_mask.to(dtype=next(self.parameters()).dtype)  # fp16 compatibility\n",
    "        extended_attention_mask = (1.0 - extended_attention_mask) * -10000.0\n",
    "\n",
    "        # If a 2D ou 3D attention mask is provided for the cross-attention\n",
    "        # we need to make broadcastabe to [batch_size, num_heads, seq_length, seq_length]\n",
    "        if self.config.is_decoder and encoder_hidden_states is not None:\n",
    "            encoder_batch_size, encoder_sequence_length, _ = encoder_hidden_states.size()\n",
    "            encoder_hidden_shape = (encoder_batch_size, encoder_sequence_length)\n",
    "            if encoder_attention_mask is None:\n",
    "                encoder_attention_mask = torch.ones(encoder_hidden_shape, device=device)\n",
    "\n",
    "            if encoder_attention_mask.dim() == 3:\n",
    "                encoder_extended_attention_mask = encoder_attention_mask[:, None, :, :]\n",
    "            elif encoder_attention_mask.dim() == 2:\n",
    "                encoder_extended_attention_mask = encoder_attention_mask[:, None, None, :]\n",
    "            else:\n",
    "                raise ValueError(\n",
    "                    \"Wrong shape for encoder_hidden_shape (shape {}) or encoder_attention_mask (shape {})\".format(\n",
    "                        encoder_hidden_shape, encoder_attention_mask.shape\n",
    "                    )\n",
    "                )\n",
    "\n",
    "            encoder_extended_attention_mask = encoder_extended_attention_mask.to(\n",
    "                dtype=next(self.parameters()).dtype\n",
    "            )  # fp16 compatibility\n",
    "            encoder_extended_attention_mask = (1.0 - encoder_extended_attention_mask) * -10000.0\n",
    "        else:\n",
    "            encoder_extended_attention_mask = None\n",
    "\n",
    "        # Prepare head mask if needed\n",
    "        # 1.0 in head_mask indicate we keep the head\n",
    "        # attention_probs has shape bsz x n_heads x N x N\n",
    "        # input head_mask has shape [num_heads] or [num_hidden_layers x num_heads]\n",
    "        # and head_mask is converted to shape [num_hidden_layers x batch x num_heads x seq_length x seq_length]\n",
    "        if head_mask is not None:\n",
    "            if head_mask.dim() == 1:\n",
    "                head_mask = head_mask.unsqueeze(0).unsqueeze(0).unsqueeze(-1).unsqueeze(-1)\n",
    "                head_mask = head_mask.expand(self.config.num_hidden_layers, -1, -1, -1, -1)\n",
    "            elif head_mask.dim() == 2:\n",
    "                head_mask = (\n",
    "                    head_mask.unsqueeze(1).unsqueeze(-1).unsqueeze(-1)\n",
    "                )  # We can specify head_mask for each layer\n",
    "            head_mask = head_mask.to(\n",
    "                dtype=next(self.parameters()).dtype\n",
    "            )  # switch to fload if need + fp16 compatibility\n",
    "        else:\n",
    "            head_mask = [None] * self.config.num_hidden_layers\n",
    "\n",
    "        embedding_output = self.embeddings(\n",
    "            input_ids=input_ids, position_ids=position_ids, token_type_ids=token_type_ids, inputs_embeds=inputs_embeds\n",
    "        )\n",
    "        \n",
    "        #############################\n",
    "        ## Neuron changes start here\n",
    "\n",
    "        ## Set to eval - set up the inputs for tracing\n",
    "        self.encoder.eval()        \n",
    "        example_inputs=[embedding_output,extended_attention_mask]\n",
    "\n",
    "        ## We move these args from the example inputs since they break jit.trace\n",
    "        #,head_mask,encoder_hidden_states,encoder_extended_attention_mask]\n",
    "\n",
    "        ### Use this code to output the torch script graph for inspection, as well as the inputs for a sample\n",
    "        ### to allow for work directly using torchscript\n",
    "        \"\"\"\n",
    "        encoder = torch.jit.trace( self.encoder, example_inputs=example_inputs )  \n",
    "        print(\"Save JIT trace to test.pt\")\n",
    "        torch.jit.save( encoder, \"./test.pt\")\n",
    "        torch.save( example_inputs, \"example_tensor.pt\")\n",
    "        with open(\"./test_graph.txt\", \"w\") as f:\n",
    "            f.write(str(encoder.graph))\n",
    "        exit(1)\n",
    "        \"\"\"\n",
    "\n",
    "        ## Compile the neuron code into the encode - once we trace the whole model and save we have what we need\n",
    "        self.encoder = torch.neuron.trace( \n",
    "            self.encoder, example_inputs=example_inputs, \n",
    "            fallback=False, \n",
    "            compiler_workdir=\"./compile\", \n",
    "            compiler_args=self.compiler_options, \n",
    "            optimize=self.optimization, \n",
    "            use_cached_compiler_output=self.use_cached_compiler_output )\n",
    "        \n",
    "        \"\"\"\n",
    "        # Orginal code\n",
    "        encoder_outputs = self.encoder(\n",
    "            embedding_output,\n",
    "            attention_mask=extended_attention_mask,\n",
    "            head_mask=head_mask,\n",
    "            encoder_hidden_states=encoder_hidden_states,\n",
    "            encoder_attention_mask=encoder_extended_attention_mask, \n",
    "        )\n",
    "        \"\"\"\n",
    "\n",
    "        # NOTE: Using ordered args rather than KW args\n",
    "        #encoder_outputs = self.encoder(\n",
    "        #    embedding_output,\n",
    "        #    attention_mask=extended_attention_mask\n",
    "        #)\n",
    "        encoder_outputs = self.encoder(\n",
    "            embedding_output,\n",
    "            extended_attention_mask\n",
    "        )\n",
    "        \n",
    "        ## END Neuron changes\n",
    "        #############################\n",
    "        \n",
    "        sequence_output = encoder_outputs[0]\n",
    "        pooled_output = self.pooler(sequence_output)\n",
    "\n",
    "        outputs = (sequence_output, pooled_output,) + encoder_outputs[\n",
    "            1:\n",
    "        ]  # add hidden_states and attentions if they are here\n",
    "        \n",
    "        return outputs  # sequence_output, pooled_output, (hidden_states), (attentions)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modify BertForSequenceClassification\n",
    "\n",
    "The top level wrapper class.  This is just a constructor over-ride replaceing the old Bert Model with our updated one, and passing through some additional argument for torch.neuron.trace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Simple wrapper class to pass through some additional variables        \n",
    "class NeuronForwardBertForSequenceClassification(BertForSequenceClassification):\n",
    "    \n",
    "    def __init__(self, config, compiler_options, optimization=None, use_cached_compiler_output=False ):\n",
    "\n",
    "        super().__init__(config)\n",
    "        \n",
    "        self.bert = NeuronForwardBertModel(config,compiler_options, optimization=optimization, use_cached_compiler_output=use_cached_compiler_output)\n",
    "        \n",
    "        self.init_weights()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download Adapted MRPC Model\n",
    "\n",
    "This code will fetch the model we saved earlier.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download and decompress the S3 location in the cell above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import botocore\n",
    "import tarfile\n",
    "import os\n",
    "from urllib.parse import urlparse\n",
    "\n",
    "try:\n",
    "    boto3_sess = boto3.session.Session()\n",
    "except botocore.exceptions.NoCredentialsError:\n",
    "    print(\"No credentials:  Use 'aws confgure' to setup credentials or configure isengard (Amazon internal)\")\n",
    "    raise\n",
    "except:\n",
    "    raise\n",
    "\n",
    "s3 = boto3_sess.resource('s3')\n",
    "parsed = urlparse(s3_location)    \n",
    "\n",
    "path = parsed.path.lstrip('/')\n",
    "saved_model_tgz = os.path.basename(path)\n",
    "    \n",
    "if not os.path.exists(saved_model_tgz):\n",
    "    print(\"Downloading file\")\n",
    "    s3.Bucket(parsed.netloc).download_file(path, saved_model_tgz)\n",
    "else:\n",
    "    print(\"File already downloaded\")\n",
    "\n",
    "directory = \"./\"\n",
    "previous_directory_state = set(os.listdir(directory))\n",
    "print(\"Decompressing file\")\n",
    "t = tarfile.open(saved_model_tgz)\n",
    "t.extractall()\n",
    "\n",
    "current_directory_state = set(os.listdir(directory))\n",
    "changed_filenames = current_directory_state - previous_directory_state\n",
    "\n",
    "print(\"Changes:\")\n",
    "\n",
    "for file in changed_filenames:\n",
    "    print(file)\n",
    "\n",
    "print(\"-- Pretrained model downloaded and decompressed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### List the local files\n",
    "\n",
    "You can visually confirm that the adapted model was downloaded and unzipped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls -al "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set up a sanity test, neuron trace and save the model\n",
    "\n",
    "This code will set up some input values, run a CPU sanity test, compile the encoder and generate several loadable PyTorch models.\n",
    "\n",
    "Detail:\n",
    "\n",
    "1. We are using the tokenizer related to the adapted model to create input tokens.  With a maximum length of 128 to encode the sentence pairs, a mask to say which sentences are which, and a mask to show which elements are padding\n",
    "1. We run the inference on CPU using our adapted mode to sanity check out results (note it takes 80 - 90 seconds to run two BERT large inferences on CPU)\n",
    "1. We compile the sub-graph with modified network by running a forward pass (this takes ~30-40 minutes)\n",
    "1. We do a torch.jit.trace on the whole thing in memory, and not just the sub-graph we have neuron optimized (this creates torchscript for the entire model, which we can save and load)\n",
    "1. We test save the model - then test loading it\n",
    "1. We repeat this process four times to generate models which run on each of the four neuron cores of a single inf1, but skip the costly part of the loop (the compilation)\n",
    "1. We upload the set of four models to S3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compile_bert():\n",
    "    print(\"-- Loading MRPC Adapted BERT\")\n",
    "    tokenizer = BertTokenizer.from_pretrained(\"./bert-large-uncased-mrpc/\")\n",
    " \n",
    "    ## Run four inferences per neuron core\n",
    "    batch_size = 4\n",
    "    \n",
    "    # Maximum input length for the two combined sentence input is 128\n",
    "    max_length = 128\n",
    "    \n",
    "    # Let's make four models to load four neuron cores (one inf1.xlarge)\n",
    "    num_models = 4\n",
    "    \n",
    "    ## Example sentences\n",
    "    sentence_0 = \"Federal agents said yesterday they are investigating the theft.\"\n",
    "    sentence_1 = \"The agents indicated they were looking for stolen property.\"\n",
    "    sentence_2 = \"A Cuban architect was sentenced to 20 years in prison Friday.\"\n",
    "    inputs_1 = tokenizer.encode_plus(sentence_0, sentence_1, add_special_tokens=True, return_tensors='pt', max_length= max_length, pad_to_max_length=True)\n",
    "    inputs_2 = tokenizer.encode_plus(sentence_0, sentence_2, add_special_tokens=True, return_tensors='pt', max_length= max_length ,pad_to_max_length=True)\n",
    "\n",
    "    tokens_tensor = inputs_1['input_ids']\n",
    "    segments_tensors = inputs_1['token_type_ids']\n",
    "    attention_mask = inputs_1['attention_mask']\n",
    "\n",
    "    print(\"=== Confirm that BERT is doing something sane ===\")\n",
    "    print(\"Sentence 0 = '{}'\".format(sentence_0))\n",
    "    print(\"Sentence 1 = '{}'\".format(sentence_1))\n",
    "    print(\"Sentence 2 = '{}'\".format(sentence_2))\n",
    "    print()\n",
    "    \n",
    "    print(\"==Tokens (word ids) tensor==\")\n",
    "    print(tokens_tensor)\n",
    "    print(tokens_tensor.size())\n",
    "    print()\n",
    "    print(\"NB: 101 = [CLS] == start, 102 = [SEP] == separator\")\n",
    "\n",
    "    print(\"==Segment tensor (which word are in each sentence to compare)==\")\n",
    "    print(segments_tensors)\n",
    "    print(segments_tensors.size())\n",
    "    print()\n",
    "\n",
    "    print(\"==Attention tensor (which word are in each sentence to compare)==\")\n",
    "    print(\"==(words vs padding to 128 constant width)==\")\n",
    "    print(attention_mask)\n",
    "    print(attention_mask.size())\n",
    "    print()\n",
    "    \n",
    "    # Batch 2\n",
    "    if batch_size == 2:\n",
    "        tokens_tensor = torch.cat( [ inputs_1['input_ids'], inputs_2['input_ids'] ] )\n",
    "        segments_tensors = torch.cat( [ inputs_1['token_type_ids'], inputs_2['token_type_ids'] ] )\n",
    "        attention_mask = torch.cat( [ inputs_1['attention_mask'], inputs_2['attention_mask'] ] )\n",
    "\n",
    "    # Batch 4\n",
    "    if batch_size == 4:\n",
    "        tokens_tensor = torch.cat( [ inputs_1['input_ids'], inputs_2['input_ids'], inputs_1['input_ids'], inputs_2['input_ids'] ] )\n",
    "        segments_tensors = torch.cat( [ inputs_1['token_type_ids'], inputs_2['token_type_ids'], inputs_1['token_type_ids'], inputs_2['token_type_ids'] ] )\n",
    "        attention_mask = torch.cat( [ inputs_1['attention_mask'], inputs_2['attention_mask'], inputs_1['attention_mask'], inputs_2['attention_mask'] ] )\n",
    "    \n",
    "    dummy_input_1 = [tokens_tensor, attention_mask, segments_tensors]\n",
    "    \n",
    "    print(\"Trace input sizes\")\n",
    "    print(\"===\")\n",
    "    for inp in dummy_input_1:\n",
    "        print( inp.size() )\n",
    "    \n",
    "    bert_pretrained_model_dir=\"bert-large-uncased-mrpc/\"\n",
    "    \n",
    "    ## These compiler options are not user tunable for now, please reach out to the neuron team through\n",
    "    ## github if you feel that the default compiler options do not work correctly for your model\n",
    "    ## these are tuned for BERT large\n",
    "    compiler_options=\"--verbose=1 -O2\"\n",
    "    optimization=\"aggressive\"\n",
    "    neuron_model_output_names = []\n",
    "    \n",
    "    print()\n",
    "    print(\"-- Generating {} model(s)\".format(num_models))\n",
    "    for i in range(num_models):\n",
    "        name = \"bert_large_mrpc_pytorch_batch\" + str(batch_size) + '_' + str(i) + \".pt\"\n",
    "        print(\" - {}\".format(name) )\n",
    "        neuron_model_output_names.append(name)\n",
    "    \n",
    "    print()\n",
    "    print(\"Load pretrained model\")\n",
    "    \n",
    "    neuron_model = NeuronForwardBertForSequenceClassification.from_pretrained( \n",
    "        bert_pretrained_model_dir, \n",
    "        torchscript=True, \n",
    "        compiler_options=compiler_options, \n",
    "        optimization=optimization )\n",
    "    neuron_model.eval()\n",
    "    \n",
    "    ## Now compile for neuron test\n",
    "    print()\n",
    "    print(\"Partially compile Neuron Test BERT for PyTorch '{}'\".format(neuron_model_output_names[0]) )\n",
    "    start = time.time()    \n",
    "    \n",
    "    output = neuron_model(*dummy_input_1)\n",
    "    print(output)\n",
    "    delta = time.time() - start\n",
    "    print(\"Compile time is {} seconds\".format(delta))\n",
    "    \n",
    "    start = time.time()  \n",
    "    \n",
    "    ## What is this for?  To save the whole model we need to do a regular jit trace (not a neuron trace)\n",
    "    ## over the *rest* of the model.  The already traced neuron part will be skipped. Then we have something \n",
    "    ## we can save and load into pytorch\n",
    "    neuron_model = torch.jit.trace( neuron_model, example_inputs=dummy_input_1 )\n",
    "    delta = time.time() - start\n",
    "    \n",
    "    print(\"Retrace time (to save) is {} seconds\".format(delta))\n",
    "    torch.jit.save( neuron_model, neuron_model_output_names.pop(0))\n",
    "    \n",
    "    num_models_left = num_models - 1\n",
    "\n",
    "    for _ in range(num_models_left):\n",
    "        print()\n",
    "        print(\"Load pretrained model\")\n",
    "        \n",
    "        ## We are telling the code to skip the time consuming part - compiling the model    \n",
    "        neuron_model = NeuronForwardBertForSequenceClassification.from_pretrained( \n",
    "            bert_pretrained_model_dir, \n",
    "            torchscript=True, \n",
    "            compiler_options=compiler_options, \n",
    "            optimization=optimization, \n",
    "            use_cached_compiler_output=True )\n",
    "        \n",
    "        neuron_model.eval()\n",
    "        \n",
    "        ## Now compile for neuron test\n",
    "        print()\n",
    "        print(\"Partially compile Neuron Test BERT for PyTorch '{}'\".format(neuron_model_output_names[0]) )\n",
    "        print(\"  -> compiler_args={}\".format(compiler_options))\n",
    "        start = time.time()\n",
    "        output = neuron_model( *dummy_input_1 )\n",
    "        delta = time.time() - start\n",
    "        print(\"Compile time is {} seconds\".format(delta))\n",
    "        print(output)\n",
    "        \n",
    "        print(\"JIT trace the whole graph now so we can save it in torchscript #{} (retrace)\".format(i))\n",
    "        start = time.time()    \n",
    "        neuron_model = torch.jit.trace( neuron_model, example_inputs=dummy_input_1 )\n",
    "        delta = time.time() - start\n",
    "        print(\"Retrace time (to save) is {} seconds\".format(delta))\n",
    "        \n",
    "        #print(\"Attempt to save and reload\")\n",
    "        print(\"Save file\")\n",
    "        torch.jit.save( neuron_model, neuron_model_output_names.pop(0))        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now we can run it!\n",
    "The compilation will take some time, but produces a very fast BERT large for inference.\n",
    "\n",
    "A number of compiler messages will appear in your jupyter notebook log.  This is normal since it is running as a sub-process.  You will also see messages like:\n",
    "\n",
    "```\n",
    "[E neuron_runtime.cpp:85] grpc server unix:/run/neuron.sock is unavailable. Is neuron-rtd running? Is socket /run/neuron.sock writable?\n",
    "[E neuron_op_impl.cpp:52] Warning: Neuron runtime cannot be initialized; falling back to CPU execution\n",
    "[E neuron_op_impl.cpp:53] Warning: Tensor output are ** NOT CALCULATED ** during CPU execution and only indicate tensor shape\n",
    "```\n",
    "\n",
    "These are normal on CPU and not a source for concern"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compile_bert()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Upload the created files to S3 (same bucket we fetched from)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import botocore\n",
    "from urllib.parse import urlsplit\n",
    "from botocore.exceptions import ClientError\n",
    "import os\n",
    "\n",
    "        \n",
    "assert( s3_location != None )\n",
    "\n",
    "batch_size=4\n",
    "num_models=4\n",
    "neuron_model_output_names=[]\n",
    "for i in range(num_models):\n",
    "    name = \"bert_large_mrpc_pytorch_batch\" + str(batch_size) + '_' + str(i) + \".pt\"\n",
    "    print(\" - {}\".format(name) )\n",
    "    neuron_model_output_names.append(name)\n",
    "\n",
    "assert( neuron_model_output_names != None )\n",
    "assert( len(neuron_model_output_names) != 0 )\n",
    "\n",
    "for filename in neuron_model_output_names:\n",
    "    upload_and_check_file( s3_location, filename )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch_compile",
   "language": "python",
   "name": "torch_compile"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
