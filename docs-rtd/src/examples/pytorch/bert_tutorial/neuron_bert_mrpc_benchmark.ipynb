{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Benchmark PyTorch BERT on Inferentia\n",
    "Please ensure that you are running this notebook with reference to https://github.com/aws/aws-neuron-sdk/tree/master/src/examples/pytorch/bert_tutorial/README.md, or you may miss key installation steps and fail to produce required artifacts in prior steps.\n",
    "\n",
    "In previous steps in our tutorial we adapted the Hugging Face BERT model to the MRPC dataset for sequence classification.  We used the PyTorch Neuron trace command to compile for inferentia hardware, and uploaded the resulting PyTorch model files to S3.\n",
    "\n",
    "In this tutorial we will fetch the previously trained model archives and benchmark performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import some modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.neuron\n",
    "import os\n",
    "import sys\n",
    "import pandas as pd\n",
    "\n",
    "import shutil\n",
    "import boto3\n",
    "import botocore\n",
    "\n",
    "from urllib.parse import urlparse\n",
    "from urllib.parse import urlsplit\n",
    "\n",
    "from transformers import BertTokenizer\n",
    "from concurrent import futures\n",
    "\n",
    "import time\n",
    "import datetime\n",
    "from datetime import date\n",
    "import csv\n",
    "import boto3\n",
    "import botocore\n",
    "\n",
    "import numpy as np\n",
    "import multiprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PyTorch DataSet based on a TSV file\n",
    "This will read in test data and enable some stanadrd PyTorch infrastructure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BertTestDataset(torch.utils.data.Dataset):\n",
    "    \"\"\"Bert test dataset.\"\"\"\n",
    "\n",
    "    def __init__(self, tsv_file, tokenizer, max_length=128, transform=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            csv_file (string): Path to the csv file with annotations.\n",
    "            tokenizer (callable = hugging face tokenizer):  Takes a string and encodes to standard input tensor set\n",
    "            max_length (int): Maximum length that all input tensors will be padded to\n",
    "            transform (callable, optional): Optional transform to be applied\n",
    "                on a sample.\n",
    "        \"\"\"\n",
    "        with open(tsv_file, \"r\") as f:\n",
    "            reader = csv.reader(f, delimiter=\"\\t\", quotechar=None)\n",
    "            lines = list(reader)\n",
    "\n",
    "        lines.pop(0)\n",
    "\n",
    "        self.sentence_frame = pd.DataFrame( lines )\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.sentence_frame)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist()\n",
    "\n",
    "        s1_raw = self.sentence_frame.iloc[idx,3] \n",
    "        if isinstance(s1_raw, bytes):\n",
    "            s1_raw = s1_raw.decode(\"utf-8\", \"ignore\")\n",
    "        s2_raw = self.sentence_frame.iloc[idx,4]\n",
    "        if isinstance(s2_raw, bytes):\n",
    "            s1_raw = s1_raw.decode(\"utf-8\", \"ignore\")\n",
    "\n",
    "        quality = self.sentence_frame.iloc[idx,0]\n",
    "\n",
    "        encoded =  self.tokenizer.encode_plus(s1_raw, s2_raw, add_special_tokens=True,\n",
    "                    return_tensors='pt', max_length=self.max_length, pad_to_max_length=True)\n",
    "\n",
    "        sample = {'encoded': encoded, 'quality': quality}\n",
    "\n",
    "        if self.transform:\n",
    "            sample = self.transform(sample)\n",
    "\n",
    "        return sample\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BERT Results class\n",
    "We'll collect results on accuracy, latency and throughput here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BertResults():\n",
    "\n",
    "    def __init__(self, parallel, batch_size):\n",
    "        self.correct_count = 0 \n",
    "        self.inference_count = 0 \n",
    "        self.latency_array = [] \n",
    "        self.end_times = []\n",
    "        self.total_latency_parallel = 0.0 \n",
    "        self.parallel = parallel \n",
    "        self.batch_size = batch_size \n",
    "\n",
    "    def add_result( self, correct_count, inference_count, latency_array, end_times, total_latency):\n",
    "        self.correct_count += correct_count\n",
    "\n",
    "        self.inference_count += inference_count\n",
    "        self.latency_array.extend( latency_array )\n",
    "        self.end_times.extend( end_times )\n",
    "        self.total_latency_parallel += total_latency\n",
    "\n",
    "    def report( self, f ):\n",
    "        assert(len(self.latency_array) != 0)\n",
    "        p50_latency = np.percentile( self.latency_array, 50 )\n",
    "        p90_latency = np.percentile( self.latency_array, 90 )\n",
    "        p95_latency = np.percentile( self.latency_array, 95 )\n",
    "        p99_latency = np.percentile( self.latency_array, 99 )\n",
    "        p100_latency = np.percentile( self.latency_array, 100 )\n",
    "\n",
    "        if self.total_latency_parallel == 0.0:\n",
    "            self.total_latency_parallel = 1.0\n",
    "\n",
    "        ## Take all of the end time-stamps and construct a time binned histogram\n",
    "        hist, bin_edges = np.histogram(self.end_times, )\n",
    "\n",
    "        overall_throughput = self.parallel * self.inference_count / float(self.total_latency_parallel)\n",
    "\n",
    "        f.write(\"\\n\")\n",
    "        f.write(\"Histogram throughput (UTC times):\\n\")\n",
    "        f.write(\"===\\n\")\n",
    "        max_throughput = 0.0\n",
    "        for i in range(len(hist)):\n",
    "            delta = bin_edges[i+1] - bin_edges[i]\n",
    "            ## Each datestamp is batch size inferences\n",
    "            throughput = self.batch_size * hist[i] / delta\n",
    "            if throughput > max_throughput:\n",
    "                max_throughput = throughput\n",
    "            st1 = datetime.datetime.fromtimestamp(bin_edges[i]).strftime('%H:%M:%S.%f')[:-3]\n",
    "            st2 = datetime.datetime.fromtimestamp(bin_edges[i+1]).strftime('%H:%M:%S.%f')[:-3]\n",
    "            f.write(\"{} - {} => {} sentences/sec\\n\".format(st1, st2, int(throughput)) )\n",
    "        f.write(\"\\n\")\n",
    "        f.write(\"Maximum throughput (histogram) = {} sentences/sec\\n\".format(int(max_throughput)))\n",
    "        f.write(\"Overall throughput (aggregate stats * parallel) = {} sentences/sec\\n\".format(int(overall_throughput)) )\n",
    "\n",
    "        f.write(\"\\n\")\n",
    "        f.write(\"Latency Percentiles:\\n\")\n",
    "        f.write(\"===\\n\")\n",
    "        f.write(\"P50  = {} milliseconds\\n\".format(int(1000*p50_latency)))\n",
    "        f.write(\"P90  = {} milliseconds\\n\".format(int(1000*p90_latency)))\n",
    "        f.write(\"P95  = {} milliseconds\\n\".format(int(1000*p95_latency)))\n",
    "        f.write(\"P99  = {} milliseconds\\n\".format(int(1000*p99_latency)))        \n",
    "        f.write(\"P100 = {} milliseconds\\n\".format(int(1000*p100_latency)))\n",
    "        f.write(\"\\n\")\n",
    "        f.write(\"Accuracy:\\n\")\n",
    "        f.write(\"===\\n\")\n",
    "        if self.inference_count == 0:\n",
    "            self.inference_count = 1\n",
    "        accuracy = float(self.correct_count) / float(self.inference_count)\n",
    "        f.write(\"Accuracy = {}% \\n\".format(round(100*accuracy,2)))\n",
    "        f.write(\"\\n\")\n",
    "        f.write(\"Sanity test:\\n\")\n",
    "        f.write(\"===\\n\")\n",
    "        f.write(\"Processed - num batches {}\\n\".format(len(self.latency_array)))\n",
    "        f.write(\"          - batch size {}\\n\".format(self.batch_size)) \n",
    "        f.write(\"          - with {} workers\\n\".format(self.parallel))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BERT Runner \n",
    "This class will run parallel BERT threads on our inferentia hardware"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BertRunner():\n",
    "\n",
    "    def __init__(self, model_files, pretrained_model_dir, tsv_file, batch_size, max_length, parallel):\n",
    "        self.models = []\n",
    "\n",
    "        assert( parallel == len(model_files) )\n",
    "        print(\"BertRunner: Load {} models\".format(parallel))\n",
    "        for m in model_files:\n",
    "            model = torch.jit.load(m)\n",
    "\n",
    "            zeros = torch.zeros( [4,128], dtype=torch.long )\n",
    "            prime_inputs = ( zeros, zeros, zeros )\n",
    "\n",
    "            print(\"Priming {} ...\".format(m))\n",
    "            model(*prime_inputs)\n",
    "            print(\" ...done\")\n",
    "\n",
    "            self.models.append(model)\n",
    "\n",
    "        self.results = BertResults(parallel, batch_size)\n",
    "        self.batch_size = batch_size\n",
    "        self.max_length = max_length\n",
    "        self.parallel = parallel\n",
    "\n",
    "        self.tokenizer = BertTokenizer.from_pretrained(pretrained_model_dir)\n",
    "        self.ds = BertTestDataset( tsv_file=tsv_file, tokenizer=self.tokenizer, max_length=max_length )\n",
    "        self.dataloader = torch.utils.data.DataLoader(self.ds, batch_size=batch_size, shuffle=True, num_workers=parallel)\n",
    "\n",
    "    def invoke( self, idx ):\n",
    "        curr_ng_sizes = os.environ.get('NEURONCORE_GROUP_SIZES', None)\n",
    "        curr_ninfer = os.environ.get('NEURON_MAX_NUM_INFERS', None)\n",
    "\n",
    "        #print(\"Envs at infer time = {} {}\".format(curr_ng_sizes,curr_ninfer))\n",
    "\n",
    "        # Start timing\n",
    "        inference_count = 0\n",
    "        correct_count = 0\n",
    "        total_start = time.time()\n",
    "        all_latencies = []\n",
    "        all_end_times = []\n",
    "        for batch in self.dataloader:\n",
    "            ## Reformulate the batch into three batch tensors\n",
    "            encoded = batch['encoded']\n",
    "            inputs = torch.squeeze(encoded['input_ids'], 1)\n",
    "            attention = torch.squeeze(encoded['attention_mask'], 1)\n",
    "            token_type = torch.squeeze(encoded['token_type_ids'], 1)\n",
    "\n",
    "            if inputs.size()[0] != self.batch_size:\n",
    "                print(\"Input size = {} - padding\".format(inputs.size()))\n",
    "                remainder = self.batch_size - inputs.size()[0]\n",
    "                zeros = torch.zeros( [remainder, self.max_length], dtype=torch.long )\n",
    "                inputs = torch.cat( [inputs, zeros] )\n",
    "                attention = torch.cat( [attention, zeros] )\n",
    "                token_type = torch.cat( [token_type, zeros] )\n",
    "                inference_count += inputs.size()[0]\n",
    "            else:\n",
    "                inference_count += self.batch_size\n",
    "\n",
    "            assert(inputs.size()[0] == self.batch_size and inputs.size()[1] == self.max_length)\n",
    "            assert(attention.size()[0] == self.batch_size and attention.size()[1] == self.max_length)\n",
    "            assert(token_type.size()[0] == self.batch_size and token_type.size()[1] == self.max_length)\n",
    "\n",
    "            input_args = (inputs, attention, token_type)\n",
    "\n",
    "            start = time.time()\n",
    "            output = self.models[idx](*input_args)[0]\n",
    "            batch_predictions = [ row.argmax().item() for row in output ]\n",
    "            end = time.time()\n",
    "            all_latencies.append( end - start )\n",
    "            all_end_times.append( end )\n",
    "\n",
    "            quality_batch = batch['quality']\n",
    "\n",
    "            for a, b in zip(batch_predictions, quality_batch):\n",
    "                if int(a)==int(b):\n",
    "                    correct_count += 1\n",
    "\n",
    "        overall_time = time.time() - total_start\n",
    "\n",
    "        self.results.add_result( correct_count, inference_count, all_latencies, all_end_times, overall_time )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get pretrained models\n",
    "We'll use this function to fetch pretrained models from previous steps.  This will include:\n",
    "* Our adapted MRPC model (for the tokenizer)\n",
    "* Our previously compiled neuron PyTorch models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bucket_prefix=\"inferentia-test-\"\n",
    "\n",
    "try:\n",
    "    boto3_sess = boto3.session.Session()\n",
    "except botocore.exceptions.NoCredentialsError:\n",
    "    print(\"No credentials:  Use 'aws confgure' to setup credentials or configure isengard (Amazon internal)\")\n",
    "    raise\n",
    "\n",
    "try:\n",
    "    sts_client = boto3.client('sts')\n",
    "    response = sts_client.get_caller_identity()\n",
    "except ClientError as e:\n",
    "    print(e)\n",
    "    raise\n",
    "except:\n",
    "    raise\n",
    "\n",
    "ACCOUNT=response['Account']\n",
    "#TIMESTAMP=date.today().strftime(\"%Y-%m-%d\")\n",
    "FOLDER=\"bert_tutorial\"\n",
    "\n",
    "bucket_name=bucket_prefix + ACCOUNT\n",
    "bucket_path=FOLDER\n",
    "filename=\"bert-large-uncased-mrpc.tar.gz\"\n",
    "key = bucket_path + \"/\" + filename\n",
    "\n",
    "s3_base = \"s3://\" + bucket_name + \"/\" + bucket_path + \"/\"\n",
    "s3_location = \"s3://\" + bucket_name + \"/\" + key\n",
    "\n",
    "print(\"Using S3 base location: {}\".format(s3_location))\n",
    "mrpc_filename=\"bert-large-uncased-mrpc.tar.gz\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pretrained_bert_mrpc(WORKSPACE, s3_base, pretrained_model_name, extension=\"\"):\n",
    "    saved_model = os.path.join(WORKSPACE, pretrained_model_name)\n",
    "    print()\n",
    "    if not os.path.isdir(saved_model):\n",
    "        print(\"-- Pretrained model is not present - download from S3\")\n",
    "        os.makedirs(WORKSPACE, exist_ok=True)\n",
    "        boto3_sess = None\n",
    "\n",
    "        try:\n",
    "            boto3_sess = boto3.session.Session()\n",
    "        except botocore.exceptions.NoCredentialsError:\n",
    "            print(\n",
    "                \"No credentials:  Use 'aws confgure' to setup credentials or configure isengard (Amazon internal)\")\n",
    "            raise\n",
    "        except:\n",
    "            raise\n",
    "\n",
    "        s3 = boto3_sess.resource('s3')\n",
    "        s3_object = s3_base + pretrained_model_name + extension\n",
    "        parsed = urlparse(s3_object)\n",
    "        path = parsed.path.lstrip('/')\n",
    "        saved_model = os.path.join(WORKSPACE, os.path.basename(path))\n",
    "        s3.Bucket(parsed.netloc).download_file(path, saved_model)\n",
    "        if extension == \".tar.gz\":\n",
    "            saved_model_dir = os.path.join(WORKSPACE, pretrained_model_name)\n",
    "            shutil.unpack_archive(saved_model, WORKSPACE)\n",
    "            assert(os.path.isdir(saved_model_dir))\n",
    "            print(\"-- Pretrained model downloaded and decompressed in {}/{}\".format(\n",
    "                WORKSPACE, pretrained_model_name))\n",
    "        else:\n",
    "            print(\"-- Pretrained model downloaded om {}/{}\".format(\n",
    "                WORKSPACE, pretrained_model_name))\n",
    "    else:\n",
    "        print(\"-- Using existing model in {}/{}\".format(WORKSPACE, pretrained_model_name))\n",
    "\n",
    "    return saved_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Benchmark Code\n",
    "This code uses the tools setup before now:\n",
    "* Pull down the pretrained models\n",
    "* Set up the test runner with multiple pytorch models running on the four neuron cores on this inf1.2xlarge\n",
    "* Use a thread pool executor to run the test\n",
    "* Collate our results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def benchmark_accuracy_bert_large_mrpc_pytorch( s3_base, parallel ):\n",
    "    # Constants for this test - these have to match what we compiled!\n",
    "    batch_size = 4\n",
    "    max_length = 128\n",
    "\n",
    "    pretrained_model_name = \"bert-large-uncased-mrpc\"\n",
    "    WORKSPACE = './ws_bert_benchmark'\n",
    "    test_tsv_file = 'glue_mrpc_dev.tsv'\n",
    "\n",
    "    # Setup a workspace\n",
    "    os.makedirs(WORKSPACE, exist_ok=True)\n",
    "\n",
    "    # Get and load pretrained models\n",
    "    pretrained_model_dir = os.path.join(\n",
    "        WORKSPACE, pretrained_model_name)\n",
    "\n",
    "    if not os.path.exists( pretrained_model_dir ):\n",
    "        get_pretrained_bert_mrpc( WORKSPACE, s3_base, pretrained_model_name, extension=\".tar.gz\" )\n",
    "\n",
    "    neuron_model_names = []\n",
    "\n",
    "    for i in range(parallel):\n",
    "        neuron_file = \"bert_large_mrpc_pytorch_batch\" + str(batch_size) + '_' + str(i) + \".pt\"\n",
    "        neuron_file_path=os.path.join(\n",
    "            WORKSPACE, neuron_file)\n",
    "\n",
    "        if not os.path.exists(neuron_file_path):\n",
    "            print(\" -- Download {}\".format( neuron_file ))\n",
    "            get_pretrained_bert_mrpc( WORKSPACE, s3_base, neuron_file, extension=\"\" )\n",
    "\n",
    "        assert( os.path.exists(neuron_file_path) )\n",
    "\n",
    "        neuron_model_names.append( neuron_file_path )\n",
    "\n",
    "    # Get local evaluation data - load into a data set\n",
    "    eval_data_path = os.path.join(os.path.dirname(\"./\"), test_tsv_file)\n",
    "\n",
    "    old_ng_sizes = os.environ.get('NEURONCORE_GROUP_SIZES', None)\n",
    "    old_ninfer = os.environ.get('NEURON_MAX_NUM_INFERS', None)\n",
    "    os.environ['NEURONCORE_GROUP_SIZES'] = '1,1,1,1'\n",
    "    os.environ['NEURON_MAX_NUM_INFERS'] = '-1'\n",
    "\n",
    "    runner = BertRunner( \n",
    "        neuron_model_names, \n",
    "        pretrained_model_dir, \n",
    "        eval_data_path, \n",
    "        batch_size, \n",
    "        max_length, \n",
    "        parallel )\n",
    "\n",
    "    assert(len(neuron_model_names) == parallel)\n",
    "\n",
    "    with futures.ThreadPoolExecutor(max_workers=parallel) as executor:\n",
    "\n",
    "        running = {executor.submit(runner.invoke, idx): idx for idx in range(parallel)}\n",
    "        for future in futures.as_completed(running):\n",
    "            idx = running[future]\n",
    "            print(\"Worker {} completed\".format(idx))\n",
    "\n",
    "            # Wait on completion\n",
    "            future.result()\n",
    "\n",
    "        try:\n",
    "            runner.results.report(sys.stdout)\n",
    "            \n",
    "            with open(\"benchmark.txt\", \"w\") as f:\n",
    "                runner.results.report(f)\n",
    "        except Exception as exc:\n",
    "            print('Worker {} generated an exception: {}'.format(idx, exc))\n",
    "            \n",
    "    # Replace environment variables\n",
    "    if old_ng_sizes is None:\n",
    "        os.environ.pop('NEURONCORE_GROUP_SIZES', None)\n",
    "    else:\n",
    "        os.environ['NEURONCORE_GROUP_SIZES'] = old_ng_sizes\n",
    "    if old_ninfer is None:\n",
    "        os.environ.pop('NEURON_MAX_NUM_INFERS', None)\n",
    "    else:\n",
    "        os.environ['NEURON_MAX_NUM_INFERS'] = old_ninfer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run it!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Set in a cell above, and needed here!\n",
    "assert s3_base != None\n",
    "\n",
    "## We can configure up to 4 neuron cores on an inf1.2xlarge\n",
    "parallel = 4\n",
    "\n",
    "benchmark_accuracy_bert_large_mrpc_pytorch(s3_base, parallel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!cat benchmark.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Upload our benchmark results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def upload_and_check_file( s3_location, filename ):\n",
    "    \n",
    "    try:\n",
    "        boto3_sess = boto3.session.Session()\n",
    "    except botocore.exceptions.NoCredentialsError:\n",
    "        print(\"No credentials:  Use 'aws confgure' to setup credentials or configure isengard (Amazon internal)\")\n",
    "        raise\n",
    "\n",
    "    o = urlsplit(s3_location, allow_fragments = True)\n",
    "    mod_path = os.path.dirname( o.path )\n",
    "    mod_path = mod_path.lstrip('/')\n",
    "\n",
    "    print()\n",
    "    print(\"Copy model to: s3://\" + o.netloc + \"/\" + mod_path + \"/\")\n",
    "\n",
    "    assert( os.path.exists(filename) )\n",
    "\n",
    "    try:\n",
    "        s3_client = boto3_sess.client('s3')\n",
    "        print(\"Uploading ...\")\n",
    "        response = s3_client.upload_file(filename, o.netloc, mod_path + \"/\" + filename )\n",
    "        if response == None:\n",
    "            print(\" ... no errors\")\n",
    "        else:\n",
    "            print(\"Response: {}\".format(response))\n",
    "    except ClientError as e:\n",
    "        print(e)\n",
    "        raise\n",
    "    except:\n",
    "        raise\n",
    "\n",
    "    print()\n",
    "    print(\"Check the file uploaded OK ...\")\n",
    "    s3_resource = boto3_sess.resource('s3')\n",
    "    bucket = s3_resource.Bucket(o.netloc)\n",
    "    key = mod_path + \"/\" + filename\n",
    "    full_name = \"s3://\" + bucket.name + \"/\" + key\n",
    "\n",
    "    objs = list(bucket.objects.filter(Prefix=key))\n",
    "\n",
    "    print()\n",
    "    if len(objs) > 0 and objs[0].key == key:\n",
    "        print(\"{} exists!\".format(full_name))\n",
    "    else:\n",
    "        print(\"{} doesn't exist\".format(full_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "assert( s3_location != None )\n",
    "upload_and_check_file( s3_location, \"benchmark.txt\" )\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch_test",
   "language": "python",
   "name": "torch_test"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}