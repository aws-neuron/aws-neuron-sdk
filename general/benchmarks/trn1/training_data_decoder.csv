Model,Instance-Type,Training Data-Type,Nodes,Topology,Microbatch,Globalbatch, Optimizer, Sequence Length, Performance [seq/sec],Strong/Weak Scaling,Neuron Version,Neuron Tutorial/Example,Pytorch Neuron(torch-neuronx) Version, OS Type.
Llama-3.1-8B pre-training,trn1.32xlarge/trn1n.32xlarge,Autocast:BF16+FP32Optimizer,32,TP=32 DP=32 PP=1 ZeRO-1,1,1024,AdamW,8192,47.95,strong scaling,2.24.0,`NeuronX Distributed <https://github.com/aws-neuron/aws-neuron-sdk/blob/master/libraries/neuronx-distributed/tutorials/training_llama_tp_zero1.rst>`_,2.7.0.2.8.6896,U22
Llama-3.1-70B pre-training,trn1.32xlarge/trn1n.32xlarge,Autocast:BF16+FP32Optimizer,32,TP=32 DP=4 PP=8,1,1024,AdamW,8192,7.94,strong scaling,2.24.0,`NeuronX Distributed <https://github.com/aws-neuron/aws-neuron-sdk/blob/master/libraries/neuronx-distributed/tutorials/training_llama_tp_pp.rst>`_,2.7.0.2.8.6896,U22
