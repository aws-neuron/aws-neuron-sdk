Model,Instance-Type,Training Data-Type,Nodes,Topology,Microbatch,Global Minibatch, Optimizer, Sequence Length, Performance [seq/sec],Strong/Weak Scaling,Neuron Version,Neuron Tutorial/Example,Pytorch Neuron(torch-neuronx) Version, OS Type.
Llama-3.1-8B pre-training,trn1.32xlarge/trn1n.32xlarge,Autocast:BF16+FP32Optimizer,32,TP=32 DP=32 PP=1 ZeRO-1,1,1024,AdamW,8192,41.99,strong scaling,2.21.0,`NeuronX Distributed <https://github.com/aws-neuron/aws-neuron-sdk/blob/master/libraries/neuronx-distributed/tutorials/training_llama_tp_zero1.rst>`_,2.5.1.2.4.0,U22
Llama-3.1-70B pre-training,trn1.32xlarge/trn1n.32xlarge,Autocast:BF16+FP32Optimizer,32,TP=32 DP=4 PP=8,1,1024,AdamW,8192,7.51,strong scaling,2.22.0,`NeuronX Distributed <https://github.com/aws-neuron/aws-neuron-sdk/blob/master/libraries/neuronx-distributed/tutorials/training_llama_tp_pp.rst>`_,2.5.1.2.4.0,U22
