Model,Model Data-Type,Instance-Type,Training Data-Type,Nodes,Topology,Microbatch,Global Minibatch, Optimizer, Performance [seq/sec],Strong/Weak Scaling,Neuron Version,Neuron Tutorial/Example,Pytorch Neuron(torch-neuronx) Version, OS Type.
HuggingFace BERT-Large Ph1 pre-training,FP32,trn1.32xlarge/trn1n.32xlarge,Autocast:BF16+SR,16,[32xNC(DP)] x 16Nodes(DP),16,262144,Adam,48414,weak scaling,2.9.0,:ref:`hf-bert-pretraining-tutorial`,1.13.0.1.6.0, U20
HuggingFace BERT-Large Ph2 pre-training,FP32,trn1.32xlarge/trn1n.32xlarge,Autocast:BF16+SR,16,[32xNC(DP)] x 16Nodes(DP),2,524288,Adam,7522,weak scaling,2.9.0,:ref:`hf-bert-pretraining-tutorial`,1.13.0.1.6.0, U20
HuggingFace BERT-Large Ph1 pre-training,FP32,trn1.32xlarge/trn1n.32xlarge,Autocast:BF16+SR,16,[32xNC(DP)] x 16Nodes(DP),16,262144,Lamb,41242,strong scaling,2.9.0,:ref:`hf-bert-pretraining-tutorial`,1.13.0.1.6.0, U20
GPT3-6.7B pre-training,FP32,trn1.32xlarge/trn1n.32xlarge,Autocast:BF16+SR,16,[8xNC(TP)x4(DP)] x 16Nodes(DP),1,1024,,129,,2.9.0,:ref:`megatron-lm-pretraining-tutorial`,1.13.0.1.6.0, U20
