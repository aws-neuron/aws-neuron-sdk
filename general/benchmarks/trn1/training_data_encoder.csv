Model,Instance-Type,Training Data-Type,Nodes,Topology,Microbatch,Global Minibatch, Optimizer, Sequence Length, Performance [seq/sec],Strong/Weak Scaling,Neuron Version,Neuron Tutorial/Example,Pytorch Neuron(torch-neuronx) Version, OS Type.
HuggingFace BERT-Large Ph1 pre-training,trn1.32xlarge/trn1n.32xlarge,Autocast:BF16+SR,16,[32xNC(DP)] x 16Nodes(DP),16,1048576,Lamb,128,61765.4,weak scaling,2.20.0,:ref:`hf-bert-pretraining-tutorial`,2.1.2.2.3.0, U22
HuggingFace BERT-Large Ph2 pre-training,trn1.32xlarge/trn1n.32xlarge,Autocast:BF16+SR,16,[32xNC(DP)] x 16Nodes(DP),2,524288,Lamb,128,8396.1,weak scaling,2.20.0,:ref:`hf-bert-pretraining-tutorial`,2.1.2.2.3.0, U22
HuggingFace BERT-Large Ph1 pre-training,trn1.32xlarge/trn1n.32xlarge,Autocast:BF16/AMP,16,[32xNC(DP)] x 16Nodes(DP),16,16384,AdamW,128,25853.3,strong scaling,2.20.0,:ref:`hf-bert-pretraining-tutorial`,2.1.2.2.3.0, U22
HuggingFace BERT-Large Ph1 pre-training,trn1.32xlarge/trn1n.32xlarge,FP32,16,[32xNC(DP)] x 16Nodes(DP),8,1048576,Lamb,128,31637.3,weak scaling,2.20.0,:ref:`hf-bert-pretraining-tutorial`,2.1.2.2.3.0, U22
HuggingFace BERT-Large Ph1 pre-training,trn1.32xlarge/trn1n.32xlarge,Autocast:BF16+SR,1,[32xNC(DP)],16,16384,AdamW,128,4379.6,strong scaling,2.20.0,:ref:`hf-bert-pretraining-tutorial`,2.1.2.2.3.0, U22
HuggingFace BERT-Large Ph1 pre-training,trn1.32xlarge/trn1n.32xlarge,Autocast:BF16+SR,1,[32xNC(DP)],16,65536,Lamb,128,4538.5,strong scaling,2.20.0,:ref:`hf-bert-pretraining-tutorial`,2.1.2.2.3.0, U22
