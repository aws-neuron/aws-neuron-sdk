{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6a2a5707",
   "metadata": {},
   "source": [
    "# Generating Images with Black Forest Labs Flux.1-Dev on TRN1/TRN2\n",
    "\n",
    "This tutorial provides a step-by-step guide for generating images form the Flux.1/dev model from Black Forest Labs using NeuronX Distributed (NxD) Inference on a single Trn2.48xl instance. This sample specifically generates 1k x 1k images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46aaf0d2",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    ".. contents:: Table of contents\n",
    "    :local:\n",
    "    :depth: 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a71ce078",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "## Background, Concepts, and Optimizations\n",
    "\n",
    "### Tensor and Context Parallel\n",
    "\n",
    "For the latent transformer model, use a combination of Tensor Parallelism and Context Parallelism. Due to the compute bound nature of diffusion inference, you can add additional parallelism by using sharding on the sequence dimension. This is governed by the `world_size` relative to the `backbone_tp_degree`. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d6c47a4",
   "metadata": {},
   "source": [
    "\n",
    "## Step 1: Setup the environment\n",
    "### Set up and connect to a Trn2.48xlarge instance\n",
    "As a prerequisite, this tutorial requires that you have a Trn2 instance created from a Deep Learning AMI that has the Neuron SDK pre-installed.\n",
    "\n",
    "To set up a Trn2 instance using Deep Learning AMI with pre-installed Neuron SDK, see the NxDI setup guide.\n",
    "\n",
    "After setting up an instance, use SSH to connect to the Trn2 instance using the key pair that you chose when you launched the instance.\n",
    "\n",
    "To use Jupyter Notebook on the Neuron instance, you can use this guide.\n",
    "\n",
    "After you are connected, activate the Python virtual environment that includes the Neuron SDK.\n",
    "\n",
    "`source ~/aws_neuronx_venv_pytorch_2_7_nxd_inference/bin/activate`\n",
    "\n",
    "Run pip list to verify that the Neuron SDK is installed.\n",
    "\n",
    "`pip list | grep neuron`\n",
    "\n",
    "You should see Neuron packages including neuronx-distributed-inference and neuronx-cc.\n",
    "\n",
    "### Download the model\n",
    "\n",
    "To use this sample, you must first download the model checkpoint from HuggingFace to a local path on the Trn2 instance. For more information, see [Download models]() in the HuggingFace documentation. You can download and use [black-forest-labs/FLUX.1-dev](https://huggingface.co/black-forest-labs/FLUX.1-dev) for this tutorial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdce0741",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hidden Import Cell \n",
    "!pip install matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "744c9c85",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hidden Import Cell \n",
    "import os\n",
    "import torch\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "\n",
    "from neuronx_distributed_inference.models.diffusers.flux.application import NeuronFluxApplication\n",
    "from neuronx_distributed_inference.models.config import NeuronConfig\n",
    "from neuronx_distributed_inference.models.diffusers.flux.clip.modeling_clip import CLIPInferenceConfig\n",
    "from neuronx_distributed_inference.models.diffusers.flux.t5.modeling_t5 import T5InferenceConfig\n",
    "from neuronx_distributed_inference.models.diffusers.flux.modeling_flux import FluxBackboneInferenceConfig\n",
    "from neuronx_distributed_inference.models.diffusers.flux.vae.modeling_vae import VAEDecoderInferenceConfig\n",
    "from neuronx_distributed_inference.utils.hf_adapter import load_pretrained_config\n",
    "from neuronx_distributed_inference.utils.diffusers_adapter import load_diffusers_config\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f77e9c9",
   "metadata": {},
   "source": [
    "## Step 2: Setup Inference Parameters and Model Config\n",
    "\n",
    "Start by initializing our inference paramaters, which include model parallelism configuration, image sizes and model configuration. Ensure that that `CKPT_DIR' matches the local directory where you downloaded the model in Step 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29c0c1fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "world_size = 8\n",
    "backbone_tp_degree = 4\n",
    "dtype = torch.bfloat16\n",
    "\n",
    "height, width = [1024, 1024]\n",
    "guidance_scale = 3.5\n",
    "num_inference_steps = 25\n",
    "prompt = \"A robot named trn2\"\n",
    "\n",
    "\n",
    "# The Ckpt directory root under huggingface\n",
    "CKPT_DIR = \"/shared/flux/FLUX.1-dev/\"\n",
    "\n",
    "# Existing Compiled working directory for the compiler\n",
    "BASE_COMPILE_WORK_DIR = \"/tmp/flux/compiler_workdir/\"\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81915c20",
   "metadata": {},
   "source": [
    "## Step 3: Setup Model and Neuron Configuration\n",
    "\n",
    "Here, you initialize the various component model configuration objects for the models within the Flux Pipeline. The Flux pipeline contains CLIP, T5, the backbone transformer and the VAE. For each component model, you can use the following parallelism configuration:\n",
    "- For CLIP, `tp_degree` of 1\n",
    "- For T5, `tp_degree` is the same as the `world_size`. In the case of this example, this will be 8.\n",
    "- For the backbone transformer, if using Context Parallelism, `tp_degree` is half the world size. In the case of this example, this will be 4, which allows for 2 CP ranks.\n",
    "- Finally, for the VAE, `tp_degree` of 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fae08e51",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "text_encoder_path = os.path.join(CKPT_DIR, \"text_encoder\")\n",
    "text_encoder_2_path = os.path.join(CKPT_DIR, \"text_encoder_2\")\n",
    "backbone_path = os.path.join(CKPT_DIR, \"transformer\") \n",
    "vae_decoder_path = os.path.join(CKPT_DIR, \"vae\")\n",
    "\n",
    "clip_neuron_config = NeuronConfig(\n",
    "    tp_degree=1,\n",
    "    world_size=world_size,\n",
    "    torch_dtype=dtype,\n",
    ")\n",
    "clip_config = CLIPInferenceConfig(\n",
    "    neuron_config=clip_neuron_config,\n",
    "    load_config=load_pretrained_config(text_encoder_path),\n",
    ")\n",
    "\n",
    "t5_neuron_config = NeuronConfig(\n",
    "    tp_degree = world_size,     # T5: TP degree = world_size\n",
    "    world_size = world_size,\n",
    "    torch_dtype=dtype\n",
    ")\n",
    "t5_config = T5InferenceConfig(\n",
    "    neuron_config=t5_neuron_config,\n",
    "    load_config=load_pretrained_config(text_encoder_2_path),\n",
    ")\n",
    "\n",
    "backbone_neuron_config = NeuronConfig(\n",
    "    tp_degree = backbone_tp_degree,\n",
    "    world_size = world_size,\n",
    "    torch_type = dtype\n",
    ")\n",
    "backbone_config = FluxBackboneInferenceConfig(\n",
    "    neuron_config = backbone_neuron_config,\n",
    "    load_config = load_diffusers_config(backbone_path),\n",
    "    height = height,\n",
    "    width = width,\n",
    ")\n",
    "\n",
    "decoder_neuron_config = NeuronConfig(\n",
    "    tp_degree = 1,\n",
    "    world_size = world_size,\n",
    "    torch_type = dtype\n",
    ")\n",
    "decoder_config = VAEDecoderInferenceConfig(\n",
    "    neuron_config = decoder_neuron_config,\n",
    "    load_config = load_diffusers_config(vae_decoder_path),\n",
    "    height = height,\n",
    "    width = width,\n",
    "    transformer_in_channels = backbone_config.in_channels,\n",
    ")\n",
    "\n",
    "setattr(backbone_config, \"vae_scale_factor\", decoder_config.vae_scale_factor)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "870692df",
   "metadata": {},
   "source": [
    "## Step 4: Initialize the Flux Application and Compile\n",
    "\n",
    "Now you instantiate the `NeuronFluxApplication` which contains the pipeline orchestration logic, as well as the various component models. You then compile the application, which then compiles each component model individually."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd1f472a",
   "metadata": {},
   "outputs": [],
   "source": [
    "flux_app = NeuronFluxApplication(\n",
    "        model_path=CKPT_DIR,\n",
    "        text_encoder_config = clip_config,\n",
    "        text_encoder2_config = t5_config,\n",
    "        backbone_config = backbone_config,\n",
    "        decoder_config = decoder_config,\n",
    "        instance_type = \"trn2\",\n",
    "        height = height,\n",
    "        width = width,\n",
    "    )\n",
    "\n",
    "flux_app.compile(BASE_COMPILE_WORK_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4389cc4d",
   "metadata": {},
   "source": [
    "## Step 5: Warm-up Inferences\n",
    "The following loads the model. Loading the model loads the compiled model (NEFF), along with the model weights into device memory. In this case, calling load on the flux_app loads all the individual component models. You then perform a few iterations to \"warm-up\" the model on the device."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c271fe2",
   "metadata": {},
   "outputs": [],
   "source": [
    "flux_app.load(BASE_COMPILE_WORK_DIR)\n",
    "\n",
    "warmup_rounds = 5\n",
    "print(\"Warming up the model for better latency testing\")\n",
    "for i in range(warmup_rounds):\n",
    "    flux_app(\n",
    "        prompt,\n",
    "        height=height,\n",
    "        width=width,\n",
    "        guidance_scale=guidance_scale,\n",
    "        num_inference_steps=num_inference_steps\n",
    "    ).images[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5d3d857",
   "metadata": {},
   "source": [
    "## Step 6: Generate an Image\n",
    "\n",
    "Finally, you will generate a singular image and render it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c6e5781",
   "metadata": {},
   "outputs": [],
   "source": [
    "image = flux_app(\n",
    "    prompt,\n",
    "    height=height,\n",
    "    width=width,\n",
    "    guidance_scale=guidance_scale,\n",
    "    num_inference_steps=num_inference_steps\n",
    ").images[0]\n",
    "plt.imshow(image)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1db69b0",
   "metadata": {},
   "source": [
    "## Notes \n",
    "### Running Flux Inference on trn1\n",
    "\n",
    "This sample can also be deployed to a trn1.32xlarge with a few modifications. If you are using Context Parallelism specifically, then apply the following parallelism configuration\n",
    "\n",
    "```\n",
    "world_size = 16\n",
    "backbone_tp_degree = 8\n",
    "```\n",
    "\n",
    "Otherwise use the following:\n",
    "\n",
    "```\n",
    "world_size = 8\n",
    "backbone_tp_degree = 8\n",
    "```\n",
    "\n",
    "Additionally, you must to modify the `instance_type` argument in `NeuronFluxApplication` to `trn1`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acf419de",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
