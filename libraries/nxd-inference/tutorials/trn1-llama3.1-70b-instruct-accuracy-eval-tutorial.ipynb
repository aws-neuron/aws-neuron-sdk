{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tutorial: Evaluating Accuracy of Llama-3.1-70B on Neuron using open source datasets\n",
    "\n",
    "This tutorial provides a step-by-step guide to measure the accuracy of Llama3.1 70B on Trn1 with evaluation on two distinct tasks: mathematical reasoning and logical analysis.\n",
    "\n",
    "For this tutorial we use two datasets available in lm-eval, namely `gsm8k_cot`(high school math questions) and `mmlu_flan_n_shot_generative_logical_fallacies` (multiple choice questions on the subject) to demonstrate accuracy evaluation on Trn1. The metrics in these task are two variants of [ExactMatch](https://huggingface.co/spaces/evaluate-metric/exact_match) metrics called StrictMatch and FlexibleExtract which differ in how strict they are in extracting the final answer from the generated output from the model. To see the exact task definition used in lm-eval please look at [gsm8k-cot](https://github.com/EleutherAI/lm-evaluation-harness/blob/main/lm_eval/tasks/gsm8k/gsm8k-cot.yaml) and [mmlu template](https://github.com/EleutherAI/lm-evaluation-harness/blob/main/lm_eval/tasks/mmlu/flan_n_shot/generative/_mmlu_flan_generative_template_yaml).\n",
    "\n",
    "We also need the instruction-tuned version of llama-3.1 70b [meta-llama/Llama-3.1-70B-Instruct](https://huggingface.co/meta-llama/Llama-3.1-70B-Instruct) available hugging face."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "raw_mimetype": "text/restructuredtext",
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    ".. contents:: Table of contents\n",
    "    :local:\n",
    "    :depth: 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task Overview\n",
    "\n",
    "### 1. GSM8K with Chain-of-Thought (gsm8k_cot)\n",
    "\n",
    "The GSM8K dataset focuses on grade school math word problems, testing LLMs’ mathematical reasoning capabilities. Using Chain-of-Thought (CoT) prompting, we evaluate models’ ability to:\n",
    "\n",
    "- Solve complex math word problems\n",
    "\n",
    "- Show step-by-step reasoning\n",
    "\n",
    "- Arrive at accurate numerical answers\n",
    "\n",
    "### 2. MMLU Logical Fallacies (mmlu_flan_n_shot_generative_logical_fallacies)\n",
    "\n",
    "This evaluation focuses on the model’s ability to identify and explain logical fallacies, a subset of the MMLU benchmark. The task tests:\n",
    "\n",
    "- Understanding of common logical fallacies\n",
    "\n",
    "- Ability to analyze arguments\n",
    "\n",
    "- Explanation of reasoning flaws"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Environment Setup Guide"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prerequisites\n",
    "\n",
    "This tutorial requires that you have a Trn1 instance created from a Deep Learning AMI that has the Neuron SDK pre-installed. Also we depend on our fork of vLLM as described in the [vLLM User Guide for NxD Inference](https://awsdocs-neuron.readthedocs-hosted.com/en/latest/libraries/nxd-inference/developer_guides/vllm-user-guide.html#nxdi-vllm-user-guide).\n",
    "\n",
    "To use Jupyter Notebook on the Neuron instance, you can use this [guide](https://awsdocs-neuron.readthedocs-hosted.com/en/latest/general/setup/notebook/setup-jupyter-notebook-steps-troubleshooting.html).\n",
    "\n",
    "Before running evaluations, ensure your environment is properly configured by following these essential setup guides:\n",
    "\n",
    "1. [NxD Inference Setup Guide](https://awsdocs-neuron.readthedocs-hosted.com/en/latest/libraries/nxd-inference/nxdi-setup.html)\n",
    "\n",
    "    - Configure AWS Neuron environment\n",
    "\n",
    "    - Set up required dependencies\n",
    "\n",
    "    - Verify system requirements\n",
    "\n",
    "2. [vLLM User Guide for NxD Inference](https://awsdocs-neuron.readthedocs-hosted.com/en/latest/libraries/nxd-inference/developer_guides/vllm-user-guide.html)\n",
    "\n",
    "    - Setup vLLM according to the guide"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Installing dependencies\n",
    "\n",
    "Copy the [inference-benchmarking](https://github.com/aws-neuron/aws-neuron-samples/tree/master/inference-benchmarking/) directory to some location on your instance. Change directory to the your copy of [inference-benchmarking](https://github.com/aws-neuron/aws-neuron-samples/tree/master/inference-benchmarking/). Install other required dependencies in the same python env (e.g aws_neuron_venv_pytorch if you followed [manual install NxD Inference](https://awsdocs-neuron.readthedocs-hosted.com/en/latest/libraries/nxd-inference/nxdi-setup.html#id3) ) by:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "```python\n",
    "git clone --depth 1 https://github.com/aws-neuron/aws-neuron-samples.git\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "```python\n",
    "pip install -r requirements.txt\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download llama-3.1 70B\n",
    "To use this sample, you must first download [meta-llama/Llama-3.1-70B-Instruct](https://huggingface.co/meta-llama/Llama-3.1-70B-Instruct) model checkpoint from Hugging Face and store it locally. We are saving the model checkpoints at ``/home/ubuntu/models/Llama-3.1-70B-Instruct/`` on the Trn1 instance. For more information, see [Downloading models](https://huggingface.co/docs/hub/en/models-downloading) in the Hugging Face documentation.\n",
    "\n",
    "## Running Evaluations\n",
    "There are two methods that you can use [the evaluation scripts](https://github.com/aws-neuron/aws-neuron-samples/tree/master/inference-benchmarking/) to run your evaluation.\n",
    "\n",
    "1. Using a yaml configuration file and `accuracy.py` script\n",
    "\n",
    "2. writing your own python script that uses several components provided in `accuracy.py` and `server_config.py`\n",
    "\n",
    "We demonstrate each use case separately here.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Running eval with yaml config file\n",
    "In this method all you need is to create a yaml config file that specifies the server configuration and testing scenario you want to run. Create `config.yaml` with the following content."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile config.yaml\n",
    "server:\n",
    "  name: \"Llama-3.1-70B-Instruct\"\n",
    "  model_path: \"/home/ubuntu/model_hf/llama-3.1-70B-Instruct-hf\"\n",
    "  model_s3_path: null\n",
    "  compiled_model_path: \"/home/ubuntu/traced_model_hf/llama-3.1-70B-Instruct-hf\"\n",
    "  max_seq_len: 16384\n",
    "  context_encoding_len: 16384\n",
    "  tp_degree: 32\n",
    "  n_vllm_threads: 32\n",
    "  server_port: 8000\n",
    "  continuous_batch_size: 1\n",
    "\n",
    "test:\n",
    "  accuracy:\n",
    "    mytest:\n",
    "      client: \"lm_eval\"\n",
    "      datasets: [\"gsm8k_cot\", \"mmlu_flan_n_shot_generative_logical_fallacies\"]\n",
    "      max_concurrent_requests: 1\n",
    "      timeout: 3600\n",
    "      client_params:\n",
    "        limit: 200\n",
    "        use_chat: True\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For tasks that require higher sequence length you need to adjust `max_seq_len`. For the tasks in this tutorial 16384 would suffice.\n",
    "\n",
    "Run `python accuracy.py --config config.yaml`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "python accuracy.py --config config.yaml 2>&1 | tee accuracy_evaluation.log"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Running eval through your own python code\n",
    "You might be interested in running the evaluation in you python code. For instance if you want to change the configuration programatically or post-process the results. This is possible using 3 main components provided in `accuracy.py` and `server_config.py`.\n",
    "\n",
    "1. Server Configuration: Using ServerConfig to define the vLLM server settings\n",
    "\n",
    "2. Accuracy Scenario: Using AccuracyScenario to specify evaluation parameters\n",
    "\n",
    "3. Test Execution: Running the evaluation with the configured settings\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, import the necessary components:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from aws_neuron_eval.accuracy import AccuracyScenario, run_accuracy_test\n",
    "from aws_neuron_eval.server_config import ServerConfig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Configure the Server\n",
    "\n",
    "Set up your server configuration with ServerConfig. This example uses Llama 3.1-70b Instruct:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure the server settings\n",
    "name = \"Llama-3.1-70B-Instruct\"\n",
    "\n",
    "server_config = ServerConfig(\n",
    "    name=name,\n",
    "    model_path=f\"/home/ubuntu/model_hf/llama-3.1-70B-Instruct-hf\",  # Local model path\n",
    "    model_s3_path=None,                         # S3 model path (not used)\n",
    "    compiled_model_path=f\"/home/ubuntu/traced_model_hf/llama-3.1-70B-Instruct-hf\",  # Compiled model path\n",
    "    max_seq_len=16384,                          # Maximum sequence length\n",
    "    context_encoding_len=16384,                 # Context window size\n",
    "    tp_degree=32,                               # Tensor parallel degree for Trn1\n",
    "    n_vllm_threads=32,                          # Number of vLLM threads\n",
    "    server_port=8000,                           # Server port\n",
    "    continuous_batch_size=1,                    # Batch size for continuous batching\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Define Evaluation Scenarios\n",
    "\n",
    "Create an AccuracyScenario to specify your evaluation parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scenario = AccuracyScenario(\n",
    "    client=\"lm_eval\",              # Evaluation client\n",
    "    datasets=[                     # Target datasets\n",
    "        \"gsm8k_cot\",\n",
    "        \"mmlu_flan_n_shot_generative_logical_fallacies\",\n",
    "    ],\n",
    "    max_concurrent_requests=1,     # Maximum concurrent requests\n",
    "    timeout=5000,                  # Timeout in seconds - changed to 5000 from 3600\n",
    "    client_params={\"limit\": 200}   # Client-specific parameters\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Run the Evaluation\n",
    "\n",
    "Execute the evaluation using run_accuracy_test:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the test with a named scenario\n",
    "results_collection = run_accuracy_test(\n",
    "    server_config=server_config,\n",
    "    named_scenarios={\"mytest\": scenario}\n",
    ")\n",
    "\n",
    "# Display results\n",
    "print(results_collection)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code will execute the evaluation on the specified datasets and return detailed performance metrics. The results include accuracy scores and other relevant metrics for each dataset."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aws_neuron_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
