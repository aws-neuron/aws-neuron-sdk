{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4f580405",
   "metadata": {},
   "source": [
    "# Tutorial: Deploying Llama4 Multimodal Models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98b5edf6",
   "metadata": {},
   "source": [
    "This guide shows how to deploy Llama4 on an AWS Neuron Trainium2 (Trn2) instance using vLLM V1 with the vLLM-Neuron Plugin. This model supports both text and images. It uses Llama4 Scout (meta-llama/Llama-4-Scout-17B-16E) as the example model in this tutorial; however, Maverick (meta-llama/Llama-4-Maverick-17B-128E-Instruct) can also be used."
   ]
  },
  {
   "cell_type": "raw",
   "id": "88abcf3e",
   "metadata": {
    "raw_mimetype": "text/restructuredtext",
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    ".. contents:: Table of contents\n   :local:\n   :depth: 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "963ac8ed",
   "metadata": {},
   "source": [
    "## Examples\n",
    "\n",
    "- [Offline Example](#offline-example)\n",
    "- [Online Example](#online-example)\n",
    "- [Advanced Configuration Examples](#advanced-configuration-examples)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d0049f7",
   "metadata": {},
   "source": [
    "## Step 1: Set up your development environment\n",
    "\n",
    "As a prerequisite, this tutorial requires that you have a Trn2 instance created from a Deep Learning AMI that has the Neuron SDK pre-installed.\n",
    "\n",
    "To set up a Trn2 instance using Deep Learning AMI with pre-installed Neuron SDK, see [NxD Inference Setup Guide](https://awsdocs-neuron.readthedocs-hosted.com/en/latest/libraries/nxd-inference/nxdi-setup.html#nxdi-setup). To use a Jupyter Notebook (.ipynb) on a Neuron-enabled instance, see this [guide](https://awsdocs-neuron.readthedocs-hosted.com/en/latest/setup/notebook/setup-jupyter-notebook-steps-troubleshooting.html).\n",
    "\n",
    "After setting up an instance, use SSH to connect to the Trn2 instance using the key pair that you chose when you launched the instance.\n",
    "\n",
    "After you are connected, activate the Python virtual environment that includes the Neuron SDK."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dff142cd",
   "metadata": {},
   "source": [
    "## Step 2: Install the vLLM version that supports NxD Inference\n",
    "\n",
    "NxD Inference supports running models with vLLM. This functionality is available in the vLLM-Neuron GitHub repository. Install the latest release branch of vLLM-Neuron plugin following instructions in the [vLLM User Guide for NxD Inference](https://awsdocs-neuron.readthedocs-hosted.com/en/latest/libraries/nxd-inference/developer_guides/vllm-user-guide-v1.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01b099e8",
   "metadata": {},
   "source": [
    "## Step 3: Deploy with vLLM V1 Inference\n",
    "\n",
    "We provide two examples to run Llama4 with vLLM V1:\n",
    "\n",
    "* Offline inference: you can provide prompts in a python script and execute it.\n",
    "* Online inference: you will serve the model in an online server and send requests."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a59eb13",
   "metadata": {},
   "source": [
    "### Offline Example\n",
    "\n",
    "\n",
    "Prior to launching the vLLM server, you must trace the Llama4 model. Provide the trace model by setting the environment variable NEURON_COMPILED_ARTIFACTS."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08f20db7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from vllm import LLM, SamplingParams\n",
    "\n",
    "# Hugging Face authentication (replace with your token)\n",
    "# from huggingface_hub import login\n",
    "# login(token=\"your_hf_token_here\")\n",
    "\n",
    "# Configure Neuron environment for inference\n",
    "# Note: No need to set VLLM_NEURON_FRAMEWORK in V1 - it defaults to neuronx-distributed-inference\n",
    "os.environ['NEURON_COMPILED_ARTIFACTS'] = \"/home/ubuntu/llama4/traced_models/Llama-4-Scout-17B-16E-Instruct\"\n",
    "\n",
    "IMAGE_URL = \"https://httpbin.org/image/png\"\n",
    "\n",
    "# Initialize LLM with Neuron device configuration\n",
    "# Note: In V1, configuration is passed via additional_config\n",
    "llm = LLM(\n",
    "    model=\"meta-llama/Llama-4-Scout-17B-16E-Instruct\",  # or the file path to the downloaded checkpoint\n",
    "    max_num_seqs=1,\n",
    "    max_model_len=16384,\n",
    "    tensor_parallel_size=64,\n",
    "    limit_mm_per_prompt={\"image\": 5}, # Accepts up to 5 images per prompt\n",
    "    # V1 uses additional_config for Neuron-specific settings\n",
    "    additional_config=dict(\n",
    "        override_neuron_config=dict(\n",
    "            # Add any custom Neuron configurations here if needed\n",
    "        )\n",
    "    )\n",
    ")\n",
    "\n",
    "# Configure sampling for deterministic output\n",
    "sampling_params = SamplingParams(temperature=0.0, max_tokens=100)\n",
    "\n",
    "# Test 1: Text-only input\n",
    "conversation = [\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": [\n",
    "            {\"type\": \"text\", \"text\": \"what is the recipe of mayonnaise in two sentences?\"},\n",
    "        ]\n",
    "    }\n",
    "]\n",
    "for output in llm.chat(conversation, sampling_params):\n",
    "    print(f\"Generated text: {output.outputs[0].text !r}\")\n",
    "\n",
    "# Test 2: Single image with text\n",
    "conversation = [\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": [\n",
    "            {\"type\": \"image_url\", \"image_url\": {\"url\": IMAGE_URL}},\n",
    "            {\"type\": \"text\", \"text\": \"Describe this image\"},\n",
    "        ]\n",
    "    }\n",
    "]\n",
    "for output in llm.chat(conversation, sampling_params):\n",
    "    print(f\"Generated text: {output.outputs[0].text !r}\")\n",
    "\n",
    "# Test 3: Multiple images with text\n",
    "conversation = [\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": [\n",
    "            {\"type\": \"image_url\", \"image_url\": {\"url\": IMAGE_URL}},\n",
    "            {\"type\": \"image_url\", \"image_url\": {\"url\": IMAGE_URL}},\n",
    "            {\"type\": \"text\", \"text\": \"Compare these two images, tell me the difference.\"},\n",
    "        ]\n",
    "    }\n",
    "]\n",
    "for output in llm.chat(conversation, sampling_params):\n",
    "    print(f\"Generated text: {output.outputs[0].text !r}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "586cc351",
   "metadata": {},
   "source": [
    "Below is an example output:\n",
    "\n",
    "```bash\n",
    "Generated text: 'To make mayonnaise, combine 2 egg yolks, 1 tablespoon of lemon juice or vinegar, and a pinch of salt in a bowl, and whisk them together until smooth. Then, slowly pour in 1/2 cup of oil while continuously whisking the mixture until it thickens and emulsifies into a creamy sauce.'\n",
    "Generated text: \"The image depicts a cartoon-style illustration of a pig's face, characterized by its pink color and endearing expression. The pig features two small black eyes with white outlines, a curved smile, and two small nostrils on its snout. Two red circles adorn the cheeks, adding to the pig's rosy appearance.\\n\\n**Key Features:**\\n\\n* **Color:** Pink\\n* **Facial Expression:** Smiling\\n* **Eyes:** Small, black, with white outlines\\n* **Sn\"\n",
    "Generated text: \"The two images are identical, with no discernible differences. The only variation is a slight difference in the shade of pink used for the pig's face, but this could be due to different rendering or display settings rather than an actual difference in the images themselves.\\n\\n**Key Features:**\\n\\n* Both images feature a cartoon-style pig's head with a smiling face.\\n* The pig has two small ears, two eyes, and a curved smile.\\n* The background of both images is white.\\n\\n**Conclusion:**\\nGiven\"\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a04efba",
   "metadata": {},
   "source": [
    "### Online Example\n",
    "\n",
    "Prior to launching the Vllm server, you must trace the llama4 model, with the traced model path provided through the environment variable NEURON_COMPILED_ARTIFACTS.\n",
    "\n",
    "Open a terminal and spin up a server of the model. \n",
    "To accommodate multiple image inputs, include the optional argument --limit-mm-per-prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b31fb3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "export NEURON_COMPILED_ARTIFACTS=\"/home/ubuntu/llama4/traced_models/Llama-4-Scout-17B-16E-Instruct/\"\n",
    "export VLLM_RPC_TIMEOUT=100000\n",
    "\n",
    "# V1 uses different configuration syntax with --additional-config\n",
    "nohup vllm serve \\\n",
    "    --model \"meta-llama/Llama-4-Scout-17B-16E-Instruct\" \\\n",
    "    --max-num-seqs 1 \\\n",
    "    --max-model-len 16384 \\\n",
    "    --tensor-parallel-size 64 \\\n",
    "    --port 8000 \\\n",
    "    --disable-log-requests \\\n",
    "    --limit-mm-per-prompt image=5 \\\n",
    "    --additional-config '{\n",
    "        \"override_neuron_config\": {}\n",
    "    }' &\n",
    "\n",
    "# Wait for server to start\n",
    "sleep 10\n",
    "echo \"Server started. Check logs for startup completion.\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "server_output",
   "metadata": {},
   "source": [
    "Expected server startup output:\n",
    "\n",
    "```text\n",
    "INFO:     Started server process [25218]\n",
    "INFO:     Waiting for application startup.\n",
    "INFO:     Application startup complete.\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69768e75",
   "metadata": {},
   "source": [
    "Open another terminal and execute the following client code with python:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0494f47e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "\n",
    "MODEL = \"meta-llama/Llama-4-Scout-17B-16E-Instruct\"\n",
    "\n",
    "client = OpenAI(\n",
    "    api_key = \"EMPTY\",\n",
    "    base_url = \"http://localhost:8000/v1\"\n",
    ")\n",
    "\n",
    "print(\"== Test text input ==\")\n",
    "completion = client.chat.completions.create(\n",
    "    model=MODEL,\n",
    "    messages=[{\n",
    "        \"role\": \"user\",\n",
    "        \"content\": [\n",
    "            {\"type\": \"text\", \"text\": \"what is the recipe of mayonnaise in two sentences?\"},\n",
    "        ]\n",
    "    }]\n",
    ")\n",
    "print(completion.choices[0].message.content)\n",
    "\n",
    "\n",
    "print(\"== Test image input ==\")\n",
    "completion = client.chat.completions.create(\n",
    "    model=MODEL,\n",
    "    messages=[{\n",
    "        \"role\": \"user\",\n",
    "        \"content\": [\n",
    "            {\"type\": \"image_url\", \"image_url\": {\"url\": \"https://httpbin.org/image/png\"}},\n",
    "            {\"type\": \"text\", \"text\": \"Describe this image\"},\n",
    "        ]\n",
    "    }]\n",
    ")\n",
    "print(completion.choices[0].message.content)\n",
    "\n",
    "\n",
    "print(\"== Test multiple image inputs ==\")\n",
    "completion = client.chat.completions.create(\n",
    "    model=MODEL,\n",
    "    messages=[{\n",
    "        \"role\": \"user\",\n",
    "        \"content\": [\n",
    "            {\"type\": \"image_url\", \"image_url\": {\"url\": \"https://httpbin.org/image/png\"}},\n",
    "            {\"type\": \"image_url\", \"image_url\": {\"url\": \"https://httpbin.org/image/png\"}},\n",
    "            {\"type\": \"text\", \"text\": \"Compare these two images, tell me the difference.\"},\n",
    "        ]\n",
    "    }]\n",
    ")\n",
    "print(completion.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39d36c0c",
   "metadata": {},
   "source": [
    "Below is an example output:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d07d242d",
   "metadata": {},
   "source": [
    "```\n",
    "== Test text input ==\n",
    "To make mayonnaise, combine 2 egg yolks, 1 tablespoon of lemon juice or vinegar, and a pinch of salt in a bowl, and whisk them together until smooth. Then, slowly pour in 1/2 cup of oil while continuously whisking the mixture until it thickens and emulsifies into a creamy sauce.\n",
    "\n",
    "== Test image input ==\n",
    "The image depicts a cartoon-style illustration of a pig's face, characterized by its pink color and endearing expression. The pig features two small black eyes with white outlines, a curved smile, and two small nostrils on its snout. Two red circles adorn the cheeks, adding to the pig's rosy appearance.\n",
    "\n",
    "**Key Features:**\n",
    "\n",
    "* **Ears:** Two triangular ears are positioned at the top of the head.\n",
    "* **Facial Expression:** The pig's facial expression is cheerful, with a smile and rosy cheeks.\n",
    "* **Background:** The background of the image is transparent.\n",
    "\n",
    "Overall, the image presents a cute and friendly cartoon pig face.\n",
    "\n",
    "== Test multiple image inputs ==\n",
    "The two images are identical, featuring a cartoon pig's face with a pink color and black outline. The only difference is that the first image has a lighter shade of pink compared to the second image.\n",
    "\n",
    "**Key Features:**\n",
    "\n",
    "* Both images depict a cartoon pig's face.\n",
    "* They have the same facial features, including eyes, nose, mouth, and ears.\n",
    "* The background of both images is white.\n",
    "\n",
    "**Color Comparison:**\n",
    "\n",
    "* The first image has a lighter pink color (RGB: 255, 182, 193).\n",
    "* The second image has a slightly darker pink color (RGB: 240, 128, 128).\n",
    "\n",
    "Overall, while the two images appear similar at first glance, they differ slightly in terms of their pink hue.\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "advanced_config",
   "metadata": {},
   "source": [
    "### Advanced Configuration Examples\n",
    "\n",
    "#### Model Compilation and Configuration\n",
    "\n",
    "In `override_neuron_config`, to support multimodal architecture, you can define `text_config` and `vision_config` separately for text decoder and vision encoder.\n",
    "\n",
    "The image input can be represented in 1, 4, or 16 chunks based on its resolution and aspect ratio. Additionally, there is one chunk to describe the entire image, resulting in the total number of chunks. Due to the use of data parallelism (DP) together with tensor parallelism (TP), the vision model input batch size is padded to the next value divisible by the DP degree, which in this case is 4. The final padded batch size will be:\n",
    "\n",
    "* 1+1 = 2 → 4: Each rank has the batch size = 4/4 = 1\n",
    "* 4+1 = 5 → 8: Each rank has the batch size = 8/4 = 2\n",
    "* 16+1 = 17 → 20: Each rank has the batch size = 20/4 = 5\n",
    "\n",
    "There are a few fields you can configure to improve performance:\n",
    "\n",
    "- `cp_degree`: degree of context parallelism at the attention layer for prefill.\n",
    "- `blockwise_matmul_config`: the configuration of the blockwise MoE kernel for prefill.\n",
    "- `attn_block_tkg_nki_kernel_enabled` and `attn_block_tkg_nki_kernel_cache_update` to enable a NKI kernel for attention and a kernel KV cache update for decode operations.\n",
    "\n",
    "The `scout_neuron_config` shown below contains the recommended configuration for Llama4 Scout model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "scout_config",
   "metadata": {},
   "outputs": [],
   "source": [
    "scout_neuron_config = {\n",
    "    \"text_config\": {\n",
    "        \"batch_size\": 1,\n",
    "        \"is_continuous_batching\": True,\n",
    "        \"seq_len\": 16384,\n",
    "        \"enable_bucketing\": True,\n",
    "        \"context_encoding_buckets\": [256, 512, 1024, 2048, 4096, 8192, 10240, 16384],\n",
    "        \"token_generation_buckets\": [256, 512, 1024, 2048, 4096, 8192, 10240, 16384],\n",
    "        \"torch_dtype\": \"float16\",\n",
    "        \"async_mode\": True,\n",
    "        \"world_size\": 64,\n",
    "        \"tp_degree\": 64,\n",
    "        \"cp_degree\": 16,\n",
    "        \"cast_type\": \"as-declared\",\n",
    "        \"logical_neuron_cores\": 2,\n",
    "        \"cc_pipeline_tiling_factor\": 1,\n",
    "        \"sequence_parallel_enabled\": True,\n",
    "        \"fused_qkv\": True,\n",
    "        \"qkv_kernel_enabled\": True,\n",
    "        \"attn_kernel_enabled\": True,\n",
    "        \"attn_block_tkg_nki_kernel_enabled\": True,\n",
    "        \"attn_block_tkg_nki_kernel_cache_update\": True,\n",
    "        \"k_cache_transposed\": False,\n",
    "        \"blockwise_matmul_config\": {\n",
    "            \"block_size\": 256,\n",
    "            \"use_block_parallel\": True,\n",
    "            \"block_sharding_strategy\": \"HI_LO\",\n",
    "            \"skip_dma_token\": True,\n",
    "            \"skip_dma_weight\": True,\n",
    "            \"parallelize_token_to_block_mapping\": True\n",
    "        }\n",
    "    },\n",
    "    \"vision_config\": {\n",
    "        \"batch_size\": 1,\n",
    "        \"seq_len\": 8192,\n",
    "        \"torch_dtype\": \"float16\",\n",
    "        \"tp_degree\": 16,\n",
    "        \"cp_degree\": 1,\n",
    "        \"dp_degree\": 4,\n",
    "        \"world_size\": 64,\n",
    "        \"fused_qkv\": True,\n",
    "        \"qkv_kernel_enabled\": True,\n",
    "        \"attn_kernel_enabled\": True,\n",
    "        \"mlp_kernel_enabled\": True,\n",
    "        \"enable_bucketing\": True,\n",
    "        \"buckets\": [8, 28, 88],\n",
    "        \"logical_neuron_cores\": 2,\n",
    "        \"save_sharded_checkpoint\": True\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "custom_config_v1",
   "metadata": {},
   "source": [
    "#### Using Custom Neuron Configuration with vLLM V1\n",
    "\n",
    "When using vLLM V1, you can pass custom Neuron configurations using the `additional_config` parameter. Here's an example of how to use the advanced configuration:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "custom_v1_example",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Using custom Neuron configuration with vLLM V1\n",
    "import os\n",
    "from vllm import LLM, SamplingParams\n",
    "\n",
    "# Enable V1 mode\n",
    "os.environ['VLLM_USE_V1'] = '1'\n",
    "os.environ['NEURON_COMPILED_ARTIFACTS'] = \"/home/ubuntu/llama4/traced_models/Llama-4-Scout-17B-16E-Instruct\"\n",
    "\n",
    "# Initialize LLM with custom Neuron configuration\n",
    "llm = LLM(\n",
    "    model=\"meta-llama/Llama-4-Scout-17B-16E-Instruct\",\n",
    "    max_num_seqs=1,\n",
    "    max_model_len=16384,\n",
    "    tensor_parallel_size=64,\n",
    "    limit_mm_per_prompt={\"image\": 5},\n",
    "    # V1 syntax: use additional_config with override_neuron_config\n",
    "    additional_config=dict(\n",
    "        override_neuron_config=scout_neuron_config  # Use the configuration defined above\n",
    "    )\n",
    ")\n",
    "\n",
    "# The rest of your inference code remains the same\n",
    "sampling_params = SamplingParams(temperature=0.0, max_tokens=100)\n",
    "# ... inference code ..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "server_custom_config",
   "metadata": {},
   "source": [
    "#### Server Configuration with Custom Neuron Config\n",
    "\n",
    "For online inference with custom configuration, you can pass the Neuron config via the `--additional-config` flag:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "server_custom_example",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "# Example server startup with custom Neuron configuration\n",
    "export VLLM_USE_V1=1\n",
    "export NEURON_COMPILED_ARTIFACTS=\"/home/ubuntu/llama4/traced_models/Llama-4-Scout-17B-16E-Instruct/\"\n",
    "export VLLM_RPC_TIMEOUT=100000\n",
    "\n",
    "# Start server with custom Neuron configuration\n",
    "vllm serve \\\n",
    "    --model \"meta-llama/Llama-4-Scout-17B-16E-Instruct\" \\\n",
    "    --max-num-seqs 1 \\\n",
    "    --max-model-len 16384 \\\n",
    "    --tensor-parallel-size 64 \\\n",
    "    --port 8000 \\\n",
    "    --disable-log-requests \\\n",
    "    --limit-mm-per-prompt image=5 \\\n",
    "    --additional-config '{\n",
    "        \"override_neuron_config\": {\n",
    "            \"text_config\": {\n",
    "                \"batch_size\": 1,\n",
    "                \"is_continuous_batching\": true,\n",
    "                \"seq_len\": 16384,\n",
    "                \"enable_bucketing\": true,\n",
    "                \"context_encoding_buckets\": [256, 512, 1024, 2048, 4096, 8192, 10240, 16384],\n",
    "                \"token_generation_buckets\": [256, 512, 1024, 2048, 4096, 8192, 10240, 16384],\n",
    "                \"torch_dtype\": \"float16\",\n",
    "                \"async_mode\": true,\n",
    "                \"world_size\": 64,\n",
    "                \"tp_degree\": 64,\n",
    "                \"cp_degree\": 16\n",
    "            },\n",
    "            \"vision_config\": {\n",
    "                \"batch_size\": 1,\n",
    "                \"seq_len\": 8192,\n",
    "                \"torch_dtype\": \"float16\",\n",
    "                \"tp_degree\": 16,\n",
    "                \"cp_degree\": 1,\n",
    "                \"dp_degree\": 4,\n",
    "                \"world_size\": 64\n",
    "            }\n",
    "        }\n",
    "    }'"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
