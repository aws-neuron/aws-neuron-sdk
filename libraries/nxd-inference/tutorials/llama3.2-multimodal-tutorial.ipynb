{
    "cells": [
        {
            "cell_type": "markdown",
            "id": "b6038b63-c8c7-4304-addf-35f9e6f7ee7c",
            "metadata": {},
            "source": [
                "# Tutorial: Deploying Llama3.2 Multimodal Models"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "72f99b30-0eea-4c18-ba44-edea7f63b4ba",
            "metadata": {},
            "source": [
                "NeuronX Distributed Inference (NxDI) enables you to deploy ```Llama-3.2-11B-Vision-Instruct``` and ```Llama-3.2-90B-Vision-Instruct``` models on Neuron Trainium and Inferentia instances.\n",
                "\n",
                "You can run Llama3.2 Multimodal with default configuration options. NxD Inference also provides several features and configuration options that you can use to optimize and tune the performance for inference. This guide walks through how to run Llama3.2 Multimodal with vLLM, and how to enable optimizations for inference performance on Trn1/Inf2 instances. It takes about 20-60 minutes to complete."
            ]
        },
        {
            "cell_type": "raw",
            "metadata": {
                "raw_mimetype": "text/restructuredtext",
                "vscode": {
                    "languageId": "raw"
                }
            },
            "source": [
                ".. contents:: Table of contents\n",
                "    :local:\n",
                "    :depth: 2"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "e3558701-f250-4db4-a871-bd7e1fccee93",
            "metadata": {},
            "source": [
                "## Step 1: Set up Development Environment\n",
                "1. Launch a ```trn1.32xlarge``` or ```inf2.48xlarge``` instance on Ubuntu 22 with Neuron Multi-Framework DLAMI. Please refer to the [setup guide](https://awsdocs-neuron.readthedocs-hosted.com/en/latest/setup/neuron-setup/multiframework/multi-framework-ubuntu22-neuron-dlami.html#setup-ubuntu22-multi-framework-dlami) if you don’t have one yet. If you are looking to install NxD Inference library without using pre-existing DLAMI, please refer to the [NxD Inference Setup Guide](https://awsdocs-neuron.readthedocs-hosted.com/en/latest/libraries/nxd-inference/nxdi-setup.html#nxdi-setup). To use Jupyter Notebook on the Neuron instance, you can use this [guide](https://awsdocs-neuron.readthedocs-hosted.com/en/latest/setup/notebook/setup-jupyter-notebook-steps-troubleshooting.html).\n",
                "\n",
                "\n",
                "2. Use default virtual environment pre-installed with the Neuron SDK.\n",
                "\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "raw_mimetype": "text/restructuredtext",
                "vscode": {
                    "languageId": "raw"
                }
            },
            "source": [
                "```python\n",
                " source /opt/aws_neuronx_venv_pytorch_2_6_nxd_inference/bin/activate\n",
                "```"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "25677fb1",
            "metadata": {},
            "source": [
                "3. Install the latest release branch of vLLM from the AWS Neuron fork following the [vLLM User Guide for NxD Inference](https://awsdocs-neuron.readthedocs-hosted.com/en/latest/libraries/nxd-inference/developer_guides/vllm-user-guide.html#nxdi-vllm-user-guide).\n",
                "\n",
                "4. You should now have the Neuron SDK and other necessary packages installed, including ```neuronx-distributed-inference```, ```neuronx-cc```, ```torch```, ```torchvision```, and ```vllm-neuronx```."
            ]
        },
        {
            "cell_type": "markdown",
            "id": "fca00b06",
            "metadata": {},
            "source": [
                "## Step 2: Download and Convert Checkpoints\n",
                "Download Llama3.2 Multimodal models from either [Meta’s official website](https://www.llama.com/llama-downloads/) or HuggingFace(HF) ([11B](https://huggingface.co/meta-llama/Llama-3.2-11B-Vision-Instruct), [90B](https://huggingface.co/meta-llama/Llama-3.2-90B-Vision-Instruct)).\n",
                "\n",
                "NxDI supports HF checkpoint out-of-the-box. To use the Meta checkpoint, you need to run the following script to convert the downloaded Meta checkpoint into NxDI supported format."
            ]
        },
        {
            "cell_type": "markdown",
            "id": "7ac1e2f0",
            "metadata": {
                "raw_mimetype": "text/restructuredtext",
                "vscode": {
                    "languageId": "raw"
                }
            },
            "source": [
                "```python\n",
                "python -m neuronx_distributed_inference.models.mllama_convert_mllama_weights_to_neuron \\\n",
                "    --input-dir <path_to_meta_pytorch_checkpoint> \\\n",
                "    --output-dir <path_to_neuron_checkpoint> \\\n",
                "    --instruct \\\n",
                "    --num-shards 8 #(1 for 11B and 8 for 90B)\n",
                "```"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "1282d183",
            "metadata": {},
            "source": [
                "After the script is finished running, you should have the following configuration and checkpoint files. Verify by ```ls <path_to_neuron_checkpoint>```:"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "7ac1e2f0",
            "metadata": {
                "raw_mimetype": "text/restructuredtext",
                "vscode": {
                    "languageId": "raw"
                }
            },
            "source": [
                "```python\n",
                "chat_template.json      model.safetensors          tokenizer_config.json\n",
                "config.json             special_tokens_map.json\n",
                "generation_config.json  tokenizer.json\n",
                "```"
            ]
        },
        {
            "cell_type": "raw",
            "id": "d3ca62ff",
            "metadata": {
                "raw_mimetype": "text/restructuredtext",
                "vscode": {
                    "languageId": "raw"
                }
            },
            "source": [
                ".. note::\n",
                "    The following code examples use HF checkpoint, as it is supported by default, no script needs to be run."
            ]
        },
        {
            "cell_type": "markdown",
            "id": "ca26e113-770a-46e1-b3ae-e69249087710",
            "metadata": {},
            "source": [
                "## Step 3: Deploy with vLLM Inference\n",
                "\n",
                "We provide two examples to deploy Llama3.2 Multimodal with vLLM:\n",
                "\n",
                "1. Offline inference: you can provide prompts in a python script and execute it.\n",
                "\n",
                "2. Online inference: you will serve the model in an online server and send requests.\n",
                "\n",
                "If you already have a compiled model artifact in `MODEL_PATH` with the same specified configuration, or if you have set an environment variable `NEURON_COMPILED_ARTIFACTS`, the vLLM engine will load the compiled model and run inference directly. Otherwise, it will automatically compile and save a new model artifact. See [vLLM User Guide for NxD Inference](https://awsdocs-neuron.readthedocs-hosted.com/en/latest/libraries/nxd-inference/developer_guides/vllm-user-guide.html#nxdi-vllm-user-guide) for more. We provide example configurations here, continue reading on how to tune them per your use case."
            ]
        },
        {
            "cell_type": "markdown",
            "id": "ba5bd268-b2ef-44f9-aa26-b5693c41891f",
            "metadata": {},
            "source": [
                "### Configurations\n",
                "\n",
                "You should specifically tune these configurations when optimizing performance for Llama3.2 Multimodal models. Please refer to [NxD Inference Features Configuration Guide](https://awsdocs-neuron.readthedocs-hosted.com/en/latest/libraries/nxd-inference/developer_guides/feature-guide.html#nxdi-feature-guide) for detailed explanation of each configuration.\n",
                "\n",
                "- `MODEL_PATH` - The directory containing NxDI-supported configs, checkpoints, and neuron compiled artifacts.\n",
                "\n",
                "- `BATCH_SIZE` - Batch size and sequence length together are bounded by device memory. For sequence shorter than 16k, we support up to batch size 4. For longer sequence, we support batch size 1.\n",
                "\n",
                "- `SEQ_LEN` - The entire sequence length combining input and output sequence. We support sequence length up to 128k for 11B model, and 16k for 90B model.\n",
                "\n",
                "- `TENSOR_PARALLEL_SIZE` - For best performance, choose the maximum supported value by your instance, that is divisible by the model’s hidden sizes and number of attention heads: 32 for `trn1.32xlarge` and 16 for `inf2.48xlarge`.\n",
                "\n",
                "- `CONTEXT_ENCODING_BUCKETS` - Set based on your distribution of input/context length. For example, suppose 90% of the input traffic is shorter than 1k sequence, and all are less than 2k, then we should set the context encoding buckets to be `[1024, 2048]`.\n",
                "\n",
                "- `TOKEN_GENERATION_BUCKETS` - Set based on your distribution of entire sequence length. Use similar principle as above."
            ]
        },
        {
            "cell_type": "raw",
            "id": "d3ca62ff",
            "metadata": {
                "raw_mimetype": "text/restructuredtext",
                "vscode": {
                    "languageId": "raw"
                }
            },
            "source": [
                ".. note::\n",
                "    Longer sequence takes up more memory, so we should use less buckets. For example, to compile the 90B model on ``trn1.32xlarge`` with ``SEQ_LEN=16384, BATCH_SIZE=4``, we can use buckets ``[1024, 2048, 16384]`` to cover the longest possible sequence as well as shorter sequence where the majority of traffic comes from. We also set an environment variable by ``export NEURON_SCRATCHPAD_PAGE_SIZE=1024`` to increase the scratchpad size in our direct memory access engine to fit the large tensors."
            ]
        },
        {
            "cell_type": "markdown",
            "id": "ddadea5d",
            "metadata": {},
            "source": [
                "- `SEQUENCE_PARALLEL_ENABLED` - Set to ```True``` to enable sequence parallel. In principle, sequence parallel helps scaling to long sequence length by splitting tensors along the sequence dimension. However, for short sequence length less than 2k, it is not worth to pay for the collectives overhead when compute workload is manageable. So in this example, as we configured sequence length to be no more than 2k, we disabled the sequence parallel.\n",
                "\n",
                "- `IS_CONTINUOUS_BATCHING` - Set based on your input traffic. For example, suppose end-to-end latency to generate an entire output sequence (batch size 1) is 1 second in average. However, you receive a request every 0.5 second. Then it is beneficial to enable continuous batching so that new request can get generation started before prior request is finished.\n",
                "\n",
                "- `ON_DEVICE_SAMPLING_CONFIG` - We enable on-device sampling to perform sampling logic on the Neuron device (rather than on the CPU) to achieve better performance."
            ]
        },
        {
            "cell_type": "markdown",
            "id": "d56f6884-5712-454d-8cf8-662194d85c39",
            "metadata": {},
            "source": [
                "### Model Inputs\n",
                "\n",
                "- `PROMPTS: List[str]` - Batch of text prompts.\n",
                "\n",
                "- `IMAGES: List[Union[PIL.Image.Image, torch.Tensor]]` - Batch of image prompts. We currently support one image per prompt as recommended by [Meta](https://github.com/meta-llama/llama-models/blob/main/models/llama3_2/vision_prompt_format.md#notes-1). If the prompt has no image, use an empty tensor.\n",
                "\n",
                "- `SAMPLING_PARAMS: List[Dict]` - Batch of sampling parameters. With dynamic sampling, you can pass different ```top_k```, ```top_p```, and ```temperature``` values for each input in a batch."
            ]
        },
        {
            "cell_type": "markdown",
            "id": "b524b3d6-8ed1-42e4-b960-92b4a2c2bbdd",
            "metadata": {},
            "source": [
                "### Offline Example"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "084d86a3",
            "metadata": {},
            "outputs": [],
            "source": [
                "# import required modules\n",
                "import requests\n",
                "import torch\n",
                "import os\n",
                "from neuronx_distributed_inference.models.mllama.utils import add_instruct\n",
                "from PIL import Image\n",
                "from vllm import LLM, SamplingParams, TextPrompt"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "c58ac943",
            "metadata": {},
            "outputs": [],
            "source": [
                "# define a helper function to read images\n",
                "def get_image(image_url):\n",
                "    image = Image.open(requests.get(image_url, stream=True).raw)\n",
                "    return image\n",
                "    "
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "857d41f2",
            "metadata": {},
            "outputs": [],
            "source": [
                "MODEL_PATH=\"/home/ubuntu/model_hf/Llama-3.2-90B-Vision-Instruct-hf\""
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "4507b506",
            "metadata": {},
            "outputs": [],
            "source": [
                "# define a helper function to prepare mllama model inputs\n",
                "def get_VLLM_mllama_model_inputs(prompt, single_image, sampling_params):\n",
                "    # Prepare all inputs for mllama generation, including:\n",
                "    # 1. put text prompt into instruct chat template\n",
                "    # 2. compose single text and single image prompt into Vllm's prompt class\n",
                "    # 3. prepare sampling parameters\n",
                "    input_image = single_image\n",
                "    has_image = torch.tensor([1])\n",
                "    if isinstance(single_image, torch.Tensor) and single_image.numel() == 0:\n",
                "        has_image = torch.tensor([0])\n",
                "\n",
                "    instruct_prompt = add_instruct(prompt, has_image)\n",
                "    inputs = TextPrompt(prompt=instruct_prompt)\n",
                "\n",
                "    if input_image is not None:\n",
                "        inputs[\"multi_modal_data\"] = {\"image\": input_image}\n",
                "\n",
                "    sampling_params = SamplingParams(**sampling_params)\n",
                "    return inputs, sampling_params"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 2,
            "id": "caea6575",
            "metadata": {},
            "outputs": [],
            "source": [
                "# define helper function to print model outputs\n",
                "def print_outputs(outputs):\n",
                "    # Print the outputs.\n",
                "    for output in outputs:\n",
                "        prompt = output.prompt\n",
                "        generated_text = output.outputs[0].text\n",
                "        print(f\"Prompt: {prompt!r}, Generated text: {generated_text!r}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "8ea0d6cf",
            "metadata": {},
            "outputs": [],
            "source": [
                "# Model Inputs\n",
                "PROMPTS = [\n",
                "    \"What is in this image? Tell me a story\",\n",
                "    \"What is the recipe of mayonnaise in two sentences?\",\n",
                "    \"How many animanls do you see here?\",\n",
                "    \"What is the capital of Italy famous for?\",\n",
                "]\n",
                "IMAGES = [\n",
                "    get_image(\n",
                "        \"https://images.pexels.com/photos/1108099/pexels-photo-1108099.jpeg?auto=compress&cs=tinysrgb&dpr=1&w=500\"\n",
                "    ),\n",
                "    None,\n",
                "    get_image(\n",
                "        \"https://images.pexels.com/photos/1108099/pexels-photo-1108099.jpeg?auto=compress&cs=tinysrgb&dpr=1&w=500\"\n",
                "    ),\n",
                "    None,\n",
                "]\n",
                "SAMPLING_PARAMS = [\n",
                "    dict(top_k=1, temperature=1.0, top_p=1.0, max_tokens=50)\n",
                "    for _ in range(len(PROMPTS))\n",
                "]\n",
                "print(PROMPTS)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "2d712593",
            "metadata": {},
            "outputs": [],
            "source": [
                "# Main model loading and generation\n",
                "\n",
                "assert len(PROMPTS) == len(IMAGES) == len(SAMPLING_PARAMS), \\\n",
                "    f\"\"\"Text, image prompts and sampling parameters should have the \n",
                "        same batch size; but got {len(PROMPTS)}, {len(IMAGES)}, \n",
                "        and {len(SAMPLING_PARAMS)}\"\"\"\n",
                "\n",
                "# Create an LLM.\n",
                "llm = LLM(model=MODEL_PATH,\n",
                "            max_num_seqs=1,\n",
                "            max_model_len=4096,\n",
                "            block_size=4096,\n",
                "            device=\"neuron\",\n",
                "            tensor_parallel_size=32,\n",
                "            override_neuron_config={\n",
                "                \"sequence_parallel_enabled\": False,\n",
                "                \"skip_warmup\": True,\n",
                "                \"on_device_sampling_config\": {\n",
                "                    \"global_topk\": 1,\n",
                "                    \"dynamic\": False,\n",
                "                    \"deterministic\": False\n",
                "                },\n",
                "            })\n",
                "\n",
                "batched_inputs = []\n",
                "batched_sample_params = []\n",
                "for pmpt, img, params in zip(PROMPTS, IMAGES, SAMPLING_PARAMS):\n",
                "    inputs, sampling_params = get_VLLM_mllama_model_inputs(\n",
                "        pmpt, img, params)\n",
                "    # test batch-size = 1\n",
                "    outputs = llm.generate(inputs, sampling_params)\n",
                "    print_outputs(outputs)\n",
                "    batched_inputs.append(inputs)\n",
                "    batched_sample_params.append(sampling_params)\n",
                "# test batch-size = 4\n",
                "    outputs = llm.generate(batched_inputs, batched_sample_params)\n",
                "    print_outputs(outputs)"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "cdd610f1",
            "metadata": {},
            "source": [
                "This script will print the outputs. Below is an example output from image-text prompt:\n"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "85d5511b",
            "metadata": {
                "raw_mimetype": "text/restructuredtext",
                "vscode": {
                    "languageId": "raw"
                }
            },
            "source": [
                "```json\n",
                "Prompt: '<|begin_of_text|><|start_header_id|>user<|end_header_id|>\\n\\n<|image|>What is\n",
                "in this image? Tell me a story<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\n',\n",
                "Generated text: 'The image shows a dog riding a skateboard. The dog is standing on the\n",
                "skateboard, which is in the middle of the road. The dog is looking at the camera with its\n",
                "mouth open, as if it is smiling. The dog has floppy ears and a long tail. It is wearing a\n",
                "collar around its neck. The skateboard is black with red wheels. The background is blurry,\n",
                "but it appears to be a city street with buildings and cars in the distance.'\n",
                "```"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "12536d26",
            "metadata": {},
            "source": [
                "### Online Example\n",
                "\n",
                "First, open a terminal and spin up a server of the model. If you specify a new set of configurations, a new neuron model artifact will be compiled now."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "c0ac48f7-7936-47f3-aa99-4902d1986f4c",
            "metadata": {},
            "outputs": [],
            "source": [
                "%%bash\n",
                "\n",
                "MODEL_PATH=\"/home/ubuntu/model_90B/\"\n",
                "nohup python3 -m vllm.entrypoints.openai.api_server \\\n",
                "    --model $MODEL_PATH \\\n",
                "    --tensor-parallel-size 32 \\\n",
                "    --max-model-len 2048 \\\n",
                "    --max-num-seqs 4 \\\n",
                "    --device neuron \\\n",
                "    --override-neuron-config '{\n",
                "        \"context_encoding_buckets\": [1024, 2048],\n",
                "        \"token_generation_buckets\": [1024, 2048],\n",
                "        \"sequence_parallel_enabled\": false,\n",
                "        \"is_continuous_batching\": true,\n",
                "        \"on_device_sampling_config\": {\n",
                "            \"global_topk\": 64,\n",
                "            \"dynamic\": true,\n",
                "            \"deterministic\": false\n",
                "        }\n",
                "    }'\n"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "e2b7a740",
            "metadata": {},
            "source": [
                "If you see the below logs, that means your server is up and running:"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "85d5511b",
            "metadata": {
                "raw_mimetype": "text/restructuredtext",
                "vscode": {
                    "languageId": "raw"
                }
            },
            "source": [
                "```python\n",
                "INFO:     Started server process [221607]\n",
                "INFO:     Waiting for application startup.\n",
                "INFO:     Application startup complete.\n",
                "INFO:     Uvicorn running on http://0.0.0.0:8000 (Press CTRL+C to quit)\n",
                "```\n"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "ff790927",
            "metadata": {},
            "source": [
                "Then open a new terminal as the client where you can send requests to the server. We’ve enabled continuous batching by default, so you can open up to ```--max-num-seqs``` client terminals to send requests. To send a text-only request:"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "b546dc0a-72ce-4abb-950e-4820327d43a0",
            "metadata": {},
            "outputs": [],
            "source": [
                "%%bash\n",
                "\n",
                "MODEL_PATH=\"/home/ubuntu/model_hf/Llama-3.2-90B-Vision-Instruct-hf\"\n",
                "curl http://localhost:8000/v1/chat/completions \\\n",
                "-H \"Content-Type: application/json\" \\\n",
                "-d '{\n",
                "        \"model\": \"'\"$MODEL_PATH\"'\",\n",
                "        \"messages\": [\n",
                "                {\n",
                "                \"role\": \"user\",\n",
                "                \"content\": \"What is the capital of Italy?\"\n",
                "                }\n",
                "        ]\n",
                "        }'"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "32e3625f",
            "metadata": {},
            "source": [
                "You should receive outputs shown in the client terminal shortly:\n"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "85d5511b",
            "metadata": {
                "raw_mimetype": "text/restructuredtext",
                "vscode": {
                    "languageId": "raw"
                }
            },
            "source": [
                "```json\n",
                "{\"id\":\"chat-2df3e876738b470ab27b090e0a09736e\",\"object\":\"chat.completion\",\n",
                "\"created\":1734401826,\"model\":\"/home/ubuntu/model_hf/Llama-3.2-90B-Vision-Instruct-hf/\",\n",
                "\"choices\":[{\"index\":0,\"message\":{\"role\":\"assistant\",\"content\":\"The capital of Italy is\n",
                "Rome.\",\"tool_calls\":[]},\"logprobs\":null,\"finish_reason\":\"stop\",\"stop_reason\":null}],\n",
                "\"usage\":{\"prompt_tokens\":42,\"total_tokens\":50,\"completion_tokens\":8},\"prompt_logprobs\":null}\n",
                "```"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "272ef678",
            "metadata": {},
            "source": [
                "If the request fails, increase the value of the ```VLLM_RPC_TIMEOU``` environment variable using ```export VLLM_RPC_TIMEOUT=180000```, then restart the server. The timeout value depends on the model and deployment configuration used.\n",
                "\n",
                "To send a request with both text and image prompts:\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "e37c5857",
            "metadata": {},
            "outputs": [],
            "source": [
                "%%bash\n",
                "MODEL_PATH=\"/home/ubuntu/model_hf/Llama-3.2-90B-Vision-Instruct-hf\"\n",
                "\n",
                "curl http://localhost:8000/v1/chat/completions \\\n",
                "-H \"Content-Type: application/json\" \\\n",
                "-d '{\n",
                "    \"model\": \"'\"$MODEL_PATH\"'\",\n",
                "    \"messages\": [\n",
                "        {\n",
                "        \"role\": \"user\",\n",
                "        \"content\": [\n",
                "            {\n",
                "            \"type\": \"text\",\n",
                "            \"text\": \"Describe this image\"\n",
                "            },\n",
                "            {\n",
                "            \"type\": \"image_url\",\n",
                "            \"image_url\": {\n",
                "                \"url\": \"https://awsdocs-neuron.readthedocs-hosted.com/en/latest/_images/nxd-inference-block-diagram.jpg\"\n",
                "            }\n",
                "            }\n",
                "        ]\n",
                "        }\n",
                "    ]\n",
                "    }'"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "0a811af4",
            "metadata": {},
            "source": [
                "You can expect results appear in the client terminal shortly:"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "85d5511b",
            "metadata": {
                "raw_mimetype": "text/restructuredtext",
                "vscode": {
                    "languageId": "raw"
                }
            },
            "source": [
                "```json\n",
                "{\"id\":\"chat-fd1319865bd44d6aa60a4739cce61c9d\",\"object\":\"chat.completion\",\n",
                "\"created\":1734401984,\"model\":\"/home/ubuntu/model_hf/Llama-3.2-90B-Vision-Instruct-hf/\",\n",
                "\"choices\":[{\"index\":0,\"message\":{\"role\":\"assistant\",\"content\":\"The image presents a\n",
                "diagram illustrating the components of NxD Inference, with a focus on inference modules\n",
                "and additional modules. The diagram is divided into two main sections: \\\"Inference\n",
                "Modules\\\" and \\\"Additional Modules.\\\" \\n\\n**Inference Modules:**\\n\\n*   Attention\n",
                "Techniques\\n*   KV Caching\\n*   Continuous Batching\\n\\n**Additional Modules:**\\n\\n*\n",
                "Speculative Decoding (Draft model and Draft heads (Medusa / Eagle))\\n\\nThe diagram also\n",
                "includes a section titled \\\"NxD Core (Distributed Strategies, Distributed Model Tracing)\\\"\n",
                "and a logo for PyTorch at the bottom.\",\"tool_calls\":[]},\"logprobs\":null,\n",
                "\"finish_reason\":\"stop\",\"stop_reason\":null}],\"usage\":{\"prompt_tokens\":14,\"total_tokens\":137,\n",
                "\"completion_tokens\":123},\"prompt_logprobs\":null}\n",
                "```"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": ".venv",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.9.6"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 5
}
