{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tutorial: Using Prefix Caching with Llama-3.3-70B on Trn2 instances\n",
    "\n",
    "This tutorial provides a step-by-step guide to deploy Llama3.3 70B using \n",
    "NeuronX Distributed (NxD) Inference on a single Trn2.48xl instance using two\n",
    "different configurations, one with prefix caching enabled and the other\n",
    "without prefix caching. We will also measure average response time\n",
    "for both the configurations with prompts containing a common prefix."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "raw_mimetype": "text/restructuredtext",
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    ".. contents:: Table of contents\n",
    "    :local:\n",
    "    :depth: 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Background, Concepts, and Optimizations\n",
    "\n",
    "### Block KV Cache Layout\n",
    "\n",
    "To support prefix caching, NxDI now uses block kv cache layout. Enable block layout of\n",
    "the cache by setting `is_block_kv_layout=True` in NeuronConfig. The first two\n",
    "dimensions of the KV cache are set to the number of blocks and block size, respectively.\n",
    "These configurations are specified using `pa_num_blocks` and `pa_block_size` in NeuronConfig.\n",
    "\n",
    "For optimal performance with Neuron, it's recommended to set `pa_block_size=32`.\n",
    "The minimum required `pa_num_blocks` can be calculated using the formula\n",
    "`(batch_size * max_seq_len) / block_size` where batch_size is the compiled batch size\n",
    "and max_seq_len is the maximum sequence length of the compiled model on Neuron.\n",
    "While using the minimum block calculation will produce accurate results, it's recommended\n",
    "to initialize as many blocks as possible without exceeding HBM space limitations. This\n",
    "ensures that Neuron has sufficient blocks to save as much prefix data as possible. More cache\n",
    "blocks implies higher prefix caching hit rate and hence better context encoding performance.\n",
    "\n",
    "### Kernels\n",
    "\n",
    "NxD Inference supports kernels that optimize parts of the modeling code\n",
    "for best performance when prefix caching is enabled.\n",
    "\n",
    "- Token generation attention kernel with block kv cache read and update capabilities.\n",
    "  This kernel reads the cache blocks using the active block table, converts the required\n",
    "  blocks into flat layout, performs attention and scatters back the computed key and value\n",
    "  to the correct slot in the block cache layout. To enable this kernel, set\n",
    "  `attn_block_tkg_nki_kernel_enabled=True` and `attn_block_tkg_nki_kernel_cache_update=True`\n",
    "  in NeuronConfig."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prerequisites\n",
    "\n",
    "### Set up and connect to a Trn2.48xlarge instance\n",
    "\n",
    "As a prerequisite, this tutorial requires that you have a Trn2 instance\n",
    "created from a Deep Learning AMI that has the Neuron SDK pre-installed.\n",
    "\n",
    "To set up a Trn2 instance using Deep Learning AMI with pre-installed Neuron SDK,\n",
    "see the [NxDI setup guide](https://awsdocs-neuron.readthedocs-hosted.com/en/latest/libraries/nxd-inference/nxdi-setup.html#nxdi-setup).\n",
    "\n",
    "After setting up an instance, use SSH to connect to the Trn2 instance using the key pair that you\n",
    "chose when you launched the instance.\n",
    "\n",
    "To use Jupyter Notebook on the Neuron instance, you can use this [guide](https://awsdocs-neuron.readthedocs-hosted.com/en/latest/general/setup/notebook/setup-jupyter-notebook-steps-troubleshooting.html).\n",
    "\n",
    "After you are connected, activate the Python virtual environment that\n",
    "includes the Neuron SDK."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "```python\n",
    "source ~/aws_neuronx_venv_pytorch_2_7_nxd_inference/bin/activate\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run `pip list` to verify that the Neuron SDK is installed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "```python\n",
    "pip list | grep neuron\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should see Neuron packages including\n",
    "`neuronx-distributed-inference` and `neuronx-cc`.\n",
    "\n",
    "### Install packages\n",
    "\n",
    "NxD Inference supports running models with vLLM. This functionality is\n",
    "available in a fork of the vLLM GitHub repository:\n",
    "\n",
    "- [aws-neuron/upstreaming-to-vllm](https://github.com/aws-neuron/upstreaming-to-vllm/tree/neuron-2.24-vllm-v0.7.2)\n",
    "\n",
    "To run NxD Inference with vLLM, you need to download and install vLLM from this\n",
    "fork. Clone the Neuron vLLM fork."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!git clone -b neuron-2.24-vllm-v0.7.2 https://github.com/aws-neuron/upstreaming-to-vllm.git"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make sure to activate the Neuron virtual environment if using a new terminal instead of the one from connect step above."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "```python\n",
    "source ~/aws_neuronx_venv_pytorch_2_7_nxd_inference/bin/activate\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Install the Neuron vLLM fork into the virtual environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "cd upstreaming-to-vllm\n",
    "pip install -r requirements-neuron.txt\n",
    "VLLM_TARGET_DEVICE=\"neuron\" pip install -e .\n",
    "cd .."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download models\n",
    "\n",
    "To use this sample, you must first download a 70B model checkpoint from Hugging Face\n",
    "to a local path on the Trn2 instance. For more information, see\n",
    "[Downloading models](https://huggingface.co/docs/hub/en/models-downloading)\n",
    "in the Hugging Face documentation. You can download and use [meta-llama/Llama-3.3-70B-Instruct](https://huggingface.co/meta-llama/Llama-3.3-70B-Instruct)\n",
    "for this tutorial."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scenario 1: Run Llama3.3 70B on Trn2 without Prefix Caching\n",
    "\n",
    "### Step 1: Compile the model\n",
    "\n",
    "We will first compile using a command installed by `neuronx-distributed-inference`.\n",
    "\n",
    "Note that we are also using the following features as described in\n",
    "the tutorial for running 405B model [Tutorial: Deploying Llama3.1 405B (Trn2)](https://awsdocs-neuron.readthedocs-hosted.com/en/latest/libraries/nxd-inference/tutorials/trn2-llama3.1-405b-tutorial.html)\n",
    "\n",
    "- Logical NeuronCore Configuration (LNC)\n",
    "- Tensor parallelism (TP) on Trn2\n",
    "- Optimized Kernels\n",
    "\n",
    "Note the path we used to save the compiled model. This path should be used\n",
    "when launching vLLM server for inference so that the compiled model can be loaded without recompilation.\n",
    "Refer to the [NxD inference API](https://awsdocs-neuron.readthedocs-hosted.com/en/latest/libraries/nxd-inference/api-guides/api-guide.html) guide for more information on these `inference_demo` flags."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "# Replace this with the path where you downloaded and saved the model files.\n",
    "MODEL_PATH=\"/home/ubuntu/models/Llama-3.3-70B-Instruct/\"\n",
    "# This is where the compiled model will be saved. The same path\n",
    "# should be used when launching vLLM server for inference.\n",
    "COMPILED_MODEL_PATH=\"/home/ubuntu/traced_model/Llama-3.3-70B-Instruct/\"\n",
    "\n",
    "NUM_CORES=128\n",
    "TP_DEGREE=64\n",
    "LNC=2\n",
    "\n",
    "export NEURON_RT_VIRTUAL_CORE_SIZE=$LNC\n",
    "export NEURON_RT_NUM_CORES=$((NUM_CORES/NEURON_RT_VIRTUAL_CORE_SIZE))\n",
    "export NEURON_RT_EXEC_TIMEOUT=600 \n",
    "export XLA_DENSE_GATHER_FACTOR=0 \n",
    "export NEURON_RT_INSPECT_ENABLE=0\n",
    "\n",
    "inference_demo \\\n",
    "    --model-type llama \\\n",
    "    --task-type causal-lm \\\n",
    "        run \\\n",
    "        --model-path $MODEL_PATH \\\n",
    "        --compiled-model-path $COMPILED_MODEL_PATH \\\n",
    "        --torch-dtype bfloat16 \\\n",
    "        --start_rank_id 0 \\\n",
    "        --local_ranks_size $TP_DEGREE \\\n",
    "        --tp-degree $TP_DEGREE \\\n",
    "        --batch-size 4 \\\n",
    "        --is-continuous-batching \\\n",
    "        --ctx-batch-size 1 \\\n",
    "        --tkg-batch-size 4 \\\n",
    "        --max-context-length 8192 \\\n",
    "        --seq-len 8192 \\\n",
    "        --on-device-sampling \\\n",
    "        --top-k 1 \\\n",
    "        --do-sample \\\n",
    "        --fused-qkv \\\n",
    "        --sequence-parallel-enabled \\\n",
    "        --qkv-kernel-enabled \\\n",
    "        --attn-kernel-enabled \\\n",
    "        --mlp-kernel-enabled \\\n",
    "        --attn-block-tkg-nki-kernel-enabled \\\n",
    "        --attn-block-tkg-nki-kernel-cache-update \\\n",
    "        --k-cache-transposed \\\n",
    "        --cc-pipeline-tiling-factor 1 \\\n",
    "        --pad-token-id 2 \\\n",
    "        --enable-bucketing \\\n",
    "        --context-encoding-buckets 512 1024 2048 4096 8192 \\\n",
    "        --token-generation-buckets 512 1024 2048 4096 8192 \\\n",
    "        --compile-only \\\n",
    "        --prompt \"What is annapurna labs?\" 2>&1 | tee log.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Serve the model using vLLM\n",
    "\n",
    "After compiling the model, you can run the model using vLLM. Save the contents of the below script to another\n",
    "shell script file, for example, `start_vllm.sh` and then run it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile start_vllm.sh\n",
    "#!/bin/bash\n",
    "\n",
    "echo \"Running vLLM server in the background...\"\n",
    "rm -f ./vllm_server.log \n",
    "\n",
    "export NEURON_RT_INSPECT_ENABLE=0 \n",
    "export NEURON_RT_VIRTUAL_CORE_SIZE=2\n",
    "\n",
    "# These should be the same paths used when compiling the model.\n",
    "MODEL_PATH=\"/home/ubuntu/models/Llama-3.3-70B-Instruct/\"\n",
    "COMPILED_MODEL_PATH=\"/home/ubuntu/traced_model/Llama-3.3-70B-Instruct/\"\n",
    "\n",
    "export VLLM_NEURON_FRAMEWORK=\"neuronx-distributed-inference\"\n",
    "export NEURON_COMPILED_ARTIFACTS=$COMPILED_MODEL_PATH\n",
    "VLLM_RPC_TIMEOUT=100000 \n",
    "nohup python -m vllm.entrypoints.openai.api_server \\\n",
    "    --model $MODEL_PATH \\\n",
    "    --max-num-seqs 4 \\\n",
    "    --max-model-len 8192 \\\n",
    "    --tensor-parallel-size 64 \\\n",
    "    --device neuron \\\n",
    "    --use-v2-block-manager \\\n",
    "    --block-size 32 \\\n",
    "    --port 8000 > ./vllm_server.log 2>&1 &\n",
    "SERVER_PID=$!\n",
    "\n",
    "echo \"Server started in the background with the following id: $SERVER_PID. Waiting until server is ready to serve...\"\n",
    "\n",
    "until grep -q \"Server is ready to serve\" ./vllm_server.log 2>/dev/null || ! kill -0 $SERVER_PID 2>/dev/null; do sleep 0.5; done\n",
    "grep -q \"Server is ready to serve\" ./vllm_server.log 2>/dev/null && echo \"vLLM Server is ready!\" || (echo \"vLLM Server failed, check the ./vllm_server.log file\" && exit 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!chmod +x ./start_vllm.sh\n",
    "!./start_vllm.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you see the below logs, that means your server is up and running:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "INFO: Started server process [284309]\n",
    "INFO: Waiting for application startup.\n",
    "INFO: Application startup complete.\n",
    "INFO: Uvicorn running on http://0.0.0.0:8000 (Press CTRL+C to quit)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Analyze Request response from server\n",
    "\n",
    "An example script has been added to demonstrate how a common lookup table is used to\n",
    "answer 10 different questions while measuring the total response time. The lookup table\n",
    "serves as a shared prefix that's consistently applied across all 10 input prompts.\n",
    "The script will calculate and display the average time required to answer all questions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "MODEL_PATH=\"/home/ubuntu/models/Llama-3.3-70B-Instruct/\"\n",
    "COMPILED_MODEL_PATH=\"/home/ubuntu/traced_model/Llama-3.3-70B-Instruct/\"\n",
    "\n",
    "LONG_PROMPT=$(cat << 'EOL'\n",
    "You are a helpful assistant in recognizes the content of tables in markdown format. Here is a table as follows.\n",
    "# Table\n",
    "| ID  | Name          | Age | Occupation    | Country       | Email                  | Phone Number   | Address                       |\n",
    "|-----|---------------|-----|---------------|---------------|------------------------|----------------|------------------------------|\n",
    "| 1   | John Doe      | 29  | Engineer      | USA           | john.doe@example.com   | 555-1234       | 123 Elm St, Springfield, IL  |\n",
    "| 2   | Jane Smith    | 34  | Doctor        | Canada        | jane.smith@example.com | 555-5678       | 456 Oak St, Toronto, ON      |\n",
    "| 3   | Alice Johnson | 27  | Teacher       | UK            | alice.j@example.com    | 555-8765       | 789 Pine St, London, UK      |\n",
    "| 4   | Bob Brown     | 45  | Artist        | Australia     | bob.b@example.com      | 555-4321       | 321 Maple St, Sydney, NSW    |\n",
    "| 5   | Carol White   | 31  | Scientist     | New Zealand   | carol.w@example.com    | 555-6789       | 654 Birch St, Wellington, NZ |\n",
    "| 6   | Dave Green    | 28  | Lawyer        | Ireland       | dave.g@example.com     | 555-3456       | 987 Cedar St, Dublin, IE     |\n",
    "| 7   | Emma Black    | 40  | Musician      | USA           | emma.b@example.com     | 555-1111       | 246 Ash St, New York, NY     |\n",
    "| 8   | Frank Blue    | 37  | Chef          | Canada        | frank.b@example.com    | 555-2222       | 135 Spruce St, Vancouver, BC |\n",
    "| 9   | Grace Yellow  | 50  | Engineer      | UK            | grace.y@example.com    | 555-3333       | 864 Fir St, Manchester, UK   |\n",
    "| 10  | Henry Violet  | 32  | Artist        | Australia     | henry.v@example.com    | 555-4444       | 753 Willow St, Melbourne, VIC|\n",
    "| 11  | Irene Orange  | 26  | Scientist     | New Zealand   | irene.o@example.com    | 555-5555       | 912 Poplar St, Auckland, NZ  |\n",
    "| 12  | Jack Indigo   | 38  | Teacher       | Ireland       | jack.i@example.com     | 555-6666       | 159 Elm St, Cork, IE         |\n",
    "| 13  | Karen Red     | 41  | Lawyer        | USA           | karen.r@example.com    | 555-7777       | 357 Cedar St, Boston, MA     |\n",
    "| 14  | Leo Brown     | 30  | Chef          | Canada        | leo.b@example.com      | 555-8888       | 246 Oak St, Calgary, AB      |\n",
    "| 15  | Mia Green     | 33  | Musician      | UK            | mia.g@example.com      | 555-9999       | 975 Pine St, Edinburgh, UK   |\n",
    "| 16  | Noah Yellow   | 29  | Doctor        | Australia     | noah.y@example.com     | 555-0000       | 864 Birch St, Brisbane, QLD  |\n",
    "| 17  | Olivia Blue   | 35  | Engineer      | New Zealand   | olivia.b@example.com   | 555-1212       | 753 Maple St, Hamilton, NZ   |\n",
    "| 18  | Peter Black   | 42  | Artist        | Ireland       | peter.b@example.com    | 555-3434       | 912 Fir St, Limerick, IE     |\n",
    "| 19  | Quinn White   | 28  | Scientist     | USA           | quinn.w@example.com    | 555-5656       | 159 Willow St, Seattle, WA   |\n",
    "| 20  | Rachel Red    | 31  | Teacher       | Canada        | rachel.r@example.com   | 555-7878       | 357 Poplar St, Ottawa, ON    |\n",
    "| 21  | Steve Green   | 44  | Lawyer        | UK            | steve.g@example.com    | 555-9090       | 753 Elm St, Birmingham, UK   |\n",
    "| 22  | Tina Blue     | 36  | Musician      | Australia     | tina.b@example.com     | 555-1213       | 864 Cedar St, Perth, WA      |\n",
    "| 23  | Umar Black    | 39  | Chef          | New Zealand   | umar.b@example.com     | 555-3435       | 975 Spruce St, Christchurch, NZ|\n",
    "| 24  | Victor Yellow | 43  | Engineer      | Ireland       | victor.y@example.com   | 555-5657       | 246 Willow St, Galway, IE    |\n",
    "| 25  | Wendy Orange  | 27  | Artist        | USA           | wendy.o@example.com    | 555-7879       | 135 Elm St, Denver, CO       |\n",
    "| 26  | Xavier Green  | 34  | Scientist     | Canada        | xavier.g@example.com   | 555-9091       | 357 Oak St, Montreal, QC     |\n",
    "| 27  | Yara Red      | 41  | Teacher       | UK            | yara.r@example.com     | 555-1214       | 975 Pine St, Leeds, UK       |\n",
    "| 28  | Zack Blue     | 30  | Lawyer        | Australia     | zack.b@example.com     | 555-3436       | 135 Birch St, Adelaide, SA   |\n",
    "| 29  | Amy White     | 33  | Musician      | New Zealand   | amy.w@example.com      | 555-5658       | 159 Maple St, Wellington, NZ |\n",
    "| 30  | Ben Black     | 38  | Chef          | Ireland       | ben.b@example.com      | 555-7870       | 246 Fir St, Waterford, IE    |\n",
    "EOL\n",
    ")\n",
    "\n",
    "questions=(\n",
    "    \"Question: what is the age of John Doe? Your answer: The age of John Doe is \"\n",
    "    \"Question: what is the age of Zack Blue? Your answer: The age of Zack Blue is \"\n",
    "    \"Question: Which country is Ben Black from? Your answer: The country of Ben Black is \"\n",
    "    \"Question: Who has rachel.r@example.com as their email domain? Your answer: The email domain rachel.r@example.com belongs to \"\n",
    "    \"Question: What is the phone number for contacting Karen Red? Your answer: The phone number for contacting Karen Red is \"\n",
    "    \"Question: What is the occupation of Tina Blue? Your answer: The occupation of Tina Blue is \"\n",
    "    \"Question: What is the name of the person with id as 29? Your answer: The name of the person with id as 29 is \"\n",
    "    \"Question: What is the address of Alice Johnson? Your answer: The address of Alice Johnson is \"\n",
    "    \"Question: What is the id of Irene Orange? Your answer: The id of Irene Orange is \"\n",
    "    \"Question: What is the age of Leo Brown? Your answer: The age of Leo Brown is \"\n",
    ")\n",
    "\n",
    "\n",
    "# Function to make a single request\n",
    "make_request() {\n",
    "    local question=$1\n",
    "    local prompt_with_suffix=\"${LONG_PROMPT}\n",
    "\n",
    "Based on the table above, please answer this question:\n",
    "${question}\"\n",
    "    \n",
    "    local escaped_prompt=$(echo \"$prompt_with_suffix\" | jq -Rs .)\n",
    "    \n",
    "    # Make the curl request and capture both response and time\n",
    "    local response_file=$(mktemp)\n",
    "    time_output=$(TIMEFORMAT='%R'; { time curl -s http://localhost:8000/v1/chat/completions \\\n",
    "        -H \"Content-Type: application/json\" \\\n",
    "        -d \"{\n",
    "            \\\"model\\\": \\\"$MODEL_PATH\\\",\n",
    "            \\\"messages\\\": [\n",
    "                {\n",
    "                    \\\"role\\\": \\\"user\\\",\n",
    "                    \\\"content\\\": ${escaped_prompt}\n",
    "                }\n",
    "            ]\n",
    "        }\" > \"$response_file\"; } 2>&1)\n",
    "    \n",
    "    # Extract the response content\n",
    "    local response_content=$(cat \"$response_file\" | jq -r '.choices[0].message.content')\n",
    "    rm \"$response_file\"\n",
    "    \n",
    "    # Return both time and response\n",
    "    echo \"TIME:$time_output\"\n",
    "    echo \"RESPONSE:$response_content\"\n",
    "}\n",
    "\n",
    "# Make first request (warm-up) with a random question\n",
    "random_index=$((RANDOM % ${#questions[@]}))\n",
    "echo \"Warm-up request with question: ${questions[$random_index]}\"\n",
    "IFS=$'\\n' read -r -d '' time_str response_str < <(make_request \"${questions[$random_index]}\" && echo '')\n",
    "echo \"Response: $response_str\"\n",
    "echo \"Time taken: ${time_str#TIME:} seconds\"\n",
    "echo \"Warm-up complete\"\n",
    "echo \"-------------------\"\n",
    "\n",
    "# Make 10 timed requests with random questions\n",
    "total_time=0\n",
    "for i in {0..9}; do\n",
    "    random_index=$i\n",
    "    #random_index=$((RANDOM % ${#questions[@]}))\n",
    "    question=\"${questions[$random_index]}\"\n",
    "    echo \"Request $i with question: $question\"\n",
    "    \n",
    "    IFS=$'\\n' read -r -d '' time_str response_str < <(make_request \"$question\" && echo '')\n",
    "    time_taken=${time_str#TIME:}\n",
    "    response=${response_str#RESPONSE:}\n",
    "    \n",
    "    total_time=$(echo \"$total_time + $time_taken\" | bc -l)\n",
    "    echo \"Response: $response\"\n",
    "    echo \"Time taken: ${time_taken} seconds\"\n",
    "    echo \"-------------------\"\n",
    "done\n",
    "\n",
    "# Calculate and display average time\n",
    "average_time=$(echo \"scale=3; $total_time / 10\" | bc -l)\n",
    "echo \"Average time across 10 requests: ${average_time} seconds\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Output from the script would include all the answers to the questions along with the average time to process all the requests at the very end as shown below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "Average time across 10 requests: .388 seconds\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scenario 2: Run Llama3.3 70B on Trn2 with Prefix Caching"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Compile the model\n",
    "\n",
    "The compilation script with prefix caching adds extra flags specific to prefix caching to enable and configure Block KV cache layout along with enabling the kernels used with prefix caching. Please refer to the [Prefix Caching Support](https://awsdocs-neuron.readthedocs-hosted.com/en/latest/libraries/nxd-inference/developer_guides/feature-guide.html#prefix-caching-support) documentation for more information on the prefix caching flags used below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "# Replace this with the path where you downloaded and saved the model files.\n",
    "MODEL_PATH=\"/home/ubuntu/models/Llama-3.3-70B-Instruct/\"\n",
    "# This is where the compiled model will be saved. The same path\n",
    "# should be used when launching vLLM server for inference.\n",
    "COMPILED_MODEL_PATH=\"/home/ubuntu/traced_model/Llama-3.3-70B-Instruct/\"\n",
    "\n",
    "NUM_CORES=128\n",
    "TP_DEGREE=64\n",
    "LNC=2\n",
    "\n",
    "export NEURON_RT_VIRTUAL_CORE_SIZE=$LNC\n",
    "export NEURON_RT_NUM_CORES=$((NUM_CORES/NEURON_RT_VIRTUAL_CORE_SIZE))\n",
    "export NEURON_RT_EXEC_TIMEOUT=600 \n",
    "export XLA_DENSE_GATHER_FACTOR=0 \n",
    "export NEURON_RT_INSPECT_ENABLE=0\n",
    "\n",
    "inference_demo \\\n",
    "    --model-type llama \\\n",
    "    --task-type causal-lm \\\n",
    "        run \\\n",
    "        --model-path $MODEL_PATH \\\n",
    "        --compiled-model-path $COMPILED_MODEL_PATH \\\n",
    "        --torch-dtype bfloat16 \\\n",
    "        --start_rank_id 0 \\\n",
    "        --local_ranks_size $TP_DEGREE \\\n",
    "        --tp-degree $TP_DEGREE \\\n",
    "        --batch-size 4 \\\n",
    "        --is-continuous-batching \\\n",
    "        --ctx-batch-size 1 \\\n",
    "        --tkg-batch-size 4 \\\n",
    "        --max-context-length 8192 \\\n",
    "        --seq-len 8192 \\\n",
    "        --on-device-sampling \\\n",
    "        --top-k 1 \\\n",
    "        --do-sample \\\n",
    "        --fused-qkv \\\n",
    "        --sequence-parallel-enabled \\\n",
    "        --qkv-kernel-enabled \\\n",
    "        --attn-kernel-enabled \\\n",
    "        --mlp-kernel-enabled \\\n",
    "        --attn-block-tkg-nki-kernel-enabled \\\n",
    "        --attn-block-tkg-nki-kernel-cache-update \\\n",
    "        --cc-pipeline-tiling-factor 1 \\\n",
    "        --pad-token-id 2 \\\n",
    "        --enable-bucketing \\\n",
    "        --context-encoding-buckets 512 1024 2048 4096 8192 \\\n",
    "        --token-generation-buckets 512 1024 2048 4096 8192 \\\n",
    "        --prefix-buckets 512 1024 2048 \\\n",
    "        --enable-block-kv-layout \\\n",
    "        --pa-num-blocks 2048 \\\n",
    "        --pa-block-size 32 \\\n",
    "        --enable-prefix-caching \\\n",
    "        --compile-only \\\n",
    "        --prompt \"What is annapurna labs?\" 2>&1 | tee log.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Serve the model using vLLM with prefix caching enabled\n",
    "\n",
    "After compiling the model, you can serve the model using vLLM with prefix caching enabled.\n",
    "Save the contents of the below script to another\n",
    "shell script file, for example, `start_vllm_apc.sh` and then run it.\n",
    "\n",
    "Note that we use `--enable-prefix-caching` in vLLM to enable prefix caching, along\n",
    "with `--block-size 32` and `--num-gpu-blocks-override 2048` which are consistent\n",
    "with `--pa-block-size 32` and `--pa-num-blocks 2048` flags specified during model\n",
    "compilation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile start_vllm.sh\n",
    "#!/bin/bash\n",
    "echo \"Running vLLM server in the background...\"\n",
    "rm -f ./vllm_server.log \n",
    "\n",
    "export NEURON_RT_INSPECT_ENABLE=0 \n",
    "export NEURON_RT_VIRTUAL_CORE_SIZE=2\n",
    "\n",
    "# These should be the same paths used when compiling the model.\n",
    "MODEL_PATH=\"/home/ubuntu/models/Llama-3.3-70B-Instruct/\"\n",
    "COMPILED_MODEL_PATH=\"/home/ubuntu/traced_model/Llama-3.3-70B-Instruct/\"\n",
    "\n",
    "export VLLM_NEURON_FRAMEWORK=\"neuronx-distributed-inference\"\n",
    "export NEURON_COMPILED_ARTIFACTS=$COMPILED_MODEL_PATH\n",
    "VLLM_RPC_TIMEOUT=100000 \n",
    "nohup python -m vllm.entrypoints.openai.api_server \\\n",
    "    --model $MODEL_PATH \\\n",
    "    --max-num-seqs 4 \\\n",
    "    --max-model-len 8192 \\\n",
    "    --tensor-parallel-size 64 \\\n",
    "    --device neuron \\\n",
    "    --use-v2-block-manager \\\n",
    "    --num-gpu-blocks-override 2048 \\\n",
    "    --enable-prefix-caching \\\n",
    "    --block-size 32 \\\n",
    "    --override-neuron-config \"{\\\"is_block_kv_layout\\\": true, \\\"is_prefix_caching\\\": true}\" \\\n",
    "    --port 8000 > ./vllm_server.log 2>&1 &\n",
    "SERVER_PID=$!\n",
    "\n",
    "echo \"Server started in the background with the following id: $SERVER_PID. Waiting until server is ready to serve...\"\n",
    "\n",
    "until grep -q \"Server is ready to serve\" ./vllm_server.log 2>/dev/null || ! kill -0 $SERVER_PID 2>/dev/null; do sleep 0.5; done\n",
    "grep -q \"Server is ready to serve\" ./vllm_server.log 2>/dev/null && echo \"vLLM Server is ready!\" || (echo \"vLLM Server failed, check the ./vllm_server.log file\" && exit 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!chmod +x ./start_vllm.sh\n",
    "!./start_vllm.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wait for the server to be up and running before proceeding further.\n",
    "\n",
    "### Step 3: Analyze Request response from server\n",
    "\n",
    "Execute the same script file from scenario 1,\n",
    "to send identical request to the server with prefix caching enabled.\n",
    "The average time to respond to all the requests will be printed in the terminal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "MODEL_PATH=\"/home/ubuntu/models/Llama-3.3-70B-Instruct/\"\n",
    "COMPILED_MODEL_PATH=\"/home/ubuntu/traced_model/Llama-3.3-70B-Instruct/\"\n",
    "\n",
    "LONG_PROMPT=$(cat << 'EOL'\n",
    "You are a helpful assistant in recognizes the content of tables in markdown format. Here is a table as follows.\n",
    "# Table\n",
    "| ID  | Name          | Age | Occupation    | Country       | Email                  | Phone Number   | Address                       |\n",
    "|-----|---------------|-----|---------------|---------------|------------------------|----------------|------------------------------|\n",
    "| 1   | John Doe      | 29  | Engineer      | USA           | john.doe@example.com   | 555-1234       | 123 Elm St, Springfield, IL  |\n",
    "| 2   | Jane Smith    | 34  | Doctor        | Canada        | jane.smith@example.com | 555-5678       | 456 Oak St, Toronto, ON      |\n",
    "| 3   | Alice Johnson | 27  | Teacher       | UK            | alice.j@example.com    | 555-8765       | 789 Pine St, London, UK      |\n",
    "| 4   | Bob Brown     | 45  | Artist        | Australia     | bob.b@example.com      | 555-4321       | 321 Maple St, Sydney, NSW    |\n",
    "| 5   | Carol White   | 31  | Scientist     | New Zealand   | carol.w@example.com    | 555-6789       | 654 Birch St, Wellington, NZ |\n",
    "| 6   | Dave Green    | 28  | Lawyer        | Ireland       | dave.g@example.com     | 555-3456       | 987 Cedar St, Dublin, IE     |\n",
    "| 7   | Emma Black    | 40  | Musician      | USA           | emma.b@example.com     | 555-1111       | 246 Ash St, New York, NY     |\n",
    "| 8   | Frank Blue    | 37  | Chef          | Canada        | frank.b@example.com    | 555-2222       | 135 Spruce St, Vancouver, BC |\n",
    "| 9   | Grace Yellow  | 50  | Engineer      | UK            | grace.y@example.com    | 555-3333       | 864 Fir St, Manchester, UK   |\n",
    "| 10  | Henry Violet  | 32  | Artist        | Australia     | henry.v@example.com    | 555-4444       | 753 Willow St, Melbourne, VIC|\n",
    "| 11  | Irene Orange  | 26  | Scientist     | New Zealand   | irene.o@example.com    | 555-5555       | 912 Poplar St, Auckland, NZ  |\n",
    "| 12  | Jack Indigo   | 38  | Teacher       | Ireland       | jack.i@example.com     | 555-6666       | 159 Elm St, Cork, IE         |\n",
    "| 13  | Karen Red     | 41  | Lawyer        | USA           | karen.r@example.com    | 555-7777       | 357 Cedar St, Boston, MA     |\n",
    "| 14  | Leo Brown     | 30  | Chef          | Canada        | leo.b@example.com      | 555-8888       | 246 Oak St, Calgary, AB      |\n",
    "| 15  | Mia Green     | 33  | Musician      | UK            | mia.g@example.com      | 555-9999       | 975 Pine St, Edinburgh, UK   |\n",
    "| 16  | Noah Yellow   | 29  | Doctor        | Australia     | noah.y@example.com     | 555-0000       | 864 Birch St, Brisbane, QLD  |\n",
    "| 17  | Olivia Blue   | 35  | Engineer      | New Zealand   | olivia.b@example.com   | 555-1212       | 753 Maple St, Hamilton, NZ   |\n",
    "| 18  | Peter Black   | 42  | Artist        | Ireland       | peter.b@example.com    | 555-3434       | 912 Fir St, Limerick, IE     |\n",
    "| 19  | Quinn White   | 28  | Scientist     | USA           | quinn.w@example.com    | 555-5656       | 159 Willow St, Seattle, WA   |\n",
    "| 20  | Rachel Red    | 31  | Teacher       | Canada        | rachel.r@example.com   | 555-7878       | 357 Poplar St, Ottawa, ON    |\n",
    "| 21  | Steve Green   | 44  | Lawyer        | UK            | steve.g@example.com    | 555-9090       | 753 Elm St, Birmingham, UK   |\n",
    "| 22  | Tina Blue     | 36  | Musician      | Australia     | tina.b@example.com     | 555-1213       | 864 Cedar St, Perth, WA      |\n",
    "| 23  | Umar Black    | 39  | Chef          | New Zealand   | umar.b@example.com     | 555-3435       | 975 Spruce St, Christchurch, NZ|\n",
    "| 24  | Victor Yellow | 43  | Engineer      | Ireland       | victor.y@example.com   | 555-5657       | 246 Willow St, Galway, IE    |\n",
    "| 25  | Wendy Orange  | 27  | Artist        | USA           | wendy.o@example.com    | 555-7879       | 135 Elm St, Denver, CO       |\n",
    "| 26  | Xavier Green  | 34  | Scientist     | Canada        | xavier.g@example.com   | 555-9091       | 357 Oak St, Montreal, QC     |\n",
    "| 27  | Yara Red      | 41  | Teacher       | UK            | yara.r@example.com     | 555-1214       | 975 Pine St, Leeds, UK       |\n",
    "| 28  | Zack Blue     | 30  | Lawyer        | Australia     | zack.b@example.com     | 555-3436       | 135 Birch St, Adelaide, SA   |\n",
    "| 29  | Amy White     | 33  | Musician      | New Zealand   | amy.w@example.com      | 555-5658       | 159 Maple St, Wellington, NZ |\n",
    "| 30  | Ben Black     | 38  | Chef          | Ireland       | ben.b@example.com      | 555-7870       | 246 Fir St, Waterford, IE    |\n",
    "EOL\n",
    ")\n",
    "\n",
    "questions=(\n",
    "    \"Question: what is the age of John Doe? Your answer: The age of John Doe is \"\n",
    "    \"Question: what is the age of Zack Blue? Your answer: The age of Zack Blue is \"\n",
    "    \"Question: Which country is Ben Black from? Your answer: The country of Ben Black is \"\n",
    "    \"Question: Who has rachel.r@example.com as their email domain? Your answer: The email domain rachel.r@example.com belongs to \"\n",
    "    \"Question: What is the phone number for contacting Karen Red? Your answer: The phone number for contacting Karen Red is \"\n",
    "    \"Question: What is the occupation of Tina Blue? Your answer: The occupation of Tina Blue is \"\n",
    "    \"Question: What is the name of the person with id as 29? Your answer: The name of the person with id as 29 is \"\n",
    "    \"Question: What is the address of Alice Johnson? Your answer: The address of Alice Johnson is \"\n",
    "    \"Question: What is the id of Irene Orange? Your answer: The id of Irene Orange is \"\n",
    "    \"Question: What is the age of Leo Brown? Your answer: The age of Leo Brown is \"\n",
    ")\n",
    "\n",
    "\n",
    "# Function to make a single request\n",
    "make_request() {\n",
    "    local question=$1\n",
    "    local prompt_with_suffix=\"${LONG_PROMPT}\n",
    "\n",
    "Based on the table above, please answer this question:\n",
    "${question}\"\n",
    "    \n",
    "    local escaped_prompt=$(echo \"$prompt_with_suffix\" | jq -Rs .)\n",
    "    \n",
    "    # Make the curl request and capture both response and time\n",
    "    local response_file=$(mktemp)\n",
    "    time_output=$(TIMEFORMAT='%R'; { time curl -s http://localhost:8000/v1/chat/completions \\\n",
    "        -H \"Content-Type: application/json\" \\\n",
    "        -d \"{\n",
    "            \\\"model\\\": \\\"$MODEL_PATH\\\",\n",
    "            \\\"messages\\\": [\n",
    "                {\n",
    "                    \\\"role\\\": \\\"user\\\",\n",
    "                    \\\"content\\\": ${escaped_prompt}\n",
    "                }\n",
    "            ]\n",
    "        }\" > \"$response_file\"; } 2>&1)\n",
    "    \n",
    "    # Extract the response content\n",
    "    local response_content=$(cat \"$response_file\" | jq -r '.choices[0].message.content')\n",
    "    rm \"$response_file\"\n",
    "    \n",
    "    # Return both time and response\n",
    "    echo \"TIME:$time_output\"\n",
    "    echo \"RESPONSE:$response_content\"\n",
    "}\n",
    "\n",
    "# Make first request (warm-up) with a random question\n",
    "random_index=$((RANDOM % ${#questions[@]}))\n",
    "echo \"Warm-up request with question: ${questions[$random_index]}\"\n",
    "IFS=$'\\n' read -r -d '' time_str response_str < <(make_request \"${questions[$random_index]}\" && echo '')\n",
    "echo \"Response: $response_str\"\n",
    "echo \"Time taken: ${time_str#TIME:} seconds\"\n",
    "echo \"Warm-up complete\"\n",
    "echo \"-------------------\"\n",
    "\n",
    "# Make 10 timed requests with random questions\n",
    "total_time=0\n",
    "for i in {0..9}; do\n",
    "    random_index=$i\n",
    "    #random_index=$((RANDOM % ${#questions[@]}))\n",
    "    question=\"${questions[$random_index]}\"\n",
    "    echo \"Request $i with question: $question\"\n",
    "    \n",
    "    IFS=$'\\n' read -r -d '' time_str response_str < <(make_request \"$question\" && echo '')\n",
    "    time_taken=${time_str#TIME:}\n",
    "    response=${response_str#RESPONSE:}\n",
    "    \n",
    "    total_time=$(echo \"$total_time + $time_taken\" | bc -l)\n",
    "    echo \"Response: $response\"\n",
    "    echo \"Time taken: ${time_taken} seconds\"\n",
    "    echo \"-------------------\"\n",
    "done\n",
    "\n",
    "# Calculate and display average time\n",
    "average_time=$(echo \"scale=3; $total_time / 10\" | bc -l)\n",
    "echo \"Average time across 10 requests: ${average_time} seconds\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "Average time across 10 requests: .388 seconds\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As seen from the two scenarios, average time with prefix caching enabled is lesser than the time it takes to serve the same requests with prefix caching disabled. This is attributed to the lesser time to compute the first token by reusing the common prefix across all the prompts.\n",
    "\n",
    "We also ran the same model configurations with public datasets with varying cache hit rates for benchmarking prefix caching on neuron and here are the results that we achieved:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| Dataset | TTFT (P50 in ms) without prefix caching | TTFT (P50 in ms) with prefix caching | Improvement |\n",
    "|---------|----------------------------------------|-------------------------------------|-------------|\n",
    "| math.math (>90% cache hit) | 342.81 | 107.8 | 3.18x |\n",
    "| dynamic sonnet 1k (~25% cache hit) | 123.08 | 102.15 | 1.2x |\n",
    "| dynamic sonnet 2k (~25% cache hit) | 592.8 | 377.2 | 1.57x |\n",
    "| HumanEval (No cache hit) | 89.7 | 91.8 | 0.98x |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "In general, with a higher ratio of prefix(shared prompt) to prefill tokens that results in higher cache-hit rate, \n",
    "prefix caching achieves a TTFT speedup of up to 3x compared to when prefix caching is disabled. When the dataset has\n",
    "low prefix cache hit rate, prefix caching TTFT performance can degrade slightly due to the overhead of supporting\n",
    "block KV cache layout, as seen in the HumanEval dataset."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "neuron-224",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
