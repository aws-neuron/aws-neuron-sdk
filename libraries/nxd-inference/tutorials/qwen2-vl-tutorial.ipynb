{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tutorial: Deploy Qwen2-VL on Trn2 instances\n",
    "\n",
    "This tutorial provides a step-by-step guide to deploy [Qwen/Qwen2-VL-7B-Instruct](https://huggingface.co/Qwen/Qwen2-VL-7B-Instruct) using NeuronX Distributed (NxD) Inference on a single `trn2.48xlarge` instance."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "raw_mimetype": "text/restructuredtext",
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    ".. contents:: Table of contents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Set up your development environment\n",
    "\n",
    "As a prerequisite, this tutorial requires that you have a Trn2 instance created from a Deep Learning AMI that has the Neuron SDK pre-installed.\n",
    "\n",
    "To set up a Trn2 instance using Deep Learning AMI with pre-installed Neuron SDK, see the [NxDI setup guide](https://awsdocs-neuron.readthedocs-hosted.com/en/latest/libraries/nxd-inference/nxdi-setup.html#nxdi-setup). To run a Jupyter (.ipynb) notebook on a Neuron instance, follow this [guide](https://awsdocs-neuron.readthedocs-hosted.com/en/latest/general/setup/notebook/setup-jupyter-notebook-steps-troubleshooting.html).\n",
    "\n",
    "After setting up an instance, use SSH to connect to the Trn2 instance using the key pair that you chose when you launched the instance.\n",
    "\n",
    "After you are connected, activate the Python virtual environment that includes the Neuron SDK."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "```python\n",
    "pip list | grep neuron\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should see Neuron packages including\n",
    "`neuronx-distributed-inference` and `neuronx-cc`.\n",
    "\n",
    "## Step 2: Install the vLLM version that supports NxD Inference\n",
    "\n",
    "NxD Inference supports running models with vLLM. This functionality is available in the vLLM-Neuron GitHub repository. Install the latest release branch of vLLM-Neuron plugin following instructions in the [vLLM User Guide for NxD Inference](https://awsdocs-neuron.readthedocs-hosted.com/en/latest/libraries/nxd-inference/developer_guides/vllm-user-guide-v1.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ensure that the Neuron virtual environment is activated if you are using a new terminal session instead of the one from connection step above. Then, install the Neuron vLLM fork into the virtual environment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3 Download the model from HuggingFace (Optional)\n",
    "\n",
    "To deploy [Qwen/Qwen2-VL-7B-Instruct](https://huggingface.co/Qwen/Qwen2-VL-7B-Instruct) on Neuron, first download the checkpoint from HuggingFace to a local path on the Trn2 instance. For more information on downloading models from HuggingFace, refer to [HuggingFace's guide on Downloading models](https://huggingface.co/docs/hub/en/models-downloading)).\n",
    "\n",
    "After the download, you should see a `config.json` file in the output folder along with weights in `model-xxxx-of-xxxx.safetensors` format."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Compile and deploy Qwen2-VL Inference\n",
    "\n",
    "In this step, you use the `vllm` command to deploy the model. The `neuronx-distributed-inference` model loader in vllm performs JIT compilation before deploying it with the model server. Replace the `model_name_or_path` with your specific path if you download the model checkpoint from HuggingFace(Step 3).\n",
    "\n",
    "Here are two examples of running Qwen2-VL with vLLM V1:\n",
    "\n",
    "* Offline inference: you can provide prompts in a python script and execute it.\n",
    "* Online inference: you will serve the model in an online server and send requests. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Configuration Requirements & Examples\n",
    "\n",
    "There is a known issue with `batch_size` > 1 or `tp_degree` != 4 configurations for Qwen2-VL models. Here we suggest to use `batch_size` = 1 and `tp_degree` = 4 configuration, which deploys `Qwen/Qwen2-VL-7B-Instruct` model on a single trn2 chip with 4 cores. You can replicate the setting on the `trn2.48xlarge` instance consisting of 16 chips and 64 cores.\n",
    "\n",
    "We support configurable image sizes for Qwen2-VL and use `number_of_images` as the vision buckets. For example, in the configuration below, `number_of_images` is the maximum vision bucket, i.e., `128`.\n",
    "Please specify `default_image_width` and `default_image_height` in the `vision_neuron_config` as the input image size. The default image sizes are `default_image_width: 640` and `default_image_height: 320`.\n",
    "\n",
    "<div class=\"alert alert-block alert-warning\">\n",
    "<b>Note:</b> Please make sure the number of tokens does not exceed the `max_content_length` in the `text_neuron_config`, i.e., `number_of_prompt_tokens + (default_image_width // 28) * (default_image_height // 28) * number_of_images < max_context_length - max_new_tokens`.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We configure these fields below to improve performance. For more details, refer to [NxD Inference features configurations guide](https://awsdocs-neuron.readthedocs-hosted.com/en/latest/libraries/nxd-inference/developer_guides/feature-guide.html).\n",
    "- `sequence_parallel_enabled`: whether to enable sequence parallel.\n",
    "- `fuse_qkv` and `qkv_kernel_enabled`: whether to use the fused QKV kernel. `qkv_kernel_enabled` is not supported yet in the `vision_neuron_config` for Qwen2-VL.\n",
    "- `attn_kernel_enabled`: whether to use the optimized attention kernel.\n",
    "\n",
    "Below we provide the recommended configuration with `batch_size` 1 and `tp_degree` 4.\n",
    "<div class=\"alert alert-block alert-warning\">\n",
    "<b>Note:</b> If you encounter Out-of-Memory issue during the runtime, please try to reduce the size of vision buckets as the KV cache grows linearly with batch size and sequence length.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qwen2_vl_neuron_config = {\n",
    "    \"text_neuron_config\": {\n",
    "        \"batch_size\": 1,\n",
    "        \"ctx_batch_size\": 1,\n",
    "        \"tkg_batch_size\": 1,\n",
    "        \"seq_len\": 32768,\n",
    "        \"max_new_tokens\": 64,\n",
    "        \"max_context_length\": 32768,\n",
    "        \"torch_dtype\": \"float16\",\n",
    "        \"skip_sharding\": False,\n",
    "        \"save_sharded_checkpoint\": True,\n",
    "        \"tp_degree\": 4,\n",
    "        \"world_size\": 4,\n",
    "        \"enable_bucketing\": True,\n",
    "        \"context_encoding_buckets\": [2048, 16384, 32768],\n",
    "        \"token_generation_buckets\": [2048, 16384, 32768],\n",
    "        \"fused_qkv\": True,\n",
    "        \"qkv_kernel_enabled\": True,\n",
    "        \"sequence_parallel_enabled\": True,\n",
    "        \"attn_kernel_enabled\": True,\n",
    "        \"cc_pipeline_tiling_factor\": 2,\n",
    "        \"attention_dtype\": \"float16\",\n",
    "        \"rpl_reduce_dtype\": \"float16\",\n",
    "        \"cast_type\": \"as-declared\",\n",
    "        \"logical_neuron_cores\": 2,\n",
    "        \"on_device_sampling_config\": None,\n",
    "    },\n",
    "    \"vision_neuron_config\": {\n",
    "        \"batch_size\": 1,\n",
    "        \"seq_len\": 131072,\n",
    "        \"max_context_length\": 131072,\n",
    "        \"torch_dtype\": \"bfloat16\",\n",
    "        \"skip_sharding\": False,\n",
    "        \"save_sharded_checkpoint\": True,\n",
    "        \"tp_degree\": 4,\n",
    "        \"world_size\": 4,\n",
    "        \"fused_qkv\": True,\n",
    "        \"qkv_kernel_enabled\": False,\n",
    "        \"attn_kernel_enabled\": True,\n",
    "        \"enable_bucketing\": True,\n",
    "        \"buckets\": [128],\n",
    "        \"cc_pipeline_tiling_factor\": 2,\n",
    "        \"attention_dtype\": \"bfloat16\",\n",
    "        \"rpl_reduce_dtype\": \"bfloat16\",\n",
    "        \"cast_type\": \"as-declared\",\n",
    "        \"logical_neuron_cores\": 2,\n",
    "        \"default_image_width\": 640,\n",
    "        \"default_image_height\": 320\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Offline Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"VLLM_NEURON_FRAMEWORK\"] = \"neuronx-distributed-inference\"\n",
    "\n",
    "from vllm import LLM, SamplingParams\n",
    "from vllm.assets.image import ImageAsset\n",
    "from transformers import AutoProcessor\n",
    "\n",
    "def qwen2_vl_offline_test():\n",
    "    model_name_or_path = \"Qwen/Qwen2-VL-7B-Instruct/\"\n",
    "    # Create an LLM.\n",
    "    llm = LLM(\n",
    "    model=model_name_or_path,\n",
    "    tensor_parallel_size=4,\n",
    "    max_num_seqs=1,\n",
    "    max_model_len=32768,\n",
    "    additional_config=dict(\n",
    "        override_neuron_config=qwen2_vl_neuron_config  # Use the configuration defined above\n",
    "    ),\n",
    "    enable_prefix_caching=False,\n",
    "    enable_chunked_prefill=False,\n",
    "    )\n",
    "\n",
    "    # Sample prompts.\n",
    "    prompt = \"What do you see in these images?\"\n",
    "    # Resize to default image size\n",
    "    default_image_size = (640, 320)\n",
    "\n",
    "    images = [\n",
    "        ImageAsset(\"blue_flowers\").pil_image.resize(default_image_size),\n",
    "        ImageAsset(\"bird\").pil_image.resize(default_image_size),\n",
    "    ]\n",
    "\n",
    "    processor = AutoProcessor.from_pretrained(model_name_or_path)\n",
    "\n",
    "    placeholders = [{\"type\": \"image\"} for _ in images]\n",
    "    messages = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": [\n",
    "                *placeholders,\n",
    "                {\n",
    "                \"type\": \"text\",\n",
    "                \"text\": prompt,\n",
    "                },\n",
    "        ],\n",
    "    },\n",
    "    ]\n",
    "\n",
    "    prompt = processor.apply_chat_template(\n",
    "    messages,\n",
    "    tokenize=False,\n",
    "    add_generation_prompt=True,\n",
    "    )\n",
    "    inputs = {\n",
    "    \"prompt\": prompt,\n",
    "    \"multi_modal_data\": {\n",
    "        \"image\": images,\n",
    "    },\n",
    "    }\n",
    "    outputs = llm.generate([inputs], SamplingParams(top_k=1, max_tokens=64))\n",
    "\n",
    "    for output in outputs:\n",
    "        generated_text = output.outputs[0].text\n",
    "        print(f\"Generated text: {generated_text!r}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    qwen2_vl_offline_test()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below is an example output:\n",
    "```bash\n",
    "Generated text: 'The first image shows a close-up of a flower with blue petals and water droplets on them, set against a dark background. The second image features a vibrant red bird with blue and green wings perched on a branch.'\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Online Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "VLLM_NEURON_FRAMEWORK='neuronx-distributed-inference'\n",
    "additional_neuron_config=json.dumps(dict(override_neuron_config=qwen2_vl_neuron_config))\n",
    "start_server_cmd=cmd = f'''python3 -m vllm.entrypoints.openai.api_server \\\n",
    "   --model=\\'{model_name_or_path}\\' \\\n",
    "   --tensor-parallel-size=4 \\\n",
    "   --max-num-seqs=1 \\\n",
    "   --max-model-len=32768 \\\n",
    "   --additional-config=\\'{additional_neuron_config}\\' \\\n",
    "   --no-enable-chunked-prefill \\\n",
    "   --no-enable-prefix-caching \\\n",
    "   --port=8080\n",
    "'''\n",
    "\n",
    "import os\n",
    "os.system(start_server_cmd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once the vLLM server is online, submit requests using the example below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "\n",
    "\n",
    "client = OpenAI(api_key=\"EMPTY\", base_url=\"http://0.0.0.0:8080/v1\")\n",
    "models = client.models.list()\n",
    "model_name = models.data[0].id\n",
    "\n",
    "messages = [\n",
    "   {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "   {\n",
    "      \"role\": \"user\",\n",
    "      \"content\": [\n",
    "        {\n",
    "            \"type\": \"text\",\n",
    "            \"text\": \"Describe this image.\",\n",
    "        },\n",
    "        {\n",
    "            \"type\": \"image_url\",\n",
    "            \"image_url\": {\n",
    "                \"url\": \"example_image_url\" # need to resize to {default_image_width}x{default_image_height}\n",
    "            }\n",
    "        }\n",
    "      ],\n",
    "   },\n",
    "]\n",
    "\n",
    "response = client.chat.completions.create(\n",
    "    model=model_name,\n",
    "    messages=messages,\n",
    "    max_tokens=64,\n",
    "    temperature=1.0,\n",
    "    top_p=1.0,\n",
    "    stream=False,\n",
    "    extra_body={\"top_k\": 1},\n",
    ")\n",
    "\n",
    "generated_text = response.choices[0].message.content\n",
    "print(generated_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "Congratulations ! You now know how to deploy `Qwen/Qwen2-VL-7B-Instruct` on a `trn2.48xlarge` instance. Modify the configurations and deploy the model as per your requirements and use case."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "neuron-224",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
