{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6a2a5707",
   "metadata": {},
   "source": [
    "# Inpainting Images with Black Forest Labs Flux.1-Fill-Dev on Trn1/Trn2\n",
    "\n",
    "This tutorial provides a step-by-step guide for inpainting/outpainting images using the Flux.1-Fill-dev model from Black Forest Labs with NeuronX Distributed (NxD) Inference on a single trn2.48xl instance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a71ce078",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "## Background, Concepts, and Optimizations\n",
    "\n",
    "### Tensor and Context Parallelism\n",
    "\n",
    "For the latent transformer model, use a combination of Tensor Parallelism and Context Parallelism. Due to the compute-bound nature of diffusion inference, add additional parallelism by using sharding on the sequence dimension. Sharding is governed by the `world_size` relative to the `backbone_tp_degree`. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d6c47a4",
   "metadata": {},
   "source": [
    "## Step 1: Setup the environment\n",
    "### Set up and connect to a trn2.48xlarge instance\n",
    "\n",
    "As a prerequisite, this tutorial requires that you have a Trn2 instance created from a Deep Learning AMI that has the Neuron SDK pre-installed.\n",
    "To set up a Trn2 instance using Deep Learning AMI with pre-installed Neuron SDK, see the [NxDI setup guide](https://awsdocs-neuron.readthedocs-hosted.com/en/latest/libraries/nxd-inference/nxdi-setup.html#nxdi-setup).\n",
    "\n",
    "After setting up an instance, use SSH to connect to the Trn2 instance using the key pair that you chose when you launched the instance.\n",
    "\n",
    "To use a Jupyter Notebook (`.ipynb`) on the Neuron instance, follow the [Jupyter Notebook QuickStart guide](https://awsdocs-neuron.readthedocs-hosted.com/en/latest/setup/notebook/setup-jupyter-notebook-steps-troubleshooting.html).\n",
    "\n",
    "After you are connected, activate the Python virtual environment that includes the Neuron SDK.\n",
    "\n",
    "`source ~/aws_neuronx_venv_pytorch_2_9_nxd_inference/bin/activate`\n",
    "\n",
    "Run pip list to verify that the Neuron SDK is installed.\n",
    "\n",
    "`pip list | grep neuron`\n",
    "\n",
    "You should see Neuron packages including neuronx-distributed-inference and neuronx-cc.\n",
    "\n",
    "### Download the model\n",
    "\n",
    "To use this sample, you must first download the model checkpoint from HuggingFace to a local path on the Trn2 instance. For more information, see [Download models](https://huggingface.co/docs/hub/en/models-downloading) in the HuggingFace documentation. You can download and use [black-forest-labs/FLUX.1-dev](https://huggingface.co/black-forest-labs/FLUX.1-Fill-dev) for this tutorial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdce0741",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "744c9c85",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "\n",
    "from neuronx_distributed_inference.models.diffusers.flux.application import NeuronFluxApplication\n",
    "from neuronx_distributed_inference.models.config import NeuronConfig\n",
    "from neuronx_distributed_inference.models.diffusers.flux.clip.modeling_clip import CLIPInferenceConfig\n",
    "from neuronx_distributed_inference.models.diffusers.flux.t5.modeling_t5 import T5InferenceConfig\n",
    "from neuronx_distributed_inference.models.diffusers.flux.modeling_flux import FluxBackboneInferenceConfig\n",
    "from neuronx_distributed_inference.models.diffusers.flux.vae.modeling_vae import VAEDecoderInferenceConfig\n",
    "from neuronx_distributed_inference.utils.hf_adapter import load_pretrained_config\n",
    "from neuronx_distributed_inference.utils.diffusers_adapter import load_diffusers_config\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f77e9c9",
   "metadata": {},
   "source": [
    "## Step 2: Setup Inference Parameters and Model Config\n",
    "\n",
    "Start by initializing your inference parameters, which include model parallelism configuration, image sizes and model configuration. Ensure that that `CKPT_DIR` matches the local directory where you downloaded the model in Step 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29c0c1fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "world_size = 8\n",
    "backbone_tp_degree = 4\n",
    "dtype = torch.bfloat16\n",
    "\n",
    "height, width = [1024, 1024]\n",
    "guidance_scale = 3.5\n",
    "num_inference_steps = 25\n",
    "prompt = \"Milky way galaxy in space\"\n",
    "\n",
    "\n",
    "# The Ckpt directory root under huggingface\n",
    "CKPT_DIR = \"/shared/models/FLUX.1-Fill-dev/\"\n",
    "\n",
    "# Existing Compiled working directory for the compiler\n",
    "BASE_COMPILE_WORK_DIR = \"/tmp/flux/compiler_workdir/\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81915c20",
   "metadata": {},
   "source": [
    "## Step 3: Setup Model and Neuron Configuration\n",
    "\n",
    "Here, you initialize the various component model configuration objects for the models within the Flux Pipeline. The Flux pipeline contains CLIP, T5, the backbone transformer and the VAE. For each component model, you can use the following parallelism configuration:\n",
    "- For CLIP, `tp_degree` of 1\n",
    "- For T5, `tp_degree` is the same as the `world_size`. In the case of this example, this will be 8.\n",
    "- For the backbone transformer, if using Context Parallelism, `tp_degree` is half the world size. In the case of this example, this will be 4, which allows for 2 CP ranks.\n",
    "- Finally, for the VAE, `tp_degree` of 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fae08e51",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_encoder_path = os.path.join(CKPT_DIR, \"text_encoder\")\n",
    "text_encoder_2_path = os.path.join(CKPT_DIR, \"text_encoder_2\")\n",
    "backbone_path = os.path.join(CKPT_DIR, \"transformer\")\n",
    "vae_decoder_path = os.path.join(CKPT_DIR, \"vae\")\n",
    "\n",
    "clip_neuron_config = NeuronConfig(\n",
    "    tp_degree=1,\n",
    "    world_size=world_size,\n",
    "    torch_dtype=dtype,\n",
    ")\n",
    "clip_config = CLIPInferenceConfig(\n",
    "    neuron_config=clip_neuron_config,\n",
    "    load_config=load_pretrained_config(text_encoder_path),\n",
    ")\n",
    "\n",
    "t5_neuron_config = NeuronConfig(\n",
    "    tp_degree=world_size,\n",
    "    world_size=world_size,\n",
    "    torch_dtype=dtype,\n",
    ")\n",
    "t5_config = T5InferenceConfig(\n",
    "    neuron_config=t5_neuron_config,\n",
    "    load_config=load_pretrained_config(text_encoder_2_path),\n",
    ")\n",
    "\n",
    "backbone_neuron_config = NeuronConfig(\n",
    "    tp_degree=backbone_tp_degree,\n",
    "    world_size=world_size,\n",
    "    torch_type=dtype,\n",
    ")\n",
    "backbone_config = FluxBackboneInferenceConfig(\n",
    "    neuron_config=backbone_neuron_config,\n",
    "    load_config=load_diffusers_config(backbone_path),\n",
    "    height=height,\n",
    "    width=width,\n",
    ")\n",
    "\n",
    "decoder_neuron_config = NeuronConfig(\n",
    "    tp_degree=1,\n",
    "    world_size=world_size,\n",
    "    torch_type=dtype,\n",
    ")\n",
    "decoder_config = VAEDecoderInferenceConfig(\n",
    "    neuron_config=decoder_neuron_config,\n",
    "    load_config=load_diffusers_config(vae_decoder_path),\n",
    "    height=height,\n",
    "    width=width,\n",
    ")\n",
    "\n",
    "setattr(\n",
    "    backbone_config,\n",
    "    \"vae_scale_factor\",\n",
    "    decoder_config.vae_scale_factor,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "870692df",
   "metadata": {},
   "source": [
    "## Step 4: Initialize the Flux Application and Compile\n",
    "\n",
    "Now you instantiate the `NeuronFluxApplication` which contains the pipeline orchestration logic, as well as the various component models. You then compile the application, which then compiles each component model individually."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd1f472a",
   "metadata": {},
   "outputs": [],
   "source": [
    "flux_app = NeuronFluxApplication(\n",
    "    model_path=CKPT_DIR,\n",
    "    text_encoder_config=clip_config,\n",
    "    text_encoder2_config=t5_config,\n",
    "    backbone_config=backbone_config,\n",
    "    decoder_config=decoder_config,\n",
    "    height=height,\n",
    "    width=width,\n",
    ")\n",
    "\n",
    "flux_app.compile(BASE_COMPILE_WORK_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4389cc4d",
   "metadata": {},
   "source": [
    "## Step 5: Load Model\n",
    "This step loads the compiled model (NEFF), along with the model weights into device memory. Specifically, calling load on the flux_app loads all the individual component models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c271fe2",
   "metadata": {},
   "outputs": [],
   "source": [
    "flux_app.load(BASE_COMPILE_WORK_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bceb5666",
   "metadata": {},
   "source": [
    "## Step 6: Load the Image and Mask\n",
    "\n",
    "Load the image and mask which denotes the area that has to be filled in adherence to the prompt. The `cat.png` and `mask.png` are taken from COCO dataset (https://cocodataset.org/#explore?id=261706). Ensure that the images are in the same directory as the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98fb15fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from diffusers.utils import load_image\n",
    "from PIL import Image\n",
    "\n",
    "def load_and_resize_image(image_path: str, height: int, width: int) -> Image.Image:\n",
    "    \"\"\"Load an image from a file path and resize it to the specified dimensions.\"\"\"\n",
    "    image = load_image(image_path)\n",
    "    return image.resize((width, height), Image.Resampling.LANCZOS)\n",
    "\n",
    "\n",
    "image = load_and_resize_image('./cat.png', height, width)\n",
    "mask_image = load_and_resize_image('./mask.png', height, width)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5d3d857",
   "metadata": {},
   "source": [
    "## Step 7: Generate Fill Image using the model\n",
    "\n",
    "Finally, you will fill the masked-region of the image using the prompt and render it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c6e5781",
   "metadata": {},
   "outputs": [],
   "source": [
    "image = flux_app(\n",
    "    prompt=prompt,\n",
    "    image=image,\n",
    "    mask_image=mask_image,\n",
    "    height=height,\n",
    "    width=width,\n",
    "    guidance_scale=guidance_scale,\n",
    "    num_inference_steps=num_inference_steps,\n",
    ").images[0]\n",
    "plt.imshow(image)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1db69b0",
   "metadata": {},
   "source": [
    "## Notes \n",
    "### Running Flux Inference on trn1\n",
    "\n",
    "This sample can also be deployed to a trn1.32xlarge with a few modifications. If you are using Context Parallelism specifically, then apply the following parallelism configuration\n",
    "\n",
    "```\n",
    "world_size = 16\n",
    "backbone_tp_degree = 8\n",
    "```\n",
    "\n",
    "Otherwise use the following:\n",
    "\n",
    "```\n",
    "world_size = 8\n",
    "backbone_tp_degree = 8\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
