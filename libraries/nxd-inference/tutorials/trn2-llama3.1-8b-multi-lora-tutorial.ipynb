{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tutorial: Multi-LoRA serving for Llama-3.1-8B on Trn2 instances\n",
    "\n",
    "NeuronX Distributed (NxD) Inference supports multi-LoRA serving. This tutorial provides a step-by-step guide for multi-LoRA serving with Llama-3.1-8B as the base model on a Trn2 instance. It describes two different ways of running multi-LoRA serving with NxD Inference directly and through vLLM (with NxD Inference) We will use LoRA adapters downloaded from HuggingFace as examples for serving."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "raw_mimetype": "text/restructuredtext",
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    ".. contents:: Table of contents\n",
    "    :local:\n",
    "    :depth: 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prerequisites\n",
    "\n",
    "### Set up and connect to a Trn2.48xlarge instance\n",
    "\n",
    "As a prerequisite, this tutorial requires that you have a Trn2 instance created from a Deep Learning AMI that has the Neuron SDK pre-installed.\n",
    "\n",
    "To set up a Trn2 instance using Deep Learning AMI with pre-installed Neuron SDK, see [NxD Inference Setup Guide](https://awsdocs-neuron.readthedocs-hosted.com/en/latest/libraries/nxd-inference/nxdi-setup.html#nxdi-setup). To use Jupyter Notebook on the Neuron instance, you can use this [guide](https://awsdocs-neuron.readthedocs-hosted.com/en/latest/setup/notebook/setup-jupyter-notebook-steps-troubleshooting.html).\n",
    "\n",
    "After setting up an instance, use SSH to connect to the Trn2 instance using the key pair that you chose when you launched the instance.\n",
    "\n",
    "After you are connected, activate the Python virtual environment that includes the Neuron SDK."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "source ~/aws_neuronx_venv_pytorch_2_9_nxd_inference/bin/activate\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run ```pip list``` to verify that the Neuron SDK is installed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "pip list | grep neuron\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should see Neuron packages including `neuronx-distributed-inference` and `neuronx-cc`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Install Packages\n",
    "\n",
    "NxD Inference supports running models with vLLM. This functionality is available in the AWS Neuron fork of the vLLM GitHub repository. Install the latest release branch of vLLM from the AWS Neuron fork following instructions in the [vLLM User Guide for NxD Inference](https://awsdocs-neuron.readthedocs-hosted.com/en/latest/libraries/nxd-inference/developer_guides/vllm-user-guide.html#nxdi-vllm-user-guide).\n",
    "\n",
    "### Download base model and LoRA adapters\n",
    "\n",
    "To use this sample, you must first download a [Llama-3.1-8B-Instruct](https://huggingface.co/meta-llama/Llama-3.1-8B-Instruct) model checkpoint from Hugging Face to a local path on the Trn2 instance. Note that you may need access from Meta for model download. For more information, see [Downloading models](https://huggingface.co/docs/hub/en/models-downloading) in the Hugging Face documentation.\n",
    "\n",
    "You must download LoRA adapters from Hugging Face for multi-LoRA serving. As examples, you can download [nvidia/llama-3.1-nemoguard-8b-topic-control](https://huggingface.co/nvidia/llama-3.1-nemoguard-8b-topic-control), [reissbaker/llama-3.1-8b-abliterated-lora](https://huggingface.co/reissbaker/llama-3.1-8b-abliterated-lora), [Stefano-M/aixpa_amicifamiglia_short_prompt](https://huggingface.co/Stefano-M/aixpa_amicifamiglia_short_prompt), and [GaetanMichelet/Llama-31-8B_task-2_180-samples_config-2](https://huggingface.co/GaetanMichelet/Llama-31-8B_task-2_180-samples_config-2). Suppose these LoRA adapters are saved in `/home/ubuntu/lora_adapters/`.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using vLLM V1 for multi-LoRA serving on Trn2\n",
    "\n",
    "You will run multi-LoRA serving on Trn2 with vLLM V1 using Llama-3.1-8b-instruct and four LoRA adapters, two are preloaded in HBM during model initialization and the four adapters are loaded in host memory. The data type is bfloat16 precision.\n",
    "Please refer to [vLLM User Guide for NxD Inference](https://awsdocs-neuron.readthedocs-hosted.com/en/latest/libraries/nxd-inference/developer_guides/vllm-user-guide.html#nxdi-vllm-user-guide) for more details on how to run model inference on TRN2 with vLLM V1.\n",
    "\n",
    "### Multi-LoRA Configurations\n",
    "\n",
    "You should specifically set the following configurations when enabling multi-LoRA serving with vLLM V1.\n",
    "\n",
    "- `enable_lora` - The flag to enable multi-LoRA serving in NxD Inference. Defaults to False.\n",
    "\n",
    "- `max_loras` - The maximum number of concurrent LoRA adapters in device memory.\n",
    "\n",
    "- `max_cpu_loras` - The maximum number of concurrent LoRA adapters in host memory.\n",
    "\n",
    "- `max_lora_rank` - The highest LoRA rank that needs to be supported. Defaults to ```16```. If it is not specified, the maximum LoRA rank of the LoRA adapter checkpoints will be used.\n",
    "\n",
    "- `lora-ckpt-json` - The the path of JSON file that describes the mappings for the adapter IDs and their checkpoint paths. It includes three fields:\n",
    "   - `lora-ckpt-dir` - The directory of the LoRA adapters.\n",
    "   - `lora-ckpt-paths` - The mapping between LoRA adapter IDs on HBM and their checkpoint paths at initialization. Note that they might be evicted at runtime.\n",
    "   - `lora-ckpt-paths-cpu` - The mapping between LoRA adapter IDs and their checkpoints on CPU.\n",
    "\n",
    "Here is an example of the JSON file:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "```json\n",
    "{\n",
    "    \"lora-ckpt-dir\": \"/home/ubuntu/lora_adapters/\",\n",
    "    \"lora-ckpt-paths\": {\n",
    "        \"lora_id_1\": \"llama-3.1-nemoguard-8b-topic-control\",\n",
    "        \"lora_id_2\": \"llama-3.1-8b-abliterated-lora\"\n",
    "    },\n",
    "    \"lora-ckpt-paths-cpu\": {\n",
    "        \"lora_id_1\": \"llama-3.1-nemoguard-8b-topic-control\",\n",
    "        \"lora_id_2\": \"llama-3.1-8b-abliterated-lora\",\n",
    "        \"lora_id_3\": \"aixpa_amicifamiglia_short_prompt\",\n",
    "        \"lora_id_4\": \"Llama-31-8B_task-2_180-samples_config-2\"\n",
    "    }\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Offline inference example\n",
    "\n",
    "You can run multi-LoRA serving offline on TRN2 with vLLM V1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from vllm import LLM, SamplingParams\n",
    "from vllm.lora.request import LoRARequest\n",
    "\n",
    "MODEL_PATH=\"/home/ubuntu/model_hf/llama-3.1-8b-instruct/\"\n",
    "# Replace this with the path where you saved the JSON file.\n",
    "LORA_CKPT_JSON=\"/home/ubuntu/lora_adapters/lora_adapters.json\"\n",
    "# This is where the compiled model will be saved.\n",
    "COMPILED_MODEL_PATH=\"/home/ubuntu/traced_model/llama-3.1-8B-Lora/\"\n",
    "os.environ[\"NEURON_COMPILED_ARTIFACTS\"] = (COMPILED_MODEL_PATH)\n",
    "os.environ[\"VLLM_USE_V1\"] = \"1\"\n",
    "\n",
    "# Sample prompts.\n",
    "prompts = [\n",
    "    \"The president of the United States is\",\n",
    "    \"The capital of France is\",\n",
    "]\n",
    "\n",
    "# Create a sampling params object.\n",
    "sampling_params = SamplingParams(top_k=1)\n",
    "override_neuron_config = {\n",
    "    \"skip_warmup\": True,\n",
    "    \"lora_ckpt_json\": LORA_CKPT_JSON,\n",
    "}\n",
    "\n",
    "# Create an LLM with multi-LoRA serving.\n",
    "llm = LLM(\n",
    "    model=MODEL_PATH,\n",
    "    max_num_seqs=2,\n",
    "    max_model_len=64,\n",
    "    tensor_parallel_size=32,\n",
    "    additional_config={\n",
    "        \"override_neuron_config\": override_neuron_config\n",
    "    },\n",
    "    enable_lora=True,\n",
    "    max_loras=2,\n",
    "    max_cpu_loras=4,\n",
    "    enable_prefix_caching=False,\n",
    "    enable_chunked_prefill=False,\n",
    ")\n",
    "\"\"\"\n",
    "Only the lora_name needs to be specified.\n",
    "The lora_id and lora_path are supplied at the LLM class/server initialization, after which the paths are\n",
    "handled by NxD Inference.\n",
    "\"\"\"\n",
    "# lora_id_1 is in HBM\n",
    "lora_req_1 = LoRARequest(\"lora_id_1\", 1, \" \")\n",
    "# lora_id_3 is in host memory and it will be dynamically swapped to HBM at runtime\n",
    "lora_req_2 = LoRARequest(\"lora_id_3\", 2, \" \")\n",
    "outputs = llm.generate(prompts, sampling_params, lora_request=[lora_req_1, lora_req_2])\n",
    "\n",
    "for output in outputs:\n",
    "    prompt = output.prompt\n",
    "    generated_text = output.outputs[0].text\n",
    "    print(f\"Prompt: {prompt!r}, Generated text: {generated_text!r}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Run multi-LoRA serving with model quantization\n",
    "\n",
    "To enable multi-LoRA serving with the base model quantized, you must pass some quantization-related arguments to vLLM. For example, you can add the following arguments to `override_neuron_config`. Refer to [Model Weight Quantization](https://awsdocs-neuron.readthedocs-hosted.com/en/latest/libraries/nxd-inference/developer_guides/feature-guide.html#nxdi-weight-quantization) for more information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "quantization_config = {\n",
    "    \"quantized\": True,\n",
    "    # quantized_checkpoints_path is the path that saves the quantized base model weights\n",
    "    \"quantized_checkpoints_path\": os.path.join(COMPILED_MODEL_PATH, \"model_quant.pt\"),\n",
    "    \"quantization_type\": \"per_channel_symmetric\",\n",
    "}\n",
    "# Add quantization config to override_neuron_config\n",
    "override_neuron_config.update(quantization_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Online Server Example\n",
    "\n",
    "You can also run online multi-LoRA serving on TRN2 with vLLM V1. Save the contents of the below script to another shell script file, for example, `start_vllm.sh` and then run it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile start_vllm.sh\n",
    "#!/bin/bash\n",
    "\n",
    "echo \"Running vLLM server in the background...\"\n",
    "rm -f ./vllm_server.log\n",
    "\n",
    "# These should be the same paths used when compiling the model.\n",
    "MODEL_PATH=\"/home/ubuntu/model_hf/llama-3.1-8b-instruct/\"\n",
    "# Replace this with the path where you saved the JSON file. Refer to the NxD Inference script for the JSON format.\n",
    "LORA_CKPT_JSON=\"/home/ubuntu/lora_adapters/lora_adapters.json\"\n",
    "# This is where the compiled model will be saved.\n",
    "COMPILED_MODEL_PATH=\"/home/ubuntu/traced_model/llama-3.1-8B-Lora/\"\n",
    "# Replace this with the path where you saved the LoRA adapters\n",
    "LORA_ADAPTER_DIR=\"/home/ubuntu/lora_adapters\"\n",
    "# Set lora_modules to register LoRA adapters during multi-LoRA serving\n",
    "LORA_MODULES=\"lora_id_1=${LORA_ADAPTER_DIR}/llama-3.1-nemoguard-8b-topic-control \"\n",
    "LORA_MODULES+=\"lora_id_2=${LORA_ADAPTER_DIR}/llama-3.1-8b-abliterated-lora \"\n",
    "LORA_MODULES+=\"lora_id_3=${LORA_ADAPTER_DIR}/aixpa_amicifamiglia_short_prompt \"\n",
    "LORA_MODULES+=\"lora_id_4=${LORA_ADAPTER_DIR}/Llama-31-8B_task-2_180-samples_config-2 \"\n",
    "\n",
    "export NEURON_COMPILED_ARTIFACTS=$COMPILED_MODEL_PATH\n",
    "VLLM_RPC_TIMEOUT=100000 \n",
    "nohup vllm serve $MODEL_PATH \\\n",
    "    --max-num-seqs 2 \\\n",
    "    --max-model-len 64 \\\n",
    "    --tensor-parallel-size 32 \\\n",
    "    --disable-log-requests \\\n",
    "    --no-enable-chunked-prefill \\\n",
    "    --no-enable-prefix-caching \\\n",
    "    --enable-lora \\\n",
    "    --max-loras 2 \\\n",
    "    --max-cpu-loras 8 \\\n",
    "    --override-neuron-config \"{\\\"sequence_parallel_enabled\\\": false}\" \\\n",
    "    --lora-modules ${LORA_MODULES} \\\n",
    "    --port 8000 ./vllm_server.log 2>&1 & \n",
    "\n",
    "SERVER_PID=$!\n",
    "\n",
    "echo \"Server started in the background with the following id: $SERVER_PID. Waiting until server is ready to serve...\"\n",
    "\n",
    "until grep -q \"Server is ready to serve\" ./vllm_server.log 2>/dev/null || ! kill -0 $SERVER_PID 2>/dev/null; do sleep 0.5; done\n",
    "grep -q \"Server is ready to serve\" ./vllm_server.log 2>/dev/null && echo \"vLLM Server is ready!\" || (echo \"vLLM Server failed, check the ./vllm_server.log file\" && exit 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!chmod +x ./start_vllm.sh\n",
    "!./start_vllm.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After the vLLM server is launched, you can check the registered LoRA adapters in the vLLM server."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "curl http://localhost:8000/v1/models | jq"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can send requests to the server for serving with the `model` field as one of the registered LoRA adapter IDs. Two sample requests are:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "# request LoRA adapter in HBM\n",
    "curl http://localhost:8000/v1/completions \\\n",
    "    -H \"Content-Type: application/json\" \\\n",
    "    -d '{\n",
    "        \"model\": \"lora_id_1\",\n",
    "        \"prompt\": \"The president of the United States is\",\n",
    "        \"max_tokens\": 32,\n",
    "        \"temperature\": 0\n",
    "    }' | jq\n",
    "\n",
    "# request LoRA adapter in host memory with dynamic swap\n",
    "curl http://localhost:8000/v1/completions \\\n",
    "    -H \"Content-Type: application/json\" \\\n",
    "    -d '{\n",
    "        \"model\": \"lora_id_3\",\n",
    "        \"prompt\": \"The capital of France is\",\n",
    "        \"max_tokens\": 32,\n",
    "        \"temperature\": 0\n",
    "    }' | jq\n",
    "    \n",
    "# request the base model for serving\n",
    "curl http://localhost:8000/v1/completions \\\n",
    "    -H \"Content-Type: application/json\" \\\n",
    "    -d '{\n",
    "        \"model\": $MODEL_PATH,\n",
    "        \"prompt\": \"The capital of France is\",\n",
    "        \"max_tokens\": 32,\n",
    "        \"temperature\": 0\n",
    "    }' | jq"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dynamically loading LoRA Adapters\n",
    "\n",
    "In addition to specifying LoRA adapters at server startup, you can also dynamically configure LoRA adapters at runtime through dedicated API endpoints. This feature can be particularly useful when the flexibility to change LoRA adapters on-the-fly is needed.\n",
    "\n",
    "Note: the LoRA adapter checkpoints must be stored locally on the host where the server is running before a LoRA adapter is loaded.\n",
    "\n",
    "To enable dynamic LoRA configuration, ensure that the environment variable `VLLM_ALLOW_RUNTIME_LORA_UPDATING` is set to True when starting the server engine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "export VLLM_ALLOW_RUNTIME_LORA_UPDATING=True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example request to load a LoRA adapter:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "curl -X POST http://localhost:8000/v1/load_lora_adapter \\\n",
    "-H \"Content-Type: application/json\" \\\n",
    "-d '{\n",
    "    \"lora_name\": \"lora_id_5\",\n",
    "    \"lora_path\": \"/path/to/lora-adapter-5\"\n",
    "}'\n",
    "\n",
    "# check the registered LoRA adapters in the vLLM server.\n",
    "curl http://localhost:8000/v1/models | jq"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example request to unload a LoRA adapter:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "curl -X POST http://localhost:8000/v1/unload_lora_adapter \\\n",
    "-H \"Content-Type: application/json\" \\\n",
    "-d '{\n",
    "    \"lora_name\": \"lora_id_1\"\n",
    "}'\n",
    "\n",
    "# check the registered LoRA adapters in the vLLM server.\n",
    "curl http://localhost:8000/v1/models | jq"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "neuron-224",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
