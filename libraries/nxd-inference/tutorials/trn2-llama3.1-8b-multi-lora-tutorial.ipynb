{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tutorial: Multi-LoRA serving for Llama-3.1-8B on Trn2 instances\n",
    "\n",
    "NeuronX Distributed (NxD) Inference supports multi-LoRA serving. This tutorial provides a step-by-step guide for multi-LoRA serving with Llama-3.1-8B as the base model on a Trn2 instance. It describes two different ways of running multi-LoRA serving with NxD Inference directly and through vLLM (with NxD Inference) We will use LoRA adapters downloaded from HuggingFace as examples for serving."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "raw_mimetype": "text/restructuredtext",
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    ".. contents:: Table of contents\n",
    "    :local:\n",
    "    :depth: 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prerequisites\n",
    "\n",
    "### Set up and connect to a Trn2.48xlarge instance\n",
    "\n",
    "As a prerequisite, this tutorial requires that you have a Trn2 instance created from a Deep Learning AMI that has the Neuron SDK pre-installed.\n",
    "\n",
    "To set up a Trn2 instance using Deep Learning AMI with pre-installed Neuron SDK, see [NxD Inference Setup Guide](https://awsdocs-neuron.readthedocs-hosted.com/en/latest/libraries/nxd-inference/nxdi-setup.html#nxdi-setup). To use Jupyter Notebook on the Neuron instance, you can use this [guide](https://awsdocs-neuron.readthedocs-hosted.com/en/latest/general/setup/notebook/setup-jupyter-notebook-steps-troubleshooting.html).\n",
    "\n",
    "After setting up an instance, use SSH to connect to the Trn2 instance using the key pair that you chose when you launched the instance.\n",
    "\n",
    "After you are connected, activate the Python virtual environment that includes the Neuron SDK."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "source ~/aws_neuronx_venv_pytorch_2_5_nxd_inference/bin/activate\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run ```pip list``` to verify that the Neuron SDK is installed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "pip list | grep neuron\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should see Neuron packages including `neuronx-distributed-inference` and `neuronx-cc`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Install Packages\n",
    "\n",
    "NxD Inference supports running models with vLLM. This functionality is available in the AWS Neuron fork of the vLLM GitHub repository. Install the latest release branch of vLLM from the AWS Neuron fork following instructions in the [vLLM User Guide for NxD Inference](https://awsdocs-neuron.readthedocs-hosted.com/en/latest/libraries/nxd-inference/developer_guides/vllm-user-guide.html#nxdi-vllm-user-guide).\n",
    "\n",
    "### Download base model and LoRA adapters\n",
    "\n",
    "To use this sample, you must first download a [Llama-3.1-8B-Instruct](https://huggingface.co/meta-llama/Llama-3.1-8B-Instruct) model checkpoint from Hugging Face to a local path on the Trn2 instance. Note that you may need access from Meta for model download. For more information, see [Downloading models](https://huggingface.co/docs/hub/en/models-downloading) in the Hugging Face documentation.\n",
    "\n",
    "You also need to download LoRA adapters from Hugging Face for multi-LoRA serving. As examples, you can download [nvidia/llama-3.1-nemoguard-8b-topic-control](https://huggingface.co/nvidia/llama-3.1-nemoguard-8b-topic-control) and [reissbaker/llama-3.1-8b-abliterated-lora](https://huggingface.co/reissbaker/llama-3.1-8b-abliterated-lora).\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run multi-LoRA serving on Trn2 from NxD Inference\n",
    "\n",
    "We will run multi-LoRA serving from NxD inference with `inference_demo` on Trn2 using Llama-3.1-8B and two LoRA adapters. The data type is bfloat16 precision.\n",
    "\n",
    "You should specifically set the following configurations when enabling multi-LoRA serving with `inference_demo`.\n",
    "\n",
    "- `enable_lora` - The flag to enable multi-LoRA serving in NxD Inference. Defaults to False.\n",
    "\n",
    "- `max_loras` - The maximum number of concurrent LoRA adapters in device memory. Defaults to 1.\n",
    "\n",
    "- `max_lora_rank` - The highest LoRA rank that needs to be supported. Defaults to ```16```. If it is not specified, the maximum LoRA rank of the LoRA adapter checkpoints will be used.\n",
    "\n",
    "- `lora_ckpt_path` - The checkpoint path for LoRA adapter in the format of     `adapter_id : path`. Please set this flag multiple times if multiple LoRA adapters are needs.\n",
    "\n",
    "- `adapter_id` - The adapter ID for prompt. Each prompt comes with an adapter ID.\n",
    "\n",
    "The script compiles the model and runs generation on the given input prompt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "# Replace this with the path where you downloaded and saved the model files.\n",
    "MODEL_PATH=\"/home/ubuntu/models/Llama-3.1-8B-Instruct/\"\n",
    "# Replace the following with the paths where you downloaded and saved the LoRA adapters.\n",
    "LORA_PATH_1=\"/home/ubuntu/models/loras/llama-3.1-nemoguard-8b-topic-control\"\n",
    "LORA_PATH_2=\"/home/ubuntu/models/loras/llama-3.1-8b-abliterated-lora\"\n",
    "# This is where the compiled model will be saved.\n",
    "COMPILED_MODEL_PATH=\"/home/ubuntu/traced_model/Llama-3.1-8B-Lora/\"\n",
    "\n",
    "NUM_CORES=128\n",
    "TP_DEGREE=32\n",
    "LNC=2\n",
    "\n",
    "export NEURON_RT_VIRTUAL_CORE_SIZE=$LNC\n",
    "export NEURON_RT_NUM_CORES=$TP_DEGREE\n",
    "export NEURON_RT_EXEC_TIMEOUT=600\n",
    "export XLA_DENSE_GATHER_FACTOR=0\n",
    "export NEURON_RT_INSPECT_ENABLE=0\n",
    "\n",
    "inference_demo \\\n",
    "    --model-type llama \\\n",
    "    --task-type causal-lm \\\n",
    "        run \\\n",
    "        --model-path $MODEL_PATH \\\n",
    "        --compiled-model-path $COMPILED_MODEL_PATH \\\n",
    "        --torch-dtype bfloat16 \\\n",
    "        --start_rank_id 0 \\\n",
    "        --local_ranks_size $TP_DEGREE \\\n",
    "        --tp-degree $TP_DEGREE \\\n",
    "        --batch-size 2 \\\n",
    "        --max-length 12288 \\\n",
    "        --max-context-length 12288 \\\n",
    "        --seq-len 64 \\\n",
    "        --on-device-sampling \\\n",
    "        --top-k 1 \\\n",
    "        --do-sample \\\n",
    "        --pad-token-id 2 \\\n",
    "        --enable-bucketing \\\n",
    "        --enable-lora \\\n",
    "        --max-loras 2 \\\n",
    "        --lora-ckpt-path \"lora_id_1 : ${LORA_PATH_1}\" \\\n",
    "        --lora-ckpt-path \"lora_id_2 : ${LORA_PATH_2}\" \\\n",
    "        --prompt \"I believe the meaning of life is\" \\\n",
    "        --adapter-id lora_id_1 \\\n",
    "        --prompt \"I believe the meaning of life is\" \\\n",
    "        --adapter-id lora_id_2 \\\n",
    "        | tee log"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NxD Inference expects the same number of prompts and adapter IDs in the script. A prompt is mapped to the adapter ID with the same order. For example, the first prompt in the script assoicates with ```lora_id_1``` and the second one assoicates with ```lora_id_2```. Although the two prompts are the same, NxD Inference will generate different outputs due to different adapter IDs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using vLLM for multi-LoRA serving on Trn2\n",
    "\n",
    "We can run multi-LoRA serving on Trn2 with vLLM for Llama models. Please refer to [vLLM User Guide for NxD Inference](https://awsdocs-neuron.readthedocs-hosted.com/en/latest/libraries/nxd-inference/developer_guides/vllm-user-guide.html#nxdi-vllm-user-guide) for more details on how to run model inference on TRN2 with vLLM.\n",
    "\n",
    "### Multi-LoRA Configurations\n",
    "\n",
    "You should specifically set the following configurations when enabling multi-LoRA serving with vLLM.\n",
    "\n",
    "- `enable_lora` - The flag to enable multi-LoRA serving in NxD Inference. Defaults to False.\n",
    "\n",
    "- `max_loras` - The maximum number of concurrent LoRA adapters in device memory. Defaults to ```1```.\n",
    "\n",
    "- `max_lora_rank` - The highest LoRA rank that needs to be supported. Defaults to ```16```. If it is not specified, the maximum LoRA rank of the LoRA adapter checkpoints will be used.\n",
    "\n",
    "- `lora_modules` - Set the LoRA checkpoint paths and their adapter IDs in the format of `adapter_id_1=path1 adapter_id_2=path2 ...`.\n",
    "\n",
    "### Offline inference example\n",
    "\n",
    "You can also run multi-LoRA serving offline on TRN2 with vLLM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['VLLM_NEURON_FRAMEWORK'] = \"neuronx-distributed-inference\"\n",
    "from vllm import LLM, SamplingParams\n",
    "from vllm.entrypoints.openai.serving_models import LoRAModulePath\n",
    "from vllm.lora.request import LoRARequest\n",
    "\n",
    "MODEL_PATH=\"/home/ubuntu/models/Llama-3.1-8B-Instruct/\"\n",
    "# LoRA checkpoint paths.\n",
    "LORA_PATH_1=\"/home/ubuntu/models/loras/llama-3.1-nemoguard-8b-topic-control\"\n",
    "LORA_PATH_2=\"/home/ubuntu/models/loras/llama-3.1-8b-abliterated-lora\"\n",
    "\n",
    "# Sample prompts.\n",
    "prompts = [\n",
    "    \"The president of the United States is\",\n",
    "    \"The capital of France is\",\n",
    "]\n",
    "\n",
    "# Create a sampling params object.\n",
    "sampling_params = SamplingParams(top_k=1)\n",
    "\n",
    "# Create an LLM with multi-LoRA serving.\n",
    "llm = LLM(\n",
    "    model=MODEL_PATH,\n",
    "    max_num_seqs=2,\n",
    "    max_model_len=64,\n",
    "    tensor_parallel_size=32,\n",
    "    device=\"neuron\",\n",
    "    override_neuron_config={\n",
    "        \"sequence_parallel_enabled\": False,\n",
    "    },\n",
    "    lora_modules=[\n",
    "        LoRAModulePath(name=\"lora_id_1\", path=LORA_PATH_1),\n",
    "        LoRAModulePath(name=\"lora_id_2\", path=LORA_PATH_2),\n",
    "    ],\n",
    "    enable_lora=True,\n",
    "    max_loras=2,\n",
    ")\n",
    "\"\"\"\n",
    "NxD Inference enables static loading of LoRA adapters: https://docs.vllm.ai/en/v0.9.0/features/lora.html on vLLM server start and does\n",
    "not optionally support dynamic serving of LoRA adapters: https://docs.vllm.ai/en/v0.9.0/features/lora.html#dynamically-serving-lora-adapters\n",
    "Only the lora_name needs to be specified.\n",
    "The lora_id and lora_path are supplied at the LLM class/server initialization, after which the paths are\n",
    "handled by NxD Inference.\n",
    "\"\"\"\n",
    "lora_req_1 = LoRARequest(\"lora_id_1\", 0, \" \")\n",
    "lora_req_2 = LoRARequest(\"lora_id_2\", 1, \" \")\n",
    "outputs = llm.generate(prompts, sampling_params, lora_request=[lora_req_1, lora_req_2])\n",
    "\n",
    "for output in outputs:\n",
    "    prompt = output.prompt\n",
    "    generated_text = output.outputs[0].text\n",
    "    print(f\"Prompt: {prompt!r}, Generated text: {generated_text!r}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Online Server Example\n",
    "\n",
    "Save the contents of the below script to another shell script file, for example, `start_vllm.sh` and then run it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile start_vllm.sh\n",
    "#!/bin/bash\n",
    "\n",
    "echo \"Running vLLM server in the background...\"\n",
    "rm -f ./vllm_server.log\n",
    "\n",
    "export NEURON_RT_INSPECT_ENABLE=0\n",
    "export NEURON_RT_VIRTUAL_CORE_SIZE=2\n",
    "\n",
    "# These should be the same paths used when compiling the model.\n",
    "MODEL_PATH=\"/home/ubuntu/models/Llama-3.1-8B-Instruct/\"\n",
    "# Replace the following with the paths where you downloaded and saved the LoRA adapters.\n",
    "LORA_PATH_1=\"/home/ubuntu/models/loras/llama-3.1-nemoguard-8b-topic-control\"\n",
    "LORA_PATH_2=\"/home/ubuntu/models/loras/llama-3.1-8b-abliterated-lora\"\n",
    "# This is where the compiled model will be saved.\n",
    "COMPILED_MODEL_PATH=\"/home/ubuntu/traced_model/Llama-3.1-8B-Lora/\"\n",
    "\n",
    "export VLLM_NEURON_FRAMEWORK=\"neuronx-distributed-inference\"\n",
    "export NEURON_COMPILED_ARTIFACTS=$COMPILED_MODEL_PATH\n",
    "VLLM_RPC_TIMEOUT=100000 \n",
    "nohup python -m vllm.entrypoints.openai.api_server \\\n",
    "    --model $MODEL_PATH \\\n",
    "    --max-num-seqs 2 \\\n",
    "    --max-model-len 64 \\\n",
    "    --tensor-parallel-size 32 \\\n",
    "    --device neuron \\\n",
    "    --use-v2-block-manager \\\n",
    "    --enable-lora \\\n",
    "    --max-loras 2 \\\n",
    "    --override-neuron-config \"{\\\"sequence_parallel_enabled\\\": false}\" \\\n",
    "    --lora-modules lora_id_1=${LORA_PATH_1} lora_id_2=${LORA_PATH_2} \\\n",
    "    --port 8000 ./vllm_server.log 2>&1 & \n",
    "\n",
    "SERVER_PID=$!\n",
    "\n",
    "echo \"Server started in the background with the following id: $SERVER_PID. Waiting until server is ready to serve...\"\n",
    "\n",
    "until grep -q \"Server is ready to serve\" ./vllm_server.log 2>/dev/null || ! kill -0 $SERVER_PID 2>/dev/null; do sleep 0.5; done\n",
    "grep -q \"Server is ready to serve\" ./vllm_server.log 2>/dev/null && echo \"vLLM Server is ready!\" || (echo \"vLLM Server failed, check the ./vllm_server.log file\" && exit 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!chmod +x ./start_vllm.sh\n",
    "!./start_vllm.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After the vLLM server is launched, we can send requests to the server for serving. A sample request is:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "curl http://localhost:8000/v1/chat/completions   -H \"Content-Type: application/json\"   -d '{\n",
    "    \"model\": \"lora_id_1\",\n",
    "    \"messages\": [\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": \"The president of the United States is\"\n",
    "        }\n",
    "    ]\n",
    "}'"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "neuron-224",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
