.. _nxdt_tutorials:

Tutorials
=========

This section will go over tutorials to help users get started with NxD Training library.

.. toctree::
    :maxdepth: 1
    :hidden:

    Megatron GPT Pretraining <megatron_gpt_pretraining>
    HuggingFace Llama3.1/Llama3-8B Pretraining <hf_llama3_8B_pretraining>
    HuggingFace Llama3.1/LLama3-8B Supervised Fine-tuning <hf_llama3_8B_SFT>
    HuggingFace Llama3.1/Llama3-8B Efficient Supervised Fine-tuning with LoRA (Beta) <hf_llama3_8B_SFT_LORA>
    HuggingFace Llama3.1/Llama3-8B Direct Preference Optimization (DPO) and Odds Ratio Preference Optimization (ORPO) based Fine-tuning (Beta) <hf_llama3_8B_DPO_ORPO>
    HuggingFace Llama3.1/Llama3-70B Pretraining <hf_llama3_70B_pretraining>
    Checkpoint Conversion <checkpoint_conversion>

.. include:: /libraries/nxd-training/tutorials/tutorials.txt
